2023-11-10 10:49:14,142 - mmseg - INFO - Multi-processing start method is `None`
2023-11-10 10:49:14,142 - mmseg - INFO - OpenCV num_threads is `<built-in function getNumThreads>
2023-11-10 10:49:14,142 - mmseg - INFO - OMP num threads is 1
2023-11-10 10:49:14,177 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.10 (default, Jun 22 2022, 20:18:18) [GCC 9.4.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: Tesla V100-SXM2-32GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.3, V11.3.109
GCC: x86_64-linux-gnu-gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.11.0+cu113
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0+cu113
OpenCV: 4.2.0
MMCV: 1.5.0
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 11.3
MMSegmentation: 0.23.0+922e771
------------------------------------------------------------

2023-11-10 10:49:14,177 - mmseg - INFO - Distributed training: True
2023-11-10 10:49:14,455 - mmseg - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoder',
    backbone=dict(
        type='debi_base',
        resume='/data/Next-ViT/nextvit_cls_exp/1013_012125/checkpoint_best.pth'
    ),
    neck=dict(
        type='FPN',
        in_channels=[96, 192, 384, 768],
        out_channels=256,
        num_outs=4),
    decode_head=dict(
        type='FPNHead',
        in_channels=[256, 256, 256, 256],
        in_index=[0, 1, 2, 3],
        feature_strides=[4, 8, 16, 32],
        channels=128,
        dropout_ratio=0.1,
        num_classes=150,
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        align_corners=False,
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
dataset_type = 'ADE20KDataset'
data_root = '/data/ADEChallengeData2016'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (512, 512)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', reduce_zero_label=True),
    dict(type='Resize', img_scale=(2048, 512), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(2048, 512),
        flip=False,
        transforms=[
            dict(type='AlignResize', keep_ratio=True, size_divisor=32),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=4,
    train=dict(
        type='RepeatDataset',
        times=50,
        dataset=dict(
            type='ADE20KDataset',
            data_root='/data/ADEChallengeData2016',
            img_dir='images/training',
            ann_dir='annotations/training',
            pipeline=[
                dict(type='LoadImageFromFile'),
                dict(type='LoadAnnotations', reduce_zero_label=True),
                dict(
                    type='Resize',
                    img_scale=(2048, 512),
                    ratio_range=(0.5, 2.0)),
                dict(
                    type='RandomCrop',
                    crop_size=(512, 512),
                    cat_max_ratio=0.75),
                dict(type='RandomFlip', prob=0.5),
                dict(type='PhotoMetricDistortion'),
                dict(
                    type='Normalize',
                    mean=[123.675, 116.28, 103.53],
                    std=[58.395, 57.12, 57.375],
                    to_rgb=True),
                dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
                dict(type='DefaultFormatBundle'),
                dict(type='Collect', keys=['img', 'gt_semantic_seg'])
            ])),
    val=dict(
        type='ADE20KDataset',
        data_root='/data/ADEChallengeData2016',
        img_dir='images/validation',
        ann_dir='annotations/validation',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 512),
                flip=False,
                transforms=[
                    dict(type='AlignResize', keep_ratio=True, size_divisor=32),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='ADE20KDataset',
        data_root='/data/ADEChallengeData2016',
        img_dir='images/validation',
        ann_dir='annotations/validation',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 512),
                flip=False,
                transforms=[
                    dict(type='AlignResize', keep_ratio=True, size_divisor=32),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
gpu_multiples = 1
optimizer = dict(type='AdamW', lr=0.0001, weight_decay=0.0001)
optimizer_config = dict(type='Fp16OptimizerHook', loss_scale=512.0)
fp16 = dict()
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=3000,
    warmup_ratio=0.1,
    min_lr_ratio=1e-10)
runner = dict(type='IterBasedRunner', max_iters=160000)
checkpoint_config = dict(by_epoch=False, interval=8000)
evaluation = dict(interval=8000, metric='mIoU', save_best='mIoU')
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
find_unused_parameters = False
work_dir = './work_dirs/fpn_512_debi_base_160k'
gpu_ids = range(0, 8)
auto_resume = False

2023-11-10 10:49:22,124 - mmseg - INFO - Set random seed to 239930529, deterministic: False
2023-11-10 10:49:24,784 - mmseg - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2023-11-10 10:49:24,803 - mmseg - INFO - initialize FPNHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
Name of parameter - Initialization information

backbone.downsample_layers.0.0.weight - torch.Size([48, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.0.0.bias - torch.Size([48]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.0.1.weight - torch.Size([48]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.0.1.bias - torch.Size([48]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.0.3.weight - torch.Size([96, 48, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.0.3.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.0.4.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.0.4.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.1.0.weight - torch.Size([192, 96, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.1.0.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.1.1.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.1.1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.2.0.weight - torch.Size([384, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.2.1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.2.1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.3.0.weight - torch.Size([768, 384, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.3.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.3.1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.downsample_layers.3.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.pos_embed1.weight - torch.Size([96, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.pos_embed1.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.pos_embed2.weight - torch.Size([96, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.pos_embed2.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.norm1.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.norm1.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn1.lepe.weight - torch.Size([96, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn1.lepe.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn1.qkv.qkv.weight - torch.Size([288, 96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn1.qkv.qkv.bias - torch.Size([288]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn1.wo.weight - torch.Size([96, 96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn1.wo.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.rpe_table - torch.Size([3, 111, 111]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.lepe1.weight - torch.Size([96, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.lepe1.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.qkv_conv.qkv.weight - torch.Size([288, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.qkv_conv.qkv.bias - torch.Size([288]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.proj_q.weight - torch.Size([96, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.proj_q.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.proj_k.weight - torch.Size([96, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.proj_k.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.proj_v.weight - torch.Size([96, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.proj_v.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.proj_out.weight - torch.Size([96, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.proj_out.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.unifyheads1.weight - torch.Size([96, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.unifyheads1.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.conv_offset_q.0.weight - torch.Size([96, 1, 9, 9]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.conv_offset_q.1.norm.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.conv_offset_q.1.norm.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.conv_offset_q.3.weight - torch.Size([1, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.norm.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.norm.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.norm2.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.norm2.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.mlp.linear1.0.weight - torch.Size([288, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.mlp.linear1.0.bias - torch.Size([288]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.mlp.linear2.0.weight - torch.Size([96, 288, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.mlp.linear2.0.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.mlp.dwc.weight - torch.Size([288, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.attn2.mlp.dwc.bias - torch.Size([288]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.norm2.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.norm2.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.mlp1.linear1.0.weight - torch.Size([288, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.mlp1.linear1.0.bias - torch.Size([288]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.mlp1.linear2.0.weight - torch.Size([96, 288, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.mlp1.linear2.0.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.mlp1.dwc.weight - torch.Size([288, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.mlp1.dwc.bias - torch.Size([288]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.norm3.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.norm3.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.norm4.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.norm4.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.mlp2.linear1.0.weight - torch.Size([288, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.mlp2.linear1.0.bias - torch.Size([288]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.mlp2.linear2.0.weight - torch.Size([96, 288, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.mlp2.linear2.0.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.mlp2.dwc.weight - torch.Size([288, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.0.mlp2.dwc.bias - torch.Size([288]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.pos_embed1.weight - torch.Size([96, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.pos_embed1.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.pos_embed2.weight - torch.Size([96, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.pos_embed2.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.norm1.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.norm1.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn1.lepe.weight - torch.Size([96, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn1.lepe.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn1.qkv.qkv.weight - torch.Size([288, 96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn1.qkv.qkv.bias - torch.Size([288]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn1.wo.weight - torch.Size([96, 96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn1.wo.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.rpe_table - torch.Size([3, 111, 111]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.lepe1.weight - torch.Size([96, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.lepe1.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.qkv_conv.qkv.weight - torch.Size([288, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.qkv_conv.qkv.bias - torch.Size([288]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.proj_q.weight - torch.Size([96, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.proj_q.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.proj_k.weight - torch.Size([96, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.proj_k.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.proj_v.weight - torch.Size([96, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.proj_v.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.proj_out.weight - torch.Size([96, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.proj_out.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.unifyheads1.weight - torch.Size([96, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.unifyheads1.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.conv_offset_q.0.weight - torch.Size([96, 1, 9, 9]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.conv_offset_q.1.norm.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.conv_offset_q.1.norm.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.conv_offset_q.3.weight - torch.Size([1, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.norm.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.norm.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.norm2.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.norm2.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.mlp.linear1.0.weight - torch.Size([288, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.mlp.linear1.0.bias - torch.Size([288]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.mlp.linear2.0.weight - torch.Size([96, 288, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.mlp.linear2.0.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.mlp.dwc.weight - torch.Size([288, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.attn2.mlp.dwc.bias - torch.Size([288]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.norm2.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.norm2.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.mlp1.linear1.0.weight - torch.Size([288, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.mlp1.linear1.0.bias - torch.Size([288]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.mlp1.linear2.0.weight - torch.Size([96, 288, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.mlp1.linear2.0.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.mlp1.dwc.weight - torch.Size([288, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.mlp1.dwc.bias - torch.Size([288]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.norm3.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.norm3.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.norm4.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.norm4.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.mlp2.linear1.0.weight - torch.Size([288, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.mlp2.linear1.0.bias - torch.Size([288]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.mlp2.linear2.0.weight - torch.Size([96, 288, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.mlp2.linear2.0.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.mlp2.dwc.weight - torch.Size([288, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.0.1.mlp2.dwc.bias - torch.Size([288]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.pos_embed1.weight - torch.Size([192, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.pos_embed1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.pos_embed2.weight - torch.Size([192, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.pos_embed2.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.norm1.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.norm1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn1.lepe.weight - torch.Size([192, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn1.lepe.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn1.qkv.qkv.weight - torch.Size([576, 192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn1.qkv.qkv.bias - torch.Size([576]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn1.wo.weight - torch.Size([192, 192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn1.wo.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.rpe_table - torch.Size([6, 55, 55]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.lepe1.weight - torch.Size([192, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.lepe1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.qkv_conv.qkv.weight - torch.Size([576, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.qkv_conv.qkv.bias - torch.Size([576]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.proj_q.weight - torch.Size([192, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.proj_q.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.proj_k.weight - torch.Size([192, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.proj_k.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.proj_v.weight - torch.Size([192, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.proj_v.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.proj_out.weight - torch.Size([192, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.proj_out.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.unifyheads1.weight - torch.Size([192, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.unifyheads1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.conv_offset_q.0.weight - torch.Size([96, 1, 7, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.conv_offset_q.1.norm.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.conv_offset_q.1.norm.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.conv_offset_q.3.weight - torch.Size([1, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.norm2.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.norm2.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.mlp.linear1.0.weight - torch.Size([576, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.mlp.linear1.0.bias - torch.Size([576]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.mlp.linear2.0.weight - torch.Size([192, 576, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.mlp.linear2.0.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.mlp.dwc.weight - torch.Size([576, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.attn2.mlp.dwc.bias - torch.Size([576]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.norm2.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.norm2.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.mlp1.linear1.0.weight - torch.Size([576, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.mlp1.linear1.0.bias - torch.Size([576]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.mlp1.linear2.0.weight - torch.Size([192, 576, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.mlp1.linear2.0.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.mlp1.dwc.weight - torch.Size([576, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.mlp1.dwc.bias - torch.Size([576]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.norm3.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.norm3.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.norm4.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.norm4.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.mlp2.linear1.0.weight - torch.Size([576, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.mlp2.linear1.0.bias - torch.Size([576]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.mlp2.linear2.0.weight - torch.Size([192, 576, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.mlp2.linear2.0.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.mlp2.dwc.weight - torch.Size([576, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.0.mlp2.dwc.bias - torch.Size([576]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.pos_embed1.weight - torch.Size([192, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.pos_embed1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.pos_embed2.weight - torch.Size([192, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.pos_embed2.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.norm1.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.norm1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn1.lepe.weight - torch.Size([192, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn1.lepe.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn1.qkv.qkv.weight - torch.Size([576, 192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn1.qkv.qkv.bias - torch.Size([576]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn1.wo.weight - torch.Size([192, 192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn1.wo.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.rpe_table - torch.Size([6, 55, 55]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.lepe1.weight - torch.Size([192, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.lepe1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.qkv_conv.qkv.weight - torch.Size([576, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.qkv_conv.qkv.bias - torch.Size([576]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.proj_q.weight - torch.Size([192, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.proj_q.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.proj_k.weight - torch.Size([192, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.proj_k.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.proj_v.weight - torch.Size([192, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.proj_v.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.proj_out.weight - torch.Size([192, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.proj_out.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.unifyheads1.weight - torch.Size([192, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.unifyheads1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.conv_offset_q.0.weight - torch.Size([96, 1, 7, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.conv_offset_q.1.norm.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.conv_offset_q.1.norm.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.conv_offset_q.3.weight - torch.Size([1, 96, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.norm2.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.norm2.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.mlp.linear1.0.weight - torch.Size([576, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.mlp.linear1.0.bias - torch.Size([576]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.mlp.linear2.0.weight - torch.Size([192, 576, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.mlp.linear2.0.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.mlp.dwc.weight - torch.Size([576, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.attn2.mlp.dwc.bias - torch.Size([576]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.norm2.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.norm2.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.mlp1.linear1.0.weight - torch.Size([576, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.mlp1.linear1.0.bias - torch.Size([576]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.mlp1.linear2.0.weight - torch.Size([192, 576, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.mlp1.linear2.0.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.mlp1.dwc.weight - torch.Size([576, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.mlp1.dwc.bias - torch.Size([576]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.norm3.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.norm3.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.norm4.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.norm4.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.mlp2.linear1.0.weight - torch.Size([576, 192, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.mlp2.linear1.0.bias - torch.Size([576]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.mlp2.linear2.0.weight - torch.Size([192, 576, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.mlp2.linear2.0.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.mlp2.dwc.weight - torch.Size([576, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.1.1.mlp2.dwc.bias - torch.Size([576]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.pos_embed1.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.pos_embed1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.pos_embed2.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.pos_embed2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn1.lepe.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn1.lepe.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn1.qkv.qkv.weight - torch.Size([1152, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn1.qkv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn1.wo.weight - torch.Size([384, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn1.wo.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.rpe_table - torch.Size([12, 27, 27]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.lepe1.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.lepe1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.qkv_conv.qkv.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.qkv_conv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.proj_q.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.proj_q.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.proj_k.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.proj_k.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.proj_v.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.proj_v.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.proj_out.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.proj_out.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.unifyheads1.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.unifyheads1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.conv_offset_q.0.weight - torch.Size([128, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.conv_offset_q.1.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.conv_offset_q.1.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.conv_offset_q.3.weight - torch.Size([1, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.norm.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.norm.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.mlp.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.mlp.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.mlp.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.mlp.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.mlp.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.attn2.mlp.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.mlp1.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.mlp1.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.mlp1.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.mlp1.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.mlp1.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.mlp1.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.norm3.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.norm3.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.norm4.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.norm4.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.mlp2.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.mlp2.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.mlp2.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.mlp2.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.mlp2.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.0.mlp2.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.pos_embed1.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.pos_embed1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.pos_embed2.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.pos_embed2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn1.lepe.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn1.lepe.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn1.qkv.qkv.weight - torch.Size([1152, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn1.qkv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn1.wo.weight - torch.Size([384, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn1.wo.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.rpe_table - torch.Size([12, 27, 27]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.lepe1.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.lepe1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.qkv_conv.qkv.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.qkv_conv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.proj_q.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.proj_q.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.proj_k.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.proj_k.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.proj_v.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.proj_v.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.proj_out.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.proj_out.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.unifyheads1.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.unifyheads1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.conv_offset_q.0.weight - torch.Size([128, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.conv_offset_q.1.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.conv_offset_q.1.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.conv_offset_q.3.weight - torch.Size([1, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.norm.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.norm.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.mlp.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.mlp.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.mlp.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.mlp.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.mlp.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.attn2.mlp.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.mlp1.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.mlp1.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.mlp1.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.mlp1.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.mlp1.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.mlp1.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.norm3.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.norm3.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.norm4.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.norm4.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.mlp2.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.mlp2.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.mlp2.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.mlp2.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.mlp2.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.1.mlp2.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.pos_embed1.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.pos_embed1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.pos_embed2.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.pos_embed2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn1.lepe.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn1.lepe.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn1.qkv.qkv.weight - torch.Size([1152, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn1.qkv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn1.wo.weight - torch.Size([384, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn1.wo.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.rpe_table - torch.Size([12, 27, 27]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.lepe1.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.lepe1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.qkv_conv.qkv.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.qkv_conv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.proj_q.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.proj_q.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.proj_k.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.proj_k.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.proj_v.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.proj_v.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.proj_out.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.proj_out.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.unifyheads1.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.unifyheads1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.conv_offset_q.0.weight - torch.Size([128, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.conv_offset_q.1.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.conv_offset_q.1.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.conv_offset_q.3.weight - torch.Size([1, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.norm.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.norm.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.mlp.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.mlp.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.mlp.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.mlp.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.mlp.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.attn2.mlp.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.mlp1.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.mlp1.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.mlp1.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.mlp1.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.mlp1.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.mlp1.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.norm3.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.norm3.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.norm4.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.norm4.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.mlp2.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.mlp2.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.mlp2.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.mlp2.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.mlp2.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.2.mlp2.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.pos_embed1.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.pos_embed1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.pos_embed2.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.pos_embed2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn1.lepe.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn1.lepe.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn1.qkv.qkv.weight - torch.Size([1152, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn1.qkv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn1.wo.weight - torch.Size([384, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn1.wo.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.rpe_table - torch.Size([12, 27, 27]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.lepe1.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.lepe1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.qkv_conv.qkv.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.qkv_conv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.proj_q.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.proj_q.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.proj_k.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.proj_k.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.proj_v.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.proj_v.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.proj_out.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.proj_out.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.unifyheads1.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.unifyheads1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.conv_offset_q.0.weight - torch.Size([128, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.conv_offset_q.1.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.conv_offset_q.1.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.conv_offset_q.3.weight - torch.Size([1, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.norm.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.norm.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.mlp.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.mlp.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.mlp.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.mlp.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.mlp.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.attn2.mlp.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.mlp1.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.mlp1.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.mlp1.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.mlp1.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.mlp1.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.mlp1.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.norm3.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.norm3.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.norm4.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.norm4.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.mlp2.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.mlp2.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.mlp2.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.mlp2.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.mlp2.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.3.mlp2.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.pos_embed1.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.pos_embed1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.pos_embed2.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.pos_embed2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn1.lepe.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn1.lepe.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn1.qkv.qkv.weight - torch.Size([1152, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn1.qkv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn1.wo.weight - torch.Size([384, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn1.wo.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.rpe_table - torch.Size([12, 27, 27]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.lepe1.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.lepe1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.qkv_conv.qkv.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.qkv_conv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.proj_q.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.proj_q.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.proj_k.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.proj_k.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.proj_v.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.proj_v.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.proj_out.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.proj_out.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.unifyheads1.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.unifyheads1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.conv_offset_q.0.weight - torch.Size([128, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.conv_offset_q.1.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.conv_offset_q.1.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.conv_offset_q.3.weight - torch.Size([1, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.norm.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.norm.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.mlp.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.mlp.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.mlp.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.mlp.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.mlp.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.attn2.mlp.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.mlp1.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.mlp1.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.mlp1.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.mlp1.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.mlp1.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.mlp1.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.norm3.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.norm3.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.norm4.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.norm4.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.mlp2.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.mlp2.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.mlp2.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.mlp2.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.mlp2.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.4.mlp2.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.pos_embed1.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.pos_embed1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.pos_embed2.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.pos_embed2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn1.lepe.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn1.lepe.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn1.qkv.qkv.weight - torch.Size([1152, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn1.qkv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn1.wo.weight - torch.Size([384, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn1.wo.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.rpe_table - torch.Size([12, 27, 27]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.lepe1.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.lepe1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.qkv_conv.qkv.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.qkv_conv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.proj_q.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.proj_q.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.proj_k.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.proj_k.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.proj_v.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.proj_v.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.proj_out.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.proj_out.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.unifyheads1.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.unifyheads1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.conv_offset_q.0.weight - torch.Size([128, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.conv_offset_q.1.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.conv_offset_q.1.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.conv_offset_q.3.weight - torch.Size([1, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.norm.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.norm.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.mlp.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.mlp.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.mlp.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.mlp.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.mlp.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.attn2.mlp.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.mlp1.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.mlp1.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.mlp1.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.mlp1.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.mlp1.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.mlp1.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.norm3.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.norm3.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.norm4.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.norm4.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.mlp2.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.mlp2.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.mlp2.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.mlp2.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.mlp2.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.5.mlp2.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.pos_embed1.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.pos_embed1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.pos_embed2.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.pos_embed2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn1.lepe.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn1.lepe.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn1.qkv.qkv.weight - torch.Size([1152, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn1.qkv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn1.wo.weight - torch.Size([384, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn1.wo.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.rpe_table - torch.Size([12, 27, 27]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.lepe1.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.lepe1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.qkv_conv.qkv.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.qkv_conv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.proj_q.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.proj_q.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.proj_k.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.proj_k.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.proj_v.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.proj_v.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.proj_out.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.proj_out.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.unifyheads1.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.unifyheads1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.conv_offset_q.0.weight - torch.Size([128, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.conv_offset_q.1.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.conv_offset_q.1.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.conv_offset_q.3.weight - torch.Size([1, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.norm.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.norm.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.mlp.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.mlp.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.mlp.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.mlp.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.mlp.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.attn2.mlp.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.mlp1.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.mlp1.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.mlp1.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.mlp1.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.mlp1.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.mlp1.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.norm3.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.norm3.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.norm4.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.norm4.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.mlp2.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.mlp2.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.mlp2.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.mlp2.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.mlp2.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.6.mlp2.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.pos_embed1.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.pos_embed1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.pos_embed2.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.pos_embed2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn1.lepe.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn1.lepe.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn1.qkv.qkv.weight - torch.Size([1152, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn1.qkv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn1.wo.weight - torch.Size([384, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn1.wo.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.rpe_table - torch.Size([12, 27, 27]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.lepe1.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.lepe1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.qkv_conv.qkv.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.qkv_conv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.proj_q.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.proj_q.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.proj_k.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.proj_k.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.proj_v.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.proj_v.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.proj_out.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.proj_out.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.unifyheads1.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.unifyheads1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.conv_offset_q.0.weight - torch.Size([128, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.conv_offset_q.1.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.conv_offset_q.1.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.conv_offset_q.3.weight - torch.Size([1, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.norm.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.norm.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.mlp.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.mlp.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.mlp.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.mlp.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.mlp.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.attn2.mlp.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.mlp1.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.mlp1.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.mlp1.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.mlp1.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.mlp1.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.mlp1.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.norm3.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.norm3.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.norm4.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.norm4.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.mlp2.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.mlp2.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.mlp2.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.mlp2.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.mlp2.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.7.mlp2.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.pos_embed1.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.pos_embed1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.pos_embed2.weight - torch.Size([384, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.pos_embed2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn1.lepe.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn1.lepe.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn1.qkv.qkv.weight - torch.Size([1152, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn1.qkv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn1.wo.weight - torch.Size([384, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn1.wo.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.rpe_table - torch.Size([12, 27, 27]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.lepe1.weight - torch.Size([384, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.lepe1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.qkv_conv.qkv.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.qkv_conv.qkv.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.proj_q.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.proj_q.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.proj_k.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.proj_k.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.proj_v.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.proj_v.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.proj_out.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.proj_out.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.unifyheads1.weight - torch.Size([384, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.unifyheads1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.conv_offset_q.0.weight - torch.Size([128, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.conv_offset_q.1.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.conv_offset_q.1.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.conv_offset_q.3.weight - torch.Size([1, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.norm.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.norm.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.mlp.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.mlp.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.mlp.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.mlp.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.mlp.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.attn2.mlp.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.mlp1.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.mlp1.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.mlp1.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.mlp1.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.mlp1.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.mlp1.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.norm3.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.norm3.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.norm4.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.norm4.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.mlp2.linear1.0.weight - torch.Size([1152, 384, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.mlp2.linear1.0.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.mlp2.linear2.0.weight - torch.Size([384, 1152, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.mlp2.linear2.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.mlp2.dwc.weight - torch.Size([1152, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.2.8.mlp2.dwc.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.pos_embed1.weight - torch.Size([768, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.pos_embed1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.pos_embed2.weight - torch.Size([768, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.pos_embed2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.norm1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.norm1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.rpe_table - torch.Size([24, 13, 13]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.lepe1.weight - torch.Size([768, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.lepe1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.qkv_conv.qkv.weight - torch.Size([2304, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.qkv_conv.qkv.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.proj_q.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.proj_q.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.proj_k.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.proj_k.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.proj_v.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.proj_v.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.proj_out.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.proj_out.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.unifyheads1.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.unifyheads1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.conv_offset_q.0.weight - torch.Size([128, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.conv_offset_q.1.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.conv_offset_q.1.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.conv_offset_q.3.weight - torch.Size([1, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.norm2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.norm2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.mlp.linear1.0.weight - torch.Size([2304, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.mlp.linear1.0.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.mlp.linear2.0.weight - torch.Size([768, 2304, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.mlp.linear2.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.mlp.dwc.weight - torch.Size([2304, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn1.mlp.dwc.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.rpe_table - torch.Size([24, 13, 13]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.lepe1.weight - torch.Size([768, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.lepe1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.qkv_conv.qkv.weight - torch.Size([2304, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.qkv_conv.qkv.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.proj_q.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.proj_q.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.proj_k.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.proj_k.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.proj_v.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.proj_v.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.proj_out.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.proj_out.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.unifyheads1.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.unifyheads1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.conv_offset_q.0.weight - torch.Size([128, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.conv_offset_q.1.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.conv_offset_q.1.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.conv_offset_q.3.weight - torch.Size([1, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.norm2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.norm2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.mlp.linear1.0.weight - torch.Size([2304, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.mlp.linear1.0.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.mlp.linear2.0.weight - torch.Size([768, 2304, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.mlp.linear2.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.mlp.dwc.weight - torch.Size([2304, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.attn2.mlp.dwc.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.norm2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.norm2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.mlp1.linear1.0.weight - torch.Size([2304, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.mlp1.linear1.0.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.mlp1.linear2.0.weight - torch.Size([768, 2304, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.mlp1.linear2.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.mlp1.dwc.weight - torch.Size([2304, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.mlp1.dwc.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.norm3.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.norm3.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.norm4.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.norm4.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.mlp2.linear1.0.weight - torch.Size([2304, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.mlp2.linear1.0.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.mlp2.linear2.0.weight - torch.Size([768, 2304, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.mlp2.linear2.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.mlp2.dwc.weight - torch.Size([2304, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.0.mlp2.dwc.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.pos_embed1.weight - torch.Size([768, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.pos_embed1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.pos_embed2.weight - torch.Size([768, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.pos_embed2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.norm1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.norm1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.rpe_table - torch.Size([24, 13, 13]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.lepe1.weight - torch.Size([768, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.lepe1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.qkv_conv.qkv.weight - torch.Size([2304, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.qkv_conv.qkv.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.proj_q.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.proj_q.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.proj_k.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.proj_k.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.proj_v.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.proj_v.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.proj_out.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.proj_out.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.unifyheads1.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.unifyheads1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.conv_offset_q.0.weight - torch.Size([128, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.conv_offset_q.1.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.conv_offset_q.1.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.conv_offset_q.3.weight - torch.Size([1, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.norm2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.norm2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.mlp.linear1.0.weight - torch.Size([2304, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.mlp.linear1.0.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.mlp.linear2.0.weight - torch.Size([768, 2304, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.mlp.linear2.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.mlp.dwc.weight - torch.Size([2304, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn1.mlp.dwc.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.rpe_table - torch.Size([24, 13, 13]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.lepe1.weight - torch.Size([768, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.lepe1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.qkv_conv.qkv.weight - torch.Size([2304, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.qkv_conv.qkv.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.proj_q.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.proj_q.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.proj_k.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.proj_k.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.proj_v.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.proj_v.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.proj_out.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.proj_out.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.unifyheads1.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.unifyheads1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.conv_offset_q.0.weight - torch.Size([128, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.conv_offset_q.1.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.conv_offset_q.1.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.conv_offset_q.3.weight - torch.Size([1, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.norm2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.norm2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.mlp.linear1.0.weight - torch.Size([2304, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.mlp.linear1.0.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.mlp.linear2.0.weight - torch.Size([768, 2304, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.mlp.linear2.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.mlp.dwc.weight - torch.Size([2304, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.attn2.mlp.dwc.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.norm2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.norm2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.mlp1.linear1.0.weight - torch.Size([2304, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.mlp1.linear1.0.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.mlp1.linear2.0.weight - torch.Size([768, 2304, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.mlp1.linear2.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.mlp1.dwc.weight - torch.Size([2304, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.mlp1.dwc.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.norm3.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.norm3.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.norm4.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.norm4.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.mlp2.linear1.0.weight - torch.Size([2304, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.mlp2.linear1.0.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.mlp2.linear2.0.weight - torch.Size([768, 2304, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.mlp2.linear2.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.mlp2.dwc.weight - torch.Size([2304, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stages.3.1.mlp2.dwc.bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.extra_norms.0.ln.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.extra_norms.0.ln.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.extra_norms.1.ln.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.extra_norms.1.ln.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.extra_norms.2.ln.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.extra_norms.2.ln.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.extra_norms.3.ln.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.extra_norms.3.ln.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

neck.lateral_convs.0.conv.weight - torch.Size([256, 96, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

neck.lateral_convs.1.conv.weight - torch.Size([256, 192, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

neck.lateral_convs.2.conv.weight - torch.Size([256, 384, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

neck.lateral_convs.3.conv.weight - torch.Size([256, 768, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.conv_seg.weight - torch.Size([150, 128, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.conv_seg.bias - torch.Size([150]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.scale_heads.0.0.conv.weight - torch.Size([128, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.0.0.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.0.0.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.1.0.conv.weight - torch.Size([128, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.1.0.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.1.0.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.2.0.conv.weight - torch.Size([128, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.2.0.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.2.0.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.2.2.conv.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.2.2.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.2.2.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.3.0.conv.weight - torch.Size([128, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.3.0.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.3.0.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.3.2.conv.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.3.2.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.3.2.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.3.4.conv.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.3.4.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.scale_heads.3.4.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  
2023-11-10 10:49:24,821 - mmseg - INFO - EncoderDecoder(
  (backbone): debi_base(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): GELU()
        (3): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Sequential(
        (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): Sequential(
        (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): Sequential(
        (0): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (pos_embed1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
          (pos_embed2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn1): BiLevelRoutingAttention(
            (lepe): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv): QKVLinear(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
            )
            (wo): Linear(in_features=96, out_features=96, bias=True)
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
          )
          (attn2): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(96, 96, kernel_size=(5, 5), stride=(8, 8), padding=(2, 2), groups=96)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(96, 96, kernel_size=(9, 9), stride=(8, 8), padding=(4, 4), groups=96, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
            )
          )
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp1): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
          )
          (drop_path1): Identity()
          (drop_path2): Identity()
          (norm3): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (norm4): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp2): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
          )
        )
        (1): Block(
          (pos_embed1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
          (pos_embed2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn1): BiLevelRoutingAttention(
            (lepe): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv): QKVLinear(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
            )
            (wo): Linear(in_features=96, out_features=96, bias=True)
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
          )
          (attn2): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(96, 96, kernel_size=(5, 5), stride=(8, 8), padding=(2, 2), groups=96)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(96, 96, kernel_size=(9, 9), stride=(8, 8), padding=(4, 4), groups=96, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
            )
          )
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp1): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
          )
          (drop_path1): DropPath()
          (drop_path2): DropPath()
          (norm3): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (norm4): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp2): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
          )
        )
      )
      (1): Sequential(
        (0): Block(
          (pos_embed1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
          (pos_embed2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn1): BiLevelRoutingAttention(
            (lepe): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv): QKVLinear(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
            )
            (wo): Linear(in_features=192, out_features=192, bias=True)
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
          )
          (attn2): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(192, 192, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2), groups=192)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3), groups=96, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
            )
          )
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp1): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
          )
          (drop_path1): DropPath()
          (drop_path2): DropPath()
          (norm3): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (norm4): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp2): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
          )
        )
        (1): Block(
          (pos_embed1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
          (pos_embed2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn1): BiLevelRoutingAttention(
            (lepe): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv): QKVLinear(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
            )
            (wo): Linear(in_features=192, out_features=192, bias=True)
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
          )
          (attn2): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(192, 192, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2), groups=192)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3), groups=96, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
            )
          )
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp1): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
          )
          (drop_path1): DropPath()
          (drop_path2): DropPath()
          (norm3): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (norm4): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp2): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
          )
        )
      )
      (2): Sequential(
        (0): Block(
          (pos_embed1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (pos_embed2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn1): BiLevelRoutingAttention(
            (lepe): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv): QKVLinear(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
            )
            (wo): Linear(in_features=384, out_features=384, bias=True)
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
          )
          (attn2): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(384, 384, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=128, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
            )
          )
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp1): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
          (drop_path1): DropPath()
          (drop_path2): DropPath()
          (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (norm4): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp2): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
        )
        (1): Block(
          (pos_embed1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (pos_embed2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn1): BiLevelRoutingAttention(
            (lepe): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv): QKVLinear(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
            )
            (wo): Linear(in_features=384, out_features=384, bias=True)
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
          )
          (attn2): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(384, 384, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=128, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
            )
          )
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp1): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
          (drop_path1): DropPath()
          (drop_path2): DropPath()
          (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (norm4): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp2): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
        )
        (2): Block(
          (pos_embed1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (pos_embed2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn1): BiLevelRoutingAttention(
            (lepe): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv): QKVLinear(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
            )
            (wo): Linear(in_features=384, out_features=384, bias=True)
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
          )
          (attn2): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(384, 384, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=128, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
            )
          )
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp1): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
          (drop_path1): DropPath()
          (drop_path2): DropPath()
          (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (norm4): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp2): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
        )
        (3): Block(
          (pos_embed1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (pos_embed2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn1): BiLevelRoutingAttention(
            (lepe): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv): QKVLinear(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
            )
            (wo): Linear(in_features=384, out_features=384, bias=True)
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
          )
          (attn2): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(384, 384, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=128, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
            )
          )
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp1): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
          (drop_path1): DropPath()
          (drop_path2): DropPath()
          (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (norm4): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp2): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
        )
        (4): Block(
          (pos_embed1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (pos_embed2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn1): BiLevelRoutingAttention(
            (lepe): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv): QKVLinear(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
            )
            (wo): Linear(in_features=384, out_features=384, bias=True)
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
          )
          (attn2): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(384, 384, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=128, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
            )
          )
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp1): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
          (drop_path1): DropPath()
          (drop_path2): DropPath()
          (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (norm4): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp2): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
        )
        (5): Block(
          (pos_embed1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (pos_embed2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn1): BiLevelRoutingAttention(
            (lepe): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv): QKVLinear(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
            )
            (wo): Linear(in_features=384, out_features=384, bias=True)
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
          )
          (attn2): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(384, 384, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=128, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
            )
          )
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp1): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
          (drop_path1): DropPath()
          (drop_path2): DropPath()
          (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (norm4): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp2): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
        )
        (6): Block(
          (pos_embed1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (pos_embed2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn1): BiLevelRoutingAttention(
            (lepe): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv): QKVLinear(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
            )
            (wo): Linear(in_features=384, out_features=384, bias=True)
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
          )
          (attn2): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(384, 384, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=128, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
            )
          )
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp1): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
          (drop_path1): DropPath()
          (drop_path2): DropPath()
          (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (norm4): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp2): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
        )
        (7): Block(
          (pos_embed1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (pos_embed2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn1): BiLevelRoutingAttention(
            (lepe): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv): QKVLinear(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
            )
            (wo): Linear(in_features=384, out_features=384, bias=True)
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
          )
          (attn2): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(384, 384, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=128, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
            )
          )
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp1): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
          (drop_path1): DropPath()
          (drop_path2): DropPath()
          (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (norm4): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp2): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
        )
        (8): Block(
          (pos_embed1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (pos_embed2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn1): BiLevelRoutingAttention(
            (lepe): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv): QKVLinear(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
            )
            (wo): Linear(in_features=384, out_features=384, bias=True)
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
          )
          (attn2): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(384, 384, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=384)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=128, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
            )
          )
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp1): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
          (drop_path1): DropPath()
          (drop_path2): DropPath()
          (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (norm4): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp2): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
          )
        )
      )
      (3): Sequential(
        (0): Block(
          (pos_embed1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
          (pos_embed2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn1): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(768, 2304, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(768, 2304, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(2304, 768, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304)
            )
          )
          (attn2): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(768, 2304, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(768, 2304, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(2304, 768, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304)
            )
          )
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp1): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(768, 2304, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(2304, 768, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304)
          )
          (drop_path1): DropPath()
          (drop_path2): DropPath()
          (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp2): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(768, 2304, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(2304, 768, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304)
          )
        )
        (1): Block(
          (pos_embed1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
          (pos_embed2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn1): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(768, 2304, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(768, 2304, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(2304, 768, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304)
            )
          )
          (attn2): DeBiLevelRoutingAttention(
            (lepe1): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768)
            (router): TopkRouting(
              (emb): Identity()
              (routing_act): Softmax(dim=-1)
            )
            (kv_gather): KVGather()
            (qkv_conv): QKVConv(
              (qkv): Conv2d(768, 2304, kernel_size=(1, 1), stride=(1, 1))
            )
            (kv_down): Identity()
            (attn_act): Softmax(dim=-1)
            (proj_q): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (proj_k): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (proj_v): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (proj_out): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (unifyheads1): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
            (conv_offset_q): Sequential(
              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
              (1): LayerNormProxy(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (2): GELU()
              (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): TransformerMLPWithConv(
              (linear1): Sequential(
                (0): Conv2d(768, 2304, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop1): Dropout(p=0.0, inplace=True)
              (act): GELU()
              (linear2): Sequential(
                (0): Conv2d(2304, 768, kernel_size=(1, 1), stride=(1, 1))
              )
              (drop2): Dropout(p=0.0, inplace=True)
              (dwc): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304)
            )
          )
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp1): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(768, 2304, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(2304, 768, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304)
          )
          (drop_path1): DropPath()
          (drop_path2): DropPath()
          (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp2): TransformerMLPWithConv(
            (linear1): Sequential(
              (0): Conv2d(768, 2304, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop1): Dropout(p=0.0, inplace=True)
            (act): GELU()
            (linear2): Sequential(
              (0): Conv2d(2304, 768, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop2): Dropout(p=0.0, inplace=True)
            (dwc): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304)
          )
        )
      )
    )
    (pre_logits): Identity()
    (extra_norms): ModuleList(
      (0): LayerNorm2d(
        (ln): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      )
      (1): LayerNorm2d(
        (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      )
      (2): LayerNorm2d(
        (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (3): LayerNorm2d(
        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (decode_head): FPNHead(
    input_transform=multiple_select, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)
    (conv_seg): Conv2d(128, 150, kernel_size=(1, 1), stride=(1, 1))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (scale_heads): ModuleList(
      (0): Sequential(
        (0): ConvModule(
          (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
      )
      (1): Sequential(
        (0): ConvModule(
          (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (1): Upsample()
      )
      (2): Sequential(
        (0): ConvModule(
          (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (1): Upsample()
        (2): ConvModule(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (3): Upsample()
      )
      (3): Sequential(
        (0): ConvModule(
          (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (1): Upsample()
        (2): ConvModule(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (3): Upsample()
        (4): ConvModule(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (5): Upsample()
      )
    )
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
2023-11-10 10:49:25,227 - mmseg - INFO - Loaded 20210 images
2023-11-10 10:49:26,407 - mmseg - INFO - Loaded 2000 images
2023-11-10 10:49:26,407 - mmseg - INFO - Start running, host: cuda11py3.9@1082ba884f56, work_dir: /data/Next-ViT/segmentation/work_dirs/fpn_512_debi_base_160k
2023-11-10 10:49:26,408 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(ABOVE_NORMAL) Fp16OptimizerHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) Fp16OptimizerHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-11-10 10:49:26,408 - mmseg - INFO - workflow: [('train', 1)], max: 160000 iters
2023-11-10 10:49:26,414 - mmseg - INFO - Checkpoints will be saved to /data/Next-ViT/segmentation/work_dirs/fpn_512_debi_base_160k by HardDiskBackend.
2023-11-10 10:50:25,962 - mmseg - INFO - Iter [50/160000]	lr: 1.147e-05, eta: 1 day, 16:26:13, time: 0.910, data_time: 0.010, memory: 23129, decode.loss_ce: 3.9932, decode.acc_seg: 10.4905, loss: 3.9932
2023-11-10 10:51:00,110 - mmseg - INFO - Iter [100/160000]	lr: 1.297e-05, eta: 1 day, 11:21:41, time: 0.682, data_time: 0.009, memory: 23129, decode.loss_ce: 3.5756, decode.acc_seg: 32.6020, loss: 3.5756
2023-11-10 10:51:35,558 - mmseg - INFO - Iter [150/160000]	lr: 1.447e-05, eta: 1 day, 10:03:28, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 3.1889, decode.acc_seg: 46.6169, loss: 3.1889
2023-11-10 10:52:09,884 - mmseg - INFO - Iter [200/160000]	lr: 1.597e-05, eta: 1 day, 9:09:57, time: 0.688, data_time: 0.010, memory: 23129, decode.loss_ce: 2.9020, decode.acc_seg: 48.1846, loss: 2.9020
2023-11-10 10:52:43,295 - mmseg - INFO - Iter [250/160000]	lr: 1.747e-05, eta: 1 day, 8:26:49, time: 0.667, data_time: 0.008, memory: 23129, decode.loss_ce: 2.5901, decode.acc_seg: 53.9373, loss: 2.5901
2023-11-10 10:53:19,089 - mmseg - INFO - Iter [300/160000]	lr: 1.897e-05, eta: 1 day, 8:19:25, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 2.4125, decode.acc_seg: 55.0836, loss: 2.4125
2023-11-10 10:53:52,767 - mmseg - INFO - Iter [350/160000]	lr: 2.047e-05, eta: 1 day, 7:58:13, time: 0.674, data_time: 0.010, memory: 23129, decode.loss_ce: 2.2479, decode.acc_seg: 54.0508, loss: 2.2479
2023-11-10 10:54:26,250 - mmseg - INFO - Iter [400/160000]	lr: 2.197e-05, eta: 1 day, 7:40:29, time: 0.669, data_time: 0.008, memory: 23129, decode.loss_ce: 2.1589, decode.acc_seg: 56.0236, loss: 2.1589
2023-11-10 10:54:59,060 - mmseg - INFO - Iter [450/160000]	lr: 2.347e-05, eta: 1 day, 7:22:42, time: 0.656, data_time: 0.008, memory: 23129, decode.loss_ce: 2.0258, decode.acc_seg: 57.1945, loss: 2.0258
2023-11-10 10:55:32,423 - mmseg - INFO - Iter [500/160000]	lr: 2.497e-05, eta: 1 day, 7:11:02, time: 0.666, data_time: 0.009, memory: 23129, decode.loss_ce: 1.9447, decode.acc_seg: 56.9196, loss: 1.9447
2023-11-10 10:56:05,790 - mmseg - INFO - Iter [550/160000]	lr: 2.647e-05, eta: 1 day, 7:01:51, time: 0.668, data_time: 0.010, memory: 23129, decode.loss_ce: 1.8954, decode.acc_seg: 56.7149, loss: 1.8954
2023-11-10 10:56:38,417 - mmseg - INFO - Iter [600/160000]	lr: 2.797e-05, eta: 1 day, 6:50:40, time: 0.653, data_time: 0.009, memory: 23129, decode.loss_ce: 1.7333, decode.acc_seg: 60.6701, loss: 1.7333
2023-11-10 10:57:11,193 - mmseg - INFO - Iter [650/160000]	lr: 2.947e-05, eta: 1 day, 6:41:28, time: 0.654, data_time: 0.008, memory: 23129, decode.loss_ce: 1.6945, decode.acc_seg: 59.8413, loss: 1.6945
2023-11-10 10:57:47,186 - mmseg - INFO - Iter [700/160000]	lr: 3.097e-05, eta: 1 day, 6:45:54, time: 0.720, data_time: 0.009, memory: 23129, decode.loss_ce: 1.6600, decode.acc_seg: 60.0198, loss: 1.6600
2023-11-10 10:58:23,385 - mmseg - INFO - Iter [750/160000]	lr: 3.247e-05, eta: 1 day, 6:50:24, time: 0.724, data_time: 0.009, memory: 23129, decode.loss_ce: 1.6832, decode.acc_seg: 59.5402, loss: 1.6832
2023-11-10 10:58:58,111 - mmseg - INFO - Iter [800/160000]	lr: 3.397e-05, eta: 1 day, 6:49:20, time: 0.694, data_time: 0.009, memory: 23129, decode.loss_ce: 1.6289, decode.acc_seg: 59.1068, loss: 1.6289
2023-11-10 10:59:33,275 - mmseg - INFO - Iter [850/160000]	lr: 3.547e-05, eta: 1 day, 6:49:48, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 1.4807, decode.acc_seg: 62.6931, loss: 1.4807
2023-11-10 11:00:08,319 - mmseg - INFO - Iter [900/160000]	lr: 3.697e-05, eta: 1 day, 6:49:50, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 1.4773, decode.acc_seg: 62.3067, loss: 1.4773
2023-11-10 11:00:43,689 - mmseg - INFO - Iter [950/160000]	lr: 3.847e-05, eta: 1 day, 6:50:28, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 1.4583, decode.acc_seg: 63.4182, loss: 1.4583
2023-11-10 11:01:19,203 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 11:01:19,204 - mmseg - INFO - Iter [1000/160000]	lr: 3.997e-05, eta: 1 day, 6:51:32, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 1.4267, decode.acc_seg: 63.1323, loss: 1.4267
2023-11-10 11:01:54,926 - mmseg - INFO - Iter [1050/160000]	lr: 4.147e-05, eta: 1 day, 6:52:56, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 1.3534, decode.acc_seg: 64.2766, loss: 1.3534
2023-11-10 11:02:29,708 - mmseg - INFO - Iter [1100/160000]	lr: 4.296e-05, eta: 1 day, 6:51:52, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 1.3435, decode.acc_seg: 63.7989, loss: 1.3435
2023-11-10 11:03:04,439 - mmseg - INFO - Iter [1150/160000]	lr: 4.446e-05, eta: 1 day, 6:50:47, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 1.3294, decode.acc_seg: 63.9381, loss: 1.3294
2023-11-10 11:03:39,877 - mmseg - INFO - Iter [1200/160000]	lr: 4.596e-05, eta: 1 day, 6:51:17, time: 0.709, data_time: 0.009, memory: 23129, decode.loss_ce: 1.3210, decode.acc_seg: 63.8889, loss: 1.3210
2023-11-10 11:04:15,266 - mmseg - INFO - Iter [1250/160000]	lr: 4.746e-05, eta: 1 day, 6:51:33, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 1.3477, decode.acc_seg: 63.1055, loss: 1.3477
2023-11-10 11:04:49,858 - mmseg - INFO - Iter [1300/160000]	lr: 4.896e-05, eta: 1 day, 6:50:08, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 1.2544, decode.acc_seg: 65.2812, loss: 1.2544
2023-11-10 11:05:25,616 - mmseg - INFO - Iter [1350/160000]	lr: 5.046e-05, eta: 1 day, 6:51:05, time: 0.715, data_time: 0.010, memory: 23129, decode.loss_ce: 1.2161, decode.acc_seg: 66.2398, loss: 1.2161
2023-11-10 11:06:00,256 - mmseg - INFO - Iter [1400/160000]	lr: 5.196e-05, eta: 1 day, 6:49:56, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 1.1883, decode.acc_seg: 67.4966, loss: 1.1883
2023-11-10 11:06:33,400 - mmseg - INFO - Iter [1450/160000]	lr: 5.346e-05, eta: 1 day, 6:45:58, time: 0.663, data_time: 0.009, memory: 23129, decode.loss_ce: 1.2009, decode.acc_seg: 67.3395, loss: 1.2009
2023-11-10 11:07:08,062 - mmseg - INFO - Iter [1500/160000]	lr: 5.496e-05, eta: 1 day, 6:44:51, time: 0.692, data_time: 0.008, memory: 23129, decode.loss_ce: 1.2156, decode.acc_seg: 65.7912, loss: 1.2156
2023-11-10 11:07:42,288 - mmseg - INFO - Iter [1550/160000]	lr: 5.646e-05, eta: 1 day, 6:43:09, time: 0.685, data_time: 0.010, memory: 23129, decode.loss_ce: 1.1355, decode.acc_seg: 66.6954, loss: 1.1355
2023-11-10 11:08:16,142 - mmseg - INFO - Iter [1600/160000]	lr: 5.796e-05, eta: 1 day, 6:40:46, time: 0.676, data_time: 0.009, memory: 23129, decode.loss_ce: 1.1292, decode.acc_seg: 67.0614, loss: 1.1292
2023-11-10 11:08:52,167 - mmseg - INFO - Iter [1650/160000]	lr: 5.945e-05, eta: 1 day, 6:42:02, time: 0.720, data_time: 0.010, memory: 23129, decode.loss_ce: 1.1015, decode.acc_seg: 67.5991, loss: 1.1015
2023-11-10 11:09:28,037 - mmseg - INFO - Iter [1700/160000]	lr: 6.095e-05, eta: 1 day, 6:42:58, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 1.1172, decode.acc_seg: 67.7205, loss: 1.1172
2023-11-10 11:10:03,912 - mmseg - INFO - Iter [1750/160000]	lr: 6.245e-05, eta: 1 day, 6:43:48, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 1.1833, decode.acc_seg: 65.2540, loss: 1.1833
2023-11-10 11:10:39,934 - mmseg - INFO - Iter [1800/160000]	lr: 6.395e-05, eta: 1 day, 6:44:47, time: 0.721, data_time: 0.010, memory: 23129, decode.loss_ce: 1.1446, decode.acc_seg: 66.5985, loss: 1.1446
2023-11-10 11:11:15,800 - mmseg - INFO - Iter [1850/160000]	lr: 6.545e-05, eta: 1 day, 6:45:28, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 1.1070, decode.acc_seg: 67.1248, loss: 1.1070
2023-11-10 11:11:50,648 - mmseg - INFO - Iter [1900/160000]	lr: 6.695e-05, eta: 1 day, 6:44:41, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 1.0812, decode.acc_seg: 67.2257, loss: 1.0812
2023-11-10 11:12:25,980 - mmseg - INFO - Iter [1950/160000]	lr: 6.844e-05, eta: 1 day, 6:44:33, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 1.0720, decode.acc_seg: 67.6116, loss: 1.0720
2023-11-10 11:13:00,553 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 11:13:00,553 - mmseg - INFO - Iter [2000/160000]	lr: 6.994e-05, eta: 1 day, 6:43:22, time: 0.691, data_time: 0.009, memory: 23129, decode.loss_ce: 1.0578, decode.acc_seg: 67.6392, loss: 1.0578
2023-11-10 11:13:35,688 - mmseg - INFO - Iter [2050/160000]	lr: 7.144e-05, eta: 1 day, 6:43:03, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 1.0963, decode.acc_seg: 67.0901, loss: 1.0963
2023-11-10 11:14:09,697 - mmseg - INFO - Iter [2100/160000]	lr: 7.294e-05, eta: 1 day, 6:41:08, time: 0.679, data_time: 0.008, memory: 23129, decode.loss_ce: 1.0583, decode.acc_seg: 67.9182, loss: 1.0583
2023-11-10 11:14:44,227 - mmseg - INFO - Iter [2150/160000]	lr: 7.444e-05, eta: 1 day, 6:39:59, time: 0.690, data_time: 0.009, memory: 23129, decode.loss_ce: 0.9885, decode.acc_seg: 69.7667, loss: 0.9885
2023-11-10 11:15:18,305 - mmseg - INFO - Iter [2200/160000]	lr: 7.593e-05, eta: 1 day, 6:38:24, time: 0.683, data_time: 0.010, memory: 23129, decode.loss_ce: 1.0348, decode.acc_seg: 68.8705, loss: 1.0348
2023-11-10 11:15:53,744 - mmseg - INFO - Iter [2250/160000]	lr: 7.743e-05, eta: 1 day, 6:38:20, time: 0.708, data_time: 0.009, memory: 23129, decode.loss_ce: 1.0315, decode.acc_seg: 68.0121, loss: 1.0315
2023-11-10 11:16:29,143 - mmseg - INFO - Iter [2300/160000]	lr: 7.893e-05, eta: 1 day, 6:38:18, time: 0.709, data_time: 0.009, memory: 23129, decode.loss_ce: 1.0015, decode.acc_seg: 68.4271, loss: 1.0015
2023-11-10 11:17:02,541 - mmseg - INFO - Iter [2350/160000]	lr: 8.043e-05, eta: 1 day, 6:35:54, time: 0.667, data_time: 0.008, memory: 23129, decode.loss_ce: 1.0071, decode.acc_seg: 67.9305, loss: 1.0071
2023-11-10 11:17:38,609 - mmseg - INFO - Iter [2400/160000]	lr: 8.192e-05, eta: 1 day, 6:36:34, time: 0.722, data_time: 0.010, memory: 23129, decode.loss_ce: 0.9889, decode.acc_seg: 67.8625, loss: 0.9889
2023-11-10 11:18:12,024 - mmseg - INFO - Iter [2450/160000]	lr: 8.342e-05, eta: 1 day, 6:34:23, time: 0.670, data_time: 0.010, memory: 23129, decode.loss_ce: 1.0275, decode.acc_seg: 68.8240, loss: 1.0275
2023-11-10 11:18:46,818 - mmseg - INFO - Iter [2500/160000]	lr: 8.492e-05, eta: 1 day, 6:33:36, time: 0.695, data_time: 0.009, memory: 23129, decode.loss_ce: 1.0310, decode.acc_seg: 67.4561, loss: 1.0310
2023-11-10 11:19:21,238 - mmseg - INFO - Iter [2550/160000]	lr: 8.642e-05, eta: 1 day, 6:32:30, time: 0.688, data_time: 0.009, memory: 23129, decode.loss_ce: 0.9354, decode.acc_seg: 70.2029, loss: 0.9354
2023-11-10 11:19:56,059 - mmseg - INFO - Iter [2600/160000]	lr: 8.791e-05, eta: 1 day, 6:31:49, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.9595, decode.acc_seg: 69.3741, loss: 0.9595
2023-11-10 11:20:32,140 - mmseg - INFO - Iter [2650/160000]	lr: 8.941e-05, eta: 1 day, 6:32:24, time: 0.722, data_time: 0.009, memory: 23129, decode.loss_ce: 0.9509, decode.acc_seg: 69.4739, loss: 0.9509
2023-11-10 11:21:07,857 - mmseg - INFO - Iter [2700/160000]	lr: 9.091e-05, eta: 1 day, 6:32:35, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.9692, decode.acc_seg: 68.9607, loss: 0.9692
2023-11-10 11:21:44,137 - mmseg - INFO - Iter [2750/160000]	lr: 9.240e-05, eta: 1 day, 6:33:16, time: 0.726, data_time: 0.009, memory: 23129, decode.loss_ce: 0.9215, decode.acc_seg: 70.5463, loss: 0.9215
2023-11-10 11:22:20,212 - mmseg - INFO - Iter [2800/160000]	lr: 9.390e-05, eta: 1 day, 6:33:41, time: 0.721, data_time: 0.009, memory: 23129, decode.loss_ce: 0.9315, decode.acc_seg: 70.4243, loss: 0.9315
2023-11-10 11:22:55,739 - mmseg - INFO - Iter [2850/160000]	lr: 9.540e-05, eta: 1 day, 6:33:37, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.9440, decode.acc_seg: 70.5132, loss: 0.9440
2023-11-10 11:23:28,190 - mmseg - INFO - Iter [2900/160000]	lr: 9.689e-05, eta: 1 day, 6:30:45, time: 0.650, data_time: 0.009, memory: 23129, decode.loss_ce: 0.9788, decode.acc_seg: 68.6145, loss: 0.9788
2023-11-10 11:24:00,931 - mmseg - INFO - Iter [2950/160000]	lr: 9.839e-05, eta: 1 day, 6:28:10, time: 0.654, data_time: 0.009, memory: 23129, decode.loss_ce: 0.9384, decode.acc_seg: 70.2123, loss: 0.9384
2023-11-10 11:24:36,010 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 11:24:36,011 - mmseg - INFO - Iter [3000/160000]	lr: 9.988e-05, eta: 1 day, 6:27:43, time: 0.701, data_time: 0.009, memory: 23129, decode.loss_ce: 0.9368, decode.acc_seg: 69.9251, loss: 0.9368
2023-11-10 11:25:11,268 - mmseg - INFO - Iter [3050/160000]	lr: 9.991e-05, eta: 1 day, 6:27:25, time: 0.705, data_time: 0.009, memory: 23129, decode.loss_ce: 0.9531, decode.acc_seg: 68.0205, loss: 0.9531
2023-11-10 11:25:46,757 - mmseg - INFO - Iter [3100/160000]	lr: 9.991e-05, eta: 1 day, 6:27:18, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.9183, decode.acc_seg: 69.7562, loss: 0.9183
2023-11-10 11:26:22,438 - mmseg - INFO - Iter [3150/160000]	lr: 9.990e-05, eta: 1 day, 6:27:21, time: 0.714, data_time: 0.009, memory: 23129, decode.loss_ce: 0.8796, decode.acc_seg: 71.9283, loss: 0.8796
2023-11-10 11:26:55,139 - mmseg - INFO - Iter [3200/160000]	lr: 9.990e-05, eta: 1 day, 6:24:58, time: 0.655, data_time: 0.010, memory: 23129, decode.loss_ce: 0.8535, decode.acc_seg: 71.4454, loss: 0.8535
2023-11-10 11:27:29,268 - mmseg - INFO - Iter [3250/160000]	lr: 9.990e-05, eta: 1 day, 6:23:42, time: 0.682, data_time: 0.009, memory: 23129, decode.loss_ce: 0.8722, decode.acc_seg: 70.7430, loss: 0.8722
2023-11-10 11:28:04,800 - mmseg - INFO - Iter [3300/160000]	lr: 9.990e-05, eta: 1 day, 6:23:37, time: 0.711, data_time: 0.009, memory: 23129, decode.loss_ce: 0.8832, decode.acc_seg: 70.7058, loss: 0.8832
2023-11-10 11:28:39,992 - mmseg - INFO - Iter [3350/160000]	lr: 9.989e-05, eta: 1 day, 6:23:16, time: 0.704, data_time: 0.009, memory: 23129, decode.loss_ce: 0.8712, decode.acc_seg: 71.9022, loss: 0.8712
2023-11-10 11:29:15,218 - mmseg - INFO - Iter [3400/160000]	lr: 9.989e-05, eta: 1 day, 6:22:55, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.8724, decode.acc_seg: 71.4927, loss: 0.8724
2023-11-10 11:29:50,369 - mmseg - INFO - Iter [3450/160000]	lr: 9.989e-05, eta: 1 day, 6:22:30, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.8255, decode.acc_seg: 71.5889, loss: 0.8255
2023-11-10 11:30:26,000 - mmseg - INFO - Iter [3500/160000]	lr: 9.988e-05, eta: 1 day, 6:22:27, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.8716, decode.acc_seg: 70.8551, loss: 0.8716
2023-11-10 11:31:01,538 - mmseg - INFO - Iter [3550/160000]	lr: 9.988e-05, eta: 1 day, 6:22:19, time: 0.711, data_time: 0.009, memory: 23129, decode.loss_ce: 0.8464, decode.acc_seg: 71.3558, loss: 0.8464
2023-11-10 11:31:33,269 - mmseg - INFO - Iter [3600/160000]	lr: 9.988e-05, eta: 1 day, 6:19:26, time: 0.635, data_time: 0.011, memory: 23129, decode.loss_ce: 0.8253, decode.acc_seg: 71.9637, loss: 0.8253
2023-11-10 11:32:05,469 - mmseg - INFO - Iter [3650/160000]	lr: 9.987e-05, eta: 1 day, 6:16:56, time: 0.644, data_time: 0.009, memory: 23129, decode.loss_ce: 0.8823, decode.acc_seg: 71.3493, loss: 0.8823
2023-11-10 11:32:38,747 - mmseg - INFO - Iter [3700/160000]	lr: 9.987e-05, eta: 1 day, 6:15:13, time: 0.665, data_time: 0.008, memory: 23129, decode.loss_ce: 0.8162, decode.acc_seg: 73.1821, loss: 0.8162
2023-11-10 11:33:13,119 - mmseg - INFO - Iter [3750/160000]	lr: 9.986e-05, eta: 1 day, 6:14:17, time: 0.687, data_time: 0.009, memory: 23129, decode.loss_ce: 0.8199, decode.acc_seg: 72.1729, loss: 0.8199
2023-11-10 11:33:45,326 - mmseg - INFO - Iter [3800/160000]	lr: 9.986e-05, eta: 1 day, 6:11:57, time: 0.645, data_time: 0.010, memory: 23129, decode.loss_ce: 0.8573, decode.acc_seg: 70.8868, loss: 0.8573
2023-11-10 11:34:17,880 - mmseg - INFO - Iter [3850/160000]	lr: 9.986e-05, eta: 1 day, 6:09:48, time: 0.650, data_time: 0.008, memory: 23129, decode.loss_ce: 0.8367, decode.acc_seg: 72.1739, loss: 0.8367
2023-11-10 11:34:53,723 - mmseg - INFO - Iter [3900/160000]	lr: 9.985e-05, eta: 1 day, 6:09:56, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 0.7645, decode.acc_seg: 73.9636, loss: 0.7645
2023-11-10 11:35:28,836 - mmseg - INFO - Iter [3950/160000]	lr: 9.985e-05, eta: 1 day, 6:09:34, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.8266, decode.acc_seg: 72.2066, loss: 0.8266
2023-11-10 11:36:04,812 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 11:36:04,812 - mmseg - INFO - Iter [4000/160000]	lr: 9.985e-05, eta: 1 day, 6:09:47, time: 0.720, data_time: 0.010, memory: 23129, decode.loss_ce: 0.8624, decode.acc_seg: 71.2932, loss: 0.8624
2023-11-10 11:36:40,827 - mmseg - INFO - Iter [4050/160000]	lr: 9.984e-05, eta: 1 day, 6:09:58, time: 0.720, data_time: 0.010, memory: 23129, decode.loss_ce: 0.8185, decode.acc_seg: 71.9894, loss: 0.8185
2023-11-10 11:37:15,369 - mmseg - INFO - Iter [4100/160000]	lr: 9.984e-05, eta: 1 day, 6:09:15, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.8372, decode.acc_seg: 71.7031, loss: 0.8372
2023-11-10 11:37:47,461 - mmseg - INFO - Iter [4150/160000]	lr: 9.983e-05, eta: 1 day, 6:06:58, time: 0.642, data_time: 0.008, memory: 23129, decode.loss_ce: 0.7943, decode.acc_seg: 73.1614, loss: 0.7943
2023-11-10 11:38:19,423 - mmseg - INFO - Iter [4200/160000]	lr: 9.983e-05, eta: 1 day, 6:04:38, time: 0.639, data_time: 0.008, memory: 23129, decode.loss_ce: 0.7614, decode.acc_seg: 73.1405, loss: 0.7614
2023-11-10 11:38:54,484 - mmseg - INFO - Iter [4250/160000]	lr: 9.983e-05, eta: 1 day, 6:04:13, time: 0.700, data_time: 0.008, memory: 23129, decode.loss_ce: 0.8290, decode.acc_seg: 71.4559, loss: 0.8290
2023-11-10 11:39:29,520 - mmseg - INFO - Iter [4300/160000]	lr: 9.982e-05, eta: 1 day, 6:03:49, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.7818, decode.acc_seg: 73.0219, loss: 0.7818
2023-11-10 11:40:03,786 - mmseg - INFO - Iter [4350/160000]	lr: 9.982e-05, eta: 1 day, 6:02:58, time: 0.686, data_time: 0.009, memory: 23129, decode.loss_ce: 0.8142, decode.acc_seg: 72.5978, loss: 0.8142
2023-11-10 11:40:37,289 - mmseg - INFO - Iter [4400/160000]	lr: 9.981e-05, eta: 1 day, 6:01:39, time: 0.670, data_time: 0.008, memory: 23129, decode.loss_ce: 0.8387, decode.acc_seg: 71.5878, loss: 0.8387
2023-11-10 11:41:11,818 - mmseg - INFO - Iter [4450/160000]	lr: 9.981e-05, eta: 1 day, 6:00:56, time: 0.690, data_time: 0.009, memory: 23129, decode.loss_ce: 0.8433, decode.acc_seg: 72.0517, loss: 0.8433
2023-11-10 11:41:46,344 - mmseg - INFO - Iter [4500/160000]	lr: 9.981e-05, eta: 1 day, 6:00:14, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.7389, decode.acc_seg: 73.8486, loss: 0.7389
2023-11-10 11:42:19,213 - mmseg - INFO - Iter [4550/160000]	lr: 9.980e-05, eta: 1 day, 5:58:35, time: 0.657, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7557, decode.acc_seg: 73.6717, loss: 0.7557
2023-11-10 11:42:52,222 - mmseg - INFO - Iter [4600/160000]	lr: 9.980e-05, eta: 1 day, 5:57:05, time: 0.661, data_time: 0.010, memory: 23129, decode.loss_ce: 0.7448, decode.acc_seg: 75.1359, loss: 0.7448
2023-11-10 11:43:24,771 - mmseg - INFO - Iter [4650/160000]	lr: 9.979e-05, eta: 1 day, 5:55:16, time: 0.650, data_time: 0.008, memory: 23129, decode.loss_ce: 0.7581, decode.acc_seg: 73.5006, loss: 0.7581
2023-11-10 11:43:59,893 - mmseg - INFO - Iter [4700/160000]	lr: 9.979e-05, eta: 1 day, 5:54:57, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7691, decode.acc_seg: 73.4760, loss: 0.7691
2023-11-10 11:44:34,520 - mmseg - INFO - Iter [4750/160000]	lr: 9.978e-05, eta: 1 day, 5:54:21, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.7597, decode.acc_seg: 73.7585, loss: 0.7597
2023-11-10 11:45:09,183 - mmseg - INFO - Iter [4800/160000]	lr: 9.978e-05, eta: 1 day, 5:53:46, time: 0.693, data_time: 0.009, memory: 23129, decode.loss_ce: 0.8077, decode.acc_seg: 72.3042, loss: 0.8077
2023-11-10 11:45:44,048 - mmseg - INFO - Iter [4850/160000]	lr: 9.977e-05, eta: 1 day, 5:53:17, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.7615, decode.acc_seg: 74.2362, loss: 0.7615
2023-11-10 11:46:19,142 - mmseg - INFO - Iter [4900/160000]	lr: 9.977e-05, eta: 1 day, 5:52:56, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.7585, decode.acc_seg: 74.2657, loss: 0.7585
2023-11-10 11:46:54,667 - mmseg - INFO - Iter [4950/160000]	lr: 9.976e-05, eta: 1 day, 5:52:47, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.7285, decode.acc_seg: 74.2530, loss: 0.7285
2023-11-10 11:47:30,180 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 11:47:30,180 - mmseg - INFO - Iter [5000/160000]	lr: 9.976e-05, eta: 1 day, 5:52:38, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.7623, decode.acc_seg: 73.9786, loss: 0.7623
2023-11-10 11:48:06,067 - mmseg - INFO - Iter [5050/160000]	lr: 9.975e-05, eta: 1 day, 5:52:40, time: 0.718, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7231, decode.acc_seg: 75.1995, loss: 0.7231
2023-11-10 11:48:41,801 - mmseg - INFO - Iter [5100/160000]	lr: 9.975e-05, eta: 1 day, 5:52:37, time: 0.715, data_time: 0.010, memory: 23129, decode.loss_ce: 0.7636, decode.acc_seg: 73.8532, loss: 0.7636
2023-11-10 11:49:17,158 - mmseg - INFO - Iter [5150/160000]	lr: 9.974e-05, eta: 1 day, 5:52:21, time: 0.707, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7189, decode.acc_seg: 73.5786, loss: 0.7189
2023-11-10 11:49:52,611 - mmseg - INFO - Iter [5200/160000]	lr: 9.974e-05, eta: 1 day, 5:52:08, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.7301, decode.acc_seg: 74.9857, loss: 0.7301
2023-11-10 11:50:27,732 - mmseg - INFO - Iter [5250/160000]	lr: 9.973e-05, eta: 1 day, 5:51:45, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.7429, decode.acc_seg: 73.9873, loss: 0.7429
2023-11-10 11:51:03,353 - mmseg - INFO - Iter [5300/160000]	lr: 9.973e-05, eta: 1 day, 5:51:35, time: 0.712, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7602, decode.acc_seg: 73.9531, loss: 0.7602
2023-11-10 11:51:38,839 - mmseg - INFO - Iter [5350/160000]	lr: 9.972e-05, eta: 1 day, 5:51:24, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.7443, decode.acc_seg: 74.7585, loss: 0.7443
2023-11-10 11:52:13,072 - mmseg - INFO - Iter [5400/160000]	lr: 9.972e-05, eta: 1 day, 5:50:33, time: 0.684, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7799, decode.acc_seg: 72.9635, loss: 0.7799
2023-11-10 11:52:46,794 - mmseg - INFO - Iter [5450/160000]	lr: 9.971e-05, eta: 1 day, 5:49:29, time: 0.675, data_time: 0.010, memory: 23129, decode.loss_ce: 0.7201, decode.acc_seg: 75.1117, loss: 0.7201
2023-11-10 11:53:22,565 - mmseg - INFO - Iter [5500/160000]	lr: 9.971e-05, eta: 1 day, 5:49:24, time: 0.715, data_time: 0.010, memory: 23129, decode.loss_ce: 0.7311, decode.acc_seg: 74.1514, loss: 0.7311
2023-11-10 11:53:57,668 - mmseg - INFO - Iter [5550/160000]	lr: 9.970e-05, eta: 1 day, 5:48:59, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7069, decode.acc_seg: 74.7052, loss: 0.7069
2023-11-10 11:54:30,768 - mmseg - INFO - Iter [5600/160000]	lr: 9.970e-05, eta: 1 day, 5:47:40, time: 0.663, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7281, decode.acc_seg: 74.5235, loss: 0.7281
2023-11-10 11:55:04,887 - mmseg - INFO - Iter [5650/160000]	lr: 9.969e-05, eta: 1 day, 5:46:47, time: 0.681, data_time: 0.008, memory: 23129, decode.loss_ce: 0.7335, decode.acc_seg: 74.1791, loss: 0.7335
2023-11-10 11:55:39,999 - mmseg - INFO - Iter [5700/160000]	lr: 9.969e-05, eta: 1 day, 5:46:23, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7712, decode.acc_seg: 73.2695, loss: 0.7712
2023-11-10 11:56:14,242 - mmseg - INFO - Iter [5750/160000]	lr: 9.968e-05, eta: 1 day, 5:45:35, time: 0.685, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7112, decode.acc_seg: 75.1186, loss: 0.7112
2023-11-10 11:56:49,994 - mmseg - INFO - Iter [5800/160000]	lr: 9.968e-05, eta: 1 day, 5:45:27, time: 0.715, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7369, decode.acc_seg: 74.8402, loss: 0.7369
2023-11-10 11:57:25,411 - mmseg - INFO - Iter [5850/160000]	lr: 9.967e-05, eta: 1 day, 5:45:10, time: 0.708, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7353, decode.acc_seg: 74.2069, loss: 0.7353
2023-11-10 11:57:59,430 - mmseg - INFO - Iter [5900/160000]	lr: 9.966e-05, eta: 1 day, 5:44:17, time: 0.681, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7054, decode.acc_seg: 75.1603, loss: 0.7054
2023-11-10 11:58:30,977 - mmseg - INFO - Iter [5950/160000]	lr: 9.966e-05, eta: 1 day, 5:42:21, time: 0.631, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6598, decode.acc_seg: 77.1122, loss: 0.6598
2023-11-10 11:59:03,054 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 11:59:03,054 - mmseg - INFO - Iter [6000/160000]	lr: 9.965e-05, eta: 1 day, 5:40:37, time: 0.641, data_time: 0.008, memory: 23129, decode.loss_ce: 0.7140, decode.acc_seg: 74.9672, loss: 0.7140
2023-11-10 11:59:38,127 - mmseg - INFO - Iter [6050/160000]	lr: 9.965e-05, eta: 1 day, 5:40:12, time: 0.701, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7152, decode.acc_seg: 74.7060, loss: 0.7152
2023-11-10 12:00:12,910 - mmseg - INFO - Iter [6100/160000]	lr: 9.964e-05, eta: 1 day, 5:39:41, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6670, decode.acc_seg: 75.6280, loss: 0.6670
2023-11-10 12:00:45,666 - mmseg - INFO - Iter [6150/160000]	lr: 9.964e-05, eta: 1 day, 5:38:16, time: 0.654, data_time: 0.008, memory: 23129, decode.loss_ce: 0.6936, decode.acc_seg: 75.5053, loss: 0.6936
2023-11-10 12:01:20,725 - mmseg - INFO - Iter [6200/160000]	lr: 9.963e-05, eta: 1 day, 5:37:52, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6971, decode.acc_seg: 74.5781, loss: 0.6971
2023-11-10 12:01:56,566 - mmseg - INFO - Iter [6250/160000]	lr: 9.962e-05, eta: 1 day, 5:37:45, time: 0.716, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7277, decode.acc_seg: 74.4516, loss: 0.7277
2023-11-10 12:02:31,416 - mmseg - INFO - Iter [6300/160000]	lr: 9.962e-05, eta: 1 day, 5:37:15, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.7282, decode.acc_seg: 74.5555, loss: 0.7282
2023-11-10 12:03:06,955 - mmseg - INFO - Iter [6350/160000]	lr: 9.961e-05, eta: 1 day, 5:37:01, time: 0.711, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7049, decode.acc_seg: 74.4826, loss: 0.7049
2023-11-10 12:03:42,348 - mmseg - INFO - Iter [6400/160000]	lr: 9.961e-05, eta: 1 day, 5:36:43, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6759, decode.acc_seg: 75.8454, loss: 0.6759
2023-11-10 12:04:17,277 - mmseg - INFO - Iter [6450/160000]	lr: 9.960e-05, eta: 1 day, 5:36:13, time: 0.699, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7075, decode.acc_seg: 74.3192, loss: 0.7075
2023-11-10 12:04:52,544 - mmseg - INFO - Iter [6500/160000]	lr: 9.959e-05, eta: 1 day, 5:35:53, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6752, decode.acc_seg: 75.0584, loss: 0.6752
2023-11-10 12:05:26,073 - mmseg - INFO - Iter [6550/160000]	lr: 9.959e-05, eta: 1 day, 5:34:50, time: 0.669, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6781, decode.acc_seg: 75.6083, loss: 0.6781
2023-11-10 12:06:01,126 - mmseg - INFO - Iter [6600/160000]	lr: 9.958e-05, eta: 1 day, 5:34:23, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6558, decode.acc_seg: 76.0095, loss: 0.6558
2023-11-10 12:06:36,983 - mmseg - INFO - Iter [6650/160000]	lr: 9.957e-05, eta: 1 day, 5:34:15, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6559, decode.acc_seg: 76.8359, loss: 0.6559
2023-11-10 12:07:11,941 - mmseg - INFO - Iter [6700/160000]	lr: 9.957e-05, eta: 1 day, 5:33:46, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6831, decode.acc_seg: 75.6176, loss: 0.6831
2023-11-10 12:07:46,244 - mmseg - INFO - Iter [6750/160000]	lr: 9.956e-05, eta: 1 day, 5:33:02, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6885, decode.acc_seg: 75.8558, loss: 0.6885
2023-11-10 12:08:21,807 - mmseg - INFO - Iter [6800/160000]	lr: 9.956e-05, eta: 1 day, 5:32:46, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6957, decode.acc_seg: 75.6581, loss: 0.6957
2023-11-10 12:08:57,230 - mmseg - INFO - Iter [6850/160000]	lr: 9.955e-05, eta: 1 day, 5:32:28, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6847, decode.acc_seg: 75.8354, loss: 0.6847
2023-11-10 12:09:31,047 - mmseg - INFO - Iter [6900/160000]	lr: 9.954e-05, eta: 1 day, 5:31:34, time: 0.677, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6773, decode.acc_seg: 75.3240, loss: 0.6773
2023-11-10 12:10:04,382 - mmseg - INFO - Iter [6950/160000]	lr: 9.954e-05, eta: 1 day, 5:30:29, time: 0.667, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6521, decode.acc_seg: 76.3983, loss: 0.6521
2023-11-10 12:10:37,638 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 12:10:37,638 - mmseg - INFO - Iter [7000/160000]	lr: 9.953e-05, eta: 1 day, 5:29:23, time: 0.665, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7008, decode.acc_seg: 75.5409, loss: 0.7008
2023-11-10 12:11:09,149 - mmseg - INFO - Iter [7050/160000]	lr: 9.952e-05, eta: 1 day, 5:27:39, time: 0.630, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6934, decode.acc_seg: 75.4002, loss: 0.6934
2023-11-10 12:11:40,857 - mmseg - INFO - Iter [7100/160000]	lr: 9.952e-05, eta: 1 day, 5:26:00, time: 0.634, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6967, decode.acc_seg: 74.7671, loss: 0.6967
2023-11-10 12:12:13,949 - mmseg - INFO - Iter [7150/160000]	lr: 9.951e-05, eta: 1 day, 5:24:51, time: 0.661, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6683, decode.acc_seg: 75.8810, loss: 0.6683
2023-11-10 12:12:47,673 - mmseg - INFO - Iter [7200/160000]	lr: 9.950e-05, eta: 1 day, 5:23:58, time: 0.675, data_time: 0.009, memory: 23129, decode.loss_ce: 0.7123, decode.acc_seg: 74.5308, loss: 0.7123
2023-11-10 12:13:22,100 - mmseg - INFO - Iter [7250/160000]	lr: 9.949e-05, eta: 1 day, 5:23:19, time: 0.689, data_time: 0.008, memory: 23129, decode.loss_ce: 0.6934, decode.acc_seg: 75.0869, loss: 0.6934
2023-11-10 12:13:56,026 - mmseg - INFO - Iter [7300/160000]	lr: 9.949e-05, eta: 1 day, 5:22:28, time: 0.677, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6707, decode.acc_seg: 76.1158, loss: 0.6707
2023-11-10 12:14:31,466 - mmseg - INFO - Iter [7350/160000]	lr: 9.948e-05, eta: 1 day, 5:22:11, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6440, decode.acc_seg: 75.4885, loss: 0.6440
2023-11-10 12:15:06,787 - mmseg - INFO - Iter [7400/160000]	lr: 9.947e-05, eta: 1 day, 5:21:50, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6709, decode.acc_seg: 75.9410, loss: 0.6709
2023-11-10 12:15:40,771 - mmseg - INFO - Iter [7450/160000]	lr: 9.947e-05, eta: 1 day, 5:21:04, time: 0.681, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6673, decode.acc_seg: 76.5003, loss: 0.6673
2023-11-10 12:16:12,724 - mmseg - INFO - Iter [7500/160000]	lr: 9.946e-05, eta: 1 day, 5:19:34, time: 0.639, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6683, decode.acc_seg: 76.3613, loss: 0.6683
2023-11-10 12:16:48,797 - mmseg - INFO - Iter [7550/160000]	lr: 9.945e-05, eta: 1 day, 5:19:28, time: 0.720, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6595, decode.acc_seg: 76.2917, loss: 0.6595
2023-11-10 12:17:24,791 - mmseg - INFO - Iter [7600/160000]	lr: 9.944e-05, eta: 1 day, 5:19:21, time: 0.720, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6534, decode.acc_seg: 76.5328, loss: 0.6534
2023-11-10 12:17:57,375 - mmseg - INFO - Iter [7650/160000]	lr: 9.944e-05, eta: 1 day, 5:18:06, time: 0.651, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6250, decode.acc_seg: 77.6182, loss: 0.6250
2023-11-10 12:18:32,835 - mmseg - INFO - Iter [7700/160000]	lr: 9.943e-05, eta: 1 day, 5:17:48, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6139, decode.acc_seg: 78.1010, loss: 0.6139
2023-11-10 12:19:08,202 - mmseg - INFO - Iter [7750/160000]	lr: 9.942e-05, eta: 1 day, 5:17:28, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6600, decode.acc_seg: 76.0779, loss: 0.6600
2023-11-10 12:19:43,416 - mmseg - INFO - Iter [7800/160000]	lr: 9.941e-05, eta: 1 day, 5:17:05, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6565, decode.acc_seg: 76.9504, loss: 0.6565
2023-11-10 12:20:18,596 - mmseg - INFO - Iter [7850/160000]	lr: 9.941e-05, eta: 1 day, 5:16:40, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6751, decode.acc_seg: 76.7193, loss: 0.6751
2023-11-10 12:20:53,661 - mmseg - INFO - Iter [7900/160000]	lr: 9.940e-05, eta: 1 day, 5:16:14, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6881, decode.acc_seg: 75.3879, loss: 0.6881
2023-11-10 12:21:27,384 - mmseg - INFO - Iter [7950/160000]	lr: 9.939e-05, eta: 1 day, 5:15:22, time: 0.674, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6214, decode.acc_seg: 76.8782, loss: 0.6214
2023-11-10 12:22:02,580 - mmseg - INFO - Saving checkpoint at 8000 iterations
2023-11-10 12:22:07,079 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 12:22:07,079 - mmseg - INFO - Iter [8000/160000]	lr: 9.938e-05, eta: 1 day, 5:16:24, time: 0.795, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6186, decode.acc_seg: 77.5774, loss: 0.6186
2023-11-10 12:26:18,188 - mmseg - INFO - per class results:
2023-11-10 12:26:18,202 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 71.55 | 88.07 |
|       building      | 78.79 | 88.53 |
|         sky         | 93.15 | 97.41 |
|        floor        | 77.66 | 88.82 |
|         tree        |  72.0 | 84.98 |
|       ceiling       | 79.53 |  89.9 |
|         road        |  76.1 | 90.13 |
|         bed         | 82.81 | 92.49 |
|      windowpane     | 55.23 | 80.61 |
|        grass        | 64.46 | 86.95 |
|       cabinet       | 52.42 | 62.37 |
|       sidewalk      | 53.77 | 68.33 |
|        person       |  77.5 | 89.68 |
|        earth        | 33.57 |  43.8 |
|         door        | 33.75 | 40.65 |
|        table        | 46.93 | 68.96 |
|       mountain      | 45.43 | 51.61 |
|        plant        | 48.65 | 58.17 |
|       curtain       | 66.51 | 82.17 |
|        chair        | 46.54 | 69.31 |
|         car         | 80.27 | 88.43 |
|        water        | 51.79 |  62.9 |
|       painting      | 62.12 | 78.41 |
|         sofa        | 58.44 | 77.68 |
|        shelf        | 31.64 | 41.58 |
|        house        | 42.39 | 78.43 |
|         sea         | 62.96 | 87.95 |
|        mirror       | 44.53 | 47.31 |
|         rug         | 59.28 | 72.71 |
|        field        | 29.06 | 45.36 |
|       armchair      | 23.99 | 33.33 |
|         seat        | 57.66 | 75.83 |
|        fence        | 21.72 |  31.1 |
|         desk        | 33.93 | 50.42 |
|         rock        | 54.46 | 67.22 |
|       wardrobe      | 42.07 | 53.24 |
|         lamp        |  49.0 | 65.12 |
|       bathtub       | 55.06 | 69.92 |
|       railing       | 31.11 | 40.11 |
|       cushion       | 46.67 | 65.41 |
|         base        | 11.33 | 14.03 |
|         box         | 19.32 | 27.33 |
|        column       | 38.22 | 53.17 |
|      signboard      | 33.94 | 44.84 |
|   chest of drawers  | 33.73 | 62.16 |
|       counter       | 24.52 | 37.98 |
|         sand        | 29.07 | 38.11 |
|         sink        | 52.55 | 72.07 |
|      skyscraper     |  42.7 | 46.89 |
|      fireplace      | 47.59 | 52.37 |
|     refrigerator    | 59.45 | 77.58 |
|      grandstand     | 36.04 | 66.76 |
|         path        | 18.65 | 32.07 |
|        stairs       | 17.59 |  20.7 |
|        runway       | 63.57 |  75.1 |
|         case        | 57.03 | 80.84 |
|      pool table     | 90.06 | 92.21 |
|        pillow       |  46.7 | 59.23 |
|     screen door     | 56.54 | 70.59 |
|       stairway      | 19.26 | 39.36 |
|        river        | 10.63 |  27.6 |
|        bridge       | 25.53 | 30.66 |
|       bookcase      | 30.76 | 52.82 |
|        blind        |  6.41 |  6.56 |
|     coffee table    | 42.19 | 79.59 |
|        toilet       | 64.33 | 86.63 |
|        flower       | 25.62 | 40.21 |
|         book        | 35.34 | 48.31 |
|         hill        |  4.19 |  5.21 |
|        bench        | 37.66 | 43.97 |
|      countertop     | 43.17 | 59.28 |
|        stove        | 53.27 | 67.71 |
|         palm        | 46.77 | 58.39 |
|    kitchen island   |  26.0 |  71.6 |
|       computer      | 62.42 |  80.4 |
|     swivel chair    | 36.21 | 61.38 |
|         boat        | 50.98 | 60.28 |
|         bar         | 14.04 | 16.15 |
|    arcade machine   |  70.6 | 79.35 |
|        hovel        | 57.81 | 74.63 |
|         bus         |  72.6 | 95.76 |
|        towel        |  47.1 | 57.69 |
|        light        |  25.4 | 27.78 |
|        truck        | 25.71 | 46.43 |
|        tower        |  1.42 |  1.42 |
|      chandelier     | 52.76 | 74.88 |
|        awning       | 21.81 | 30.42 |
|     streetlight     | 13.05 | 15.36 |
|        booth        | 27.19 | 30.15 |
| television receiver | 55.66 | 68.13 |
|       airplane      | 52.92 |  66.5 |
|      dirt track     |  0.0  |  0.0  |
|       apparel       | 25.06 | 31.02 |
|         pole        | 11.88 |  13.3 |
|         land        |  0.0  |  0.0  |
|      bannister      |  5.28 |  5.66 |
|      escalator      |  4.13 |  4.16 |
|       ottoman       | 18.64 | 22.82 |
|        bottle       | 14.17 | 15.83 |
|        buffet       | 30.19 |  35.4 |
|        poster       |  0.06 |  0.06 |
|        stage        |  4.55 |  6.53 |
|         van         | 14.34 | 15.88 |
|         ship        |  15.6 | 20.39 |
|       fountain      | 30.21 | 47.91 |
|    conveyer belt    | 62.44 | 85.28 |
|        canopy       |  0.84 |  0.89 |
|        washer       | 68.02 | 75.31 |
|      plaything      |  8.2  |  9.67 |
|    swimming pool    | 61.26 | 74.45 |
|        stool        |  7.75 |  8.17 |
|        barrel       | 44.55 |  62.0 |
|        basket       | 25.49 | 31.25 |
|      waterfall      | 70.93 | 94.97 |
|         tent        | 79.04 | 99.07 |
|         bag         |  0.09 |  0.09 |
|       minibike      | 59.26 | 84.05 |
|        cradle       | 73.88 | 92.85 |
|         oven        |  7.96 |  9.89 |
|         ball        | 42.79 | 63.88 |
|         food        |  46.3 |  59.5 |
|         step        |  0.01 |  0.01 |
|         tank        | 31.74 | 31.82 |
|      trade name     | 10.64 | 11.24 |
|      microwave      | 72.87 | 82.09 |
|         pot         | 35.33 | 43.91 |
|        animal       | 55.62 | 57.32 |
|       bicycle       | 52.59 |  68.5 |
|         lake        |  0.0  |  0.0  |
|      dishwasher     | 33.51 | 44.09 |
|        screen       | 60.61 | 81.99 |
|       blanket       |  0.0  |  0.0  |
|      sculpture      | 30.98 | 37.69 |
|         hood        | 30.17 | 50.49 |
|        sconce       |  0.11 |  0.11 |
|         vase        | 21.55 | 32.83 |
|    traffic light    |  7.54 |  8.2  |
|         tray        |  0.03 |  0.03 |
|        ashcan       |  30.4 | 44.74 |
|         fan         | 37.15 | 47.83 |
|         pier        | 21.45 | 21.82 |
|      crt screen     |  1.11 |  3.59 |
|        plate        | 40.68 | 67.34 |
|       monitor       |  0.0  |  0.0  |
|    bulletin board   | 48.51 | 54.02 |
|        shower       |  0.0  |  0.0  |
|       radiator      | 31.25 | 32.08 |
|        glass        |  1.48 |  1.5  |
|        clock        | 15.56 |  16.7 |
|         flag        | 35.05 | 38.66 |
+---------------------+-------+-------+
2023-11-10 12:26:18,202 - mmseg - INFO - Summary:
2023-11-10 12:26:18,202 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 78.84 | 37.81 | 48.89 |
+-------+-------+-------+
2023-11-10 12:26:22,045 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_8000.pth.
2023-11-10 12:26:22,045 - mmseg - INFO - Best mIoU is 0.3781 at 8000 iter.
2023-11-10 12:26:22,075 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 12:26:22,075 - mmseg - INFO - Iter(val) [250]	aAcc: 0.7884, mIoU: 0.3781, mAcc: 0.4889, IoU.wall: 0.7155, IoU.building: 0.7879, IoU.sky: 0.9315, IoU.floor: 0.7766, IoU.tree: 0.7200, IoU.ceiling: 0.7953, IoU.road: 0.7610, IoU.bed : 0.8281, IoU.windowpane: 0.5523, IoU.grass: 0.6446, IoU.cabinet: 0.5242, IoU.sidewalk: 0.5377, IoU.person: 0.7750, IoU.earth: 0.3357, IoU.door: 0.3375, IoU.table: 0.4693, IoU.mountain: 0.4543, IoU.plant: 0.4865, IoU.curtain: 0.6651, IoU.chair: 0.4654, IoU.car: 0.8027, IoU.water: 0.5179, IoU.painting: 0.6212, IoU.sofa: 0.5844, IoU.shelf: 0.3164, IoU.house: 0.4239, IoU.sea: 0.6296, IoU.mirror: 0.4453, IoU.rug: 0.5928, IoU.field: 0.2906, IoU.armchair: 0.2399, IoU.seat: 0.5766, IoU.fence: 0.2172, IoU.desk: 0.3393, IoU.rock: 0.5446, IoU.wardrobe: 0.4207, IoU.lamp: 0.4900, IoU.bathtub: 0.5506, IoU.railing: 0.3111, IoU.cushion: 0.4667, IoU.base: 0.1133, IoU.box: 0.1932, IoU.column: 0.3822, IoU.signboard: 0.3394, IoU.chest of drawers: 0.3373, IoU.counter: 0.2452, IoU.sand: 0.2907, IoU.sink: 0.5255, IoU.skyscraper: 0.4270, IoU.fireplace: 0.4759, IoU.refrigerator: 0.5945, IoU.grandstand: 0.3604, IoU.path: 0.1865, IoU.stairs: 0.1759, IoU.runway: 0.6357, IoU.case: 0.5703, IoU.pool table: 0.9006, IoU.pillow: 0.4670, IoU.screen door: 0.5654, IoU.stairway: 0.1926, IoU.river: 0.1063, IoU.bridge: 0.2553, IoU.bookcase: 0.3076, IoU.blind: 0.0641, IoU.coffee table: 0.4219, IoU.toilet: 0.6433, IoU.flower: 0.2562, IoU.book: 0.3534, IoU.hill: 0.0419, IoU.bench: 0.3766, IoU.countertop: 0.4317, IoU.stove: 0.5327, IoU.palm: 0.4677, IoU.kitchen island: 0.2600, IoU.computer: 0.6242, IoU.swivel chair: 0.3621, IoU.boat: 0.5098, IoU.bar: 0.1404, IoU.arcade machine: 0.7060, IoU.hovel: 0.5781, IoU.bus: 0.7260, IoU.towel: 0.4710, IoU.light: 0.2540, IoU.truck: 0.2571, IoU.tower: 0.0142, IoU.chandelier: 0.5276, IoU.awning: 0.2181, IoU.streetlight: 0.1305, IoU.booth: 0.2719, IoU.television receiver: 0.5566, IoU.airplane: 0.5292, IoU.dirt track: 0.0000, IoU.apparel: 0.2506, IoU.pole: 0.1188, IoU.land: 0.0000, IoU.bannister: 0.0528, IoU.escalator: 0.0413, IoU.ottoman: 0.1864, IoU.bottle: 0.1417, IoU.buffet: 0.3019, IoU.poster: 0.0006, IoU.stage: 0.0455, IoU.van: 0.1434, IoU.ship: 0.1560, IoU.fountain: 0.3021, IoU.conveyer belt: 0.6244, IoU.canopy: 0.0084, IoU.washer: 0.6802, IoU.plaything: 0.0820, IoU.swimming pool: 0.6126, IoU.stool: 0.0775, IoU.barrel: 0.4455, IoU.basket: 0.2549, IoU.waterfall: 0.7093, IoU.tent: 0.7904, IoU.bag: 0.0009, IoU.minibike: 0.5926, IoU.cradle: 0.7388, IoU.oven: 0.0796, IoU.ball: 0.4279, IoU.food: 0.4630, IoU.step: 0.0001, IoU.tank: 0.3174, IoU.trade name: 0.1064, IoU.microwave: 0.7287, IoU.pot: 0.3533, IoU.animal: 0.5562, IoU.bicycle: 0.5259, IoU.lake: 0.0000, IoU.dishwasher: 0.3351, IoU.screen: 0.6061, IoU.blanket: 0.0000, IoU.sculpture: 0.3098, IoU.hood: 0.3017, IoU.sconce: 0.0011, IoU.vase: 0.2155, IoU.traffic light: 0.0754, IoU.tray: 0.0003, IoU.ashcan: 0.3040, IoU.fan: 0.3715, IoU.pier: 0.2145, IoU.crt screen: 0.0111, IoU.plate: 0.4068, IoU.monitor: 0.0000, IoU.bulletin board: 0.4851, IoU.shower: 0.0000, IoU.radiator: 0.3125, IoU.glass: 0.0148, IoU.clock: 0.1556, IoU.flag: 0.3505, Acc.wall: 0.8807, Acc.building: 0.8853, Acc.sky: 0.9741, Acc.floor: 0.8882, Acc.tree: 0.8498, Acc.ceiling: 0.8990, Acc.road: 0.9013, Acc.bed : 0.9249, Acc.windowpane: 0.8061, Acc.grass: 0.8695, Acc.cabinet: 0.6237, Acc.sidewalk: 0.6833, Acc.person: 0.8968, Acc.earth: 0.4380, Acc.door: 0.4065, Acc.table: 0.6896, Acc.mountain: 0.5161, Acc.plant: 0.5817, Acc.curtain: 0.8217, Acc.chair: 0.6931, Acc.car: 0.8843, Acc.water: 0.6290, Acc.painting: 0.7841, Acc.sofa: 0.7768, Acc.shelf: 0.4158, Acc.house: 0.7843, Acc.sea: 0.8795, Acc.mirror: 0.4731, Acc.rug: 0.7271, Acc.field: 0.4536, Acc.armchair: 0.3333, Acc.seat: 0.7583, Acc.fence: 0.3110, Acc.desk: 0.5042, Acc.rock: 0.6722, Acc.wardrobe: 0.5324, Acc.lamp: 0.6512, Acc.bathtub: 0.6992, Acc.railing: 0.4011, Acc.cushion: 0.6541, Acc.base: 0.1403, Acc.box: 0.2733, Acc.column: 0.5317, Acc.signboard: 0.4484, Acc.chest of drawers: 0.6216, Acc.counter: 0.3798, Acc.sand: 0.3811, Acc.sink: 0.7207, Acc.skyscraper: 0.4689, Acc.fireplace: 0.5237, Acc.refrigerator: 0.7758, Acc.grandstand: 0.6676, Acc.path: 0.3207, Acc.stairs: 0.2070, Acc.runway: 0.7510, Acc.case: 0.8084, Acc.pool table: 0.9221, Acc.pillow: 0.5923, Acc.screen door: 0.7059, Acc.stairway: 0.3936, Acc.river: 0.2760, Acc.bridge: 0.3066, Acc.bookcase: 0.5282, Acc.blind: 0.0656, Acc.coffee table: 0.7959, Acc.toilet: 0.8663, Acc.flower: 0.4021, Acc.book: 0.4831, Acc.hill: 0.0521, Acc.bench: 0.4397, Acc.countertop: 0.5928, Acc.stove: 0.6771, Acc.palm: 0.5839, Acc.kitchen island: 0.7160, Acc.computer: 0.8040, Acc.swivel chair: 0.6138, Acc.boat: 0.6028, Acc.bar: 0.1615, Acc.arcade machine: 0.7935, Acc.hovel: 0.7463, Acc.bus: 0.9576, Acc.towel: 0.5769, Acc.light: 0.2778, Acc.truck: 0.4643, Acc.tower: 0.0142, Acc.chandelier: 0.7488, Acc.awning: 0.3042, Acc.streetlight: 0.1536, Acc.booth: 0.3015, Acc.television receiver: 0.6813, Acc.airplane: 0.6650, Acc.dirt track: 0.0000, Acc.apparel: 0.3102, Acc.pole: 0.1330, Acc.land: 0.0000, Acc.bannister: 0.0566, Acc.escalator: 0.0416, Acc.ottoman: 0.2282, Acc.bottle: 0.1583, Acc.buffet: 0.3540, Acc.poster: 0.0006, Acc.stage: 0.0653, Acc.van: 0.1588, Acc.ship: 0.2039, Acc.fountain: 0.4791, Acc.conveyer belt: 0.8528, Acc.canopy: 0.0089, Acc.washer: 0.7531, Acc.plaything: 0.0967, Acc.swimming pool: 0.7445, Acc.stool: 0.0817, Acc.barrel: 0.6200, Acc.basket: 0.3125, Acc.waterfall: 0.9497, Acc.tent: 0.9907, Acc.bag: 0.0009, Acc.minibike: 0.8405, Acc.cradle: 0.9285, Acc.oven: 0.0989, Acc.ball: 0.6388, Acc.food: 0.5950, Acc.step: 0.0001, Acc.tank: 0.3182, Acc.trade name: 0.1124, Acc.microwave: 0.8209, Acc.pot: 0.4391, Acc.animal: 0.5732, Acc.bicycle: 0.6850, Acc.lake: 0.0000, Acc.dishwasher: 0.4409, Acc.screen: 0.8199, Acc.blanket: 0.0000, Acc.sculpture: 0.3769, Acc.hood: 0.5049, Acc.sconce: 0.0011, Acc.vase: 0.3283, Acc.traffic light: 0.0820, Acc.tray: 0.0003, Acc.ashcan: 0.4474, Acc.fan: 0.4783, Acc.pier: 0.2182, Acc.crt screen: 0.0359, Acc.plate: 0.6734, Acc.monitor: 0.0000, Acc.bulletin board: 0.5402, Acc.shower: 0.0000, Acc.radiator: 0.3208, Acc.glass: 0.0150, Acc.clock: 0.1670, Acc.flag: 0.3866
2023-11-10 12:26:56,564 - mmseg - INFO - Iter [8050/160000]	lr: 9.938e-05, eta: 1 day, 6:35:58, time: 5.788, data_time: 5.109, memory: 23129, decode.loss_ce: 0.6676, decode.acc_seg: 76.2195, loss: 0.6676
2023-11-10 12:27:31,527 - mmseg - INFO - Iter [8100/160000]	lr: 9.937e-05, eta: 1 day, 6:34:59, time: 0.701, data_time: 0.011, memory: 23129, decode.loss_ce: 0.6458, decode.acc_seg: 77.2153, loss: 0.6458
2023-11-10 12:28:04,549 - mmseg - INFO - Iter [8150/160000]	lr: 9.936e-05, eta: 1 day, 6:33:23, time: 0.660, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6791, decode.acc_seg: 75.9078, loss: 0.6791
2023-11-10 12:28:37,548 - mmseg - INFO - Iter [8200/160000]	lr: 9.935e-05, eta: 1 day, 6:31:46, time: 0.659, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6115, decode.acc_seg: 77.1872, loss: 0.6115
2023-11-10 12:29:11,580 - mmseg - INFO - Iter [8250/160000]	lr: 9.935e-05, eta: 1 day, 6:30:31, time: 0.682, data_time: 0.011, memory: 23129, decode.loss_ce: 0.6370, decode.acc_seg: 77.1656, loss: 0.6370
2023-11-10 12:29:44,422 - mmseg - INFO - Iter [8300/160000]	lr: 9.934e-05, eta: 1 day, 6:28:54, time: 0.657, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6409, decode.acc_seg: 77.0428, loss: 0.6409
2023-11-10 12:30:18,033 - mmseg - INFO - Iter [8350/160000]	lr: 9.933e-05, eta: 1 day, 6:27:30, time: 0.671, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6731, decode.acc_seg: 75.8376, loss: 0.6731
2023-11-10 12:30:51,610 - mmseg - INFO - Iter [8400/160000]	lr: 9.932e-05, eta: 1 day, 6:26:08, time: 0.672, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6933, decode.acc_seg: 74.6394, loss: 0.6933
2023-11-10 12:31:23,787 - mmseg - INFO - Iter [8450/160000]	lr: 9.931e-05, eta: 1 day, 6:24:21, time: 0.644, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6429, decode.acc_seg: 77.3045, loss: 0.6429
2023-11-10 12:31:55,673 - mmseg - INFO - Iter [8500/160000]	lr: 9.931e-05, eta: 1 day, 6:22:30, time: 0.638, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6381, decode.acc_seg: 77.2020, loss: 0.6381
2023-11-10 12:32:27,303 - mmseg - INFO - Iter [8550/160000]	lr: 9.930e-05, eta: 1 day, 6:20:34, time: 0.632, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6580, decode.acc_seg: 76.1057, loss: 0.6580
2023-11-10 12:33:01,207 - mmseg - INFO - Iter [8600/160000]	lr: 9.929e-05, eta: 1 day, 6:19:21, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6422, decode.acc_seg: 76.8264, loss: 0.6422
2023-11-10 12:33:34,330 - mmseg - INFO - Iter [8650/160000]	lr: 9.928e-05, eta: 1 day, 6:17:52, time: 0.661, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6340, decode.acc_seg: 76.8000, loss: 0.6340
2023-11-10 12:34:10,071 - mmseg - INFO - Iter [8700/160000]	lr: 9.927e-05, eta: 1 day, 6:17:11, time: 0.715, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6457, decode.acc_seg: 76.5062, loss: 0.6457
2023-11-10 12:34:45,730 - mmseg - INFO - Iter [8750/160000]	lr: 9.926e-05, eta: 1 day, 6:16:29, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6425, decode.acc_seg: 76.9481, loss: 0.6425
2023-11-10 12:35:20,992 - mmseg - INFO - Iter [8800/160000]	lr: 9.926e-05, eta: 1 day, 6:15:39, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6284, decode.acc_seg: 77.0531, loss: 0.6284
2023-11-10 12:35:56,280 - mmseg - INFO - Iter [8850/160000]	lr: 9.925e-05, eta: 1 day, 6:14:51, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6233, decode.acc_seg: 77.1683, loss: 0.6233
2023-11-10 12:36:29,836 - mmseg - INFO - Iter [8900/160000]	lr: 9.924e-05, eta: 1 day, 6:13:33, time: 0.671, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6167, decode.acc_seg: 77.8534, loss: 0.6167
2023-11-10 12:37:04,447 - mmseg - INFO - Iter [8950/160000]	lr: 9.923e-05, eta: 1 day, 6:12:33, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6017, decode.acc_seg: 78.7189, loss: 0.6017
2023-11-10 12:37:39,216 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 12:37:39,217 - mmseg - INFO - Iter [9000/160000]	lr: 9.922e-05, eta: 1 day, 6:11:38, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6941, decode.acc_seg: 75.0406, loss: 0.6941
2023-11-10 12:38:13,504 - mmseg - INFO - Iter [9050/160000]	lr: 9.921e-05, eta: 1 day, 6:10:32, time: 0.685, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6847, decode.acc_seg: 75.4490, loss: 0.6847
2023-11-10 12:38:48,213 - mmseg - INFO - Iter [9100/160000]	lr: 9.920e-05, eta: 1 day, 6:09:35, time: 0.694, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6475, decode.acc_seg: 76.7760, loss: 0.6475
2023-11-10 12:39:23,130 - mmseg - INFO - Iter [9150/160000]	lr: 9.920e-05, eta: 1 day, 6:08:42, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6125, decode.acc_seg: 77.5278, loss: 0.6125
2023-11-10 12:39:58,085 - mmseg - INFO - Iter [9200/160000]	lr: 9.919e-05, eta: 1 day, 6:07:49, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6240, decode.acc_seg: 77.4828, loss: 0.6240
2023-11-10 12:40:30,751 - mmseg - INFO - Iter [9250/160000]	lr: 9.918e-05, eta: 1 day, 6:06:20, time: 0.654, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6365, decode.acc_seg: 76.9291, loss: 0.6365
2023-11-10 12:41:02,087 - mmseg - INFO - Iter [9300/160000]	lr: 9.917e-05, eta: 1 day, 6:04:29, time: 0.626, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6339, decode.acc_seg: 76.4470, loss: 0.6339
2023-11-10 12:41:38,077 - mmseg - INFO - Iter [9350/160000]	lr: 9.916e-05, eta: 1 day, 6:03:54, time: 0.719, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5987, decode.acc_seg: 78.5035, loss: 0.5987
2023-11-10 12:42:15,358 - mmseg - INFO - Iter [9400/160000]	lr: 9.915e-05, eta: 1 day, 6:03:40, time: 0.746, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6349, decode.acc_seg: 76.9528, loss: 0.6349
2023-11-10 12:42:52,119 - mmseg - INFO - Iter [9450/160000]	lr: 9.914e-05, eta: 1 day, 6:03:17, time: 0.735, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6304, decode.acc_seg: 77.8768, loss: 0.6304
2023-11-10 12:43:27,639 - mmseg - INFO - Iter [9500/160000]	lr: 9.913e-05, eta: 1 day, 6:02:35, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6285, decode.acc_seg: 77.2389, loss: 0.6285
2023-11-10 12:44:03,059 - mmseg - INFO - Iter [9550/160000]	lr: 9.912e-05, eta: 1 day, 6:01:51, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5869, decode.acc_seg: 78.5637, loss: 0.5869
2023-11-10 12:44:37,632 - mmseg - INFO - Iter [9600/160000]	lr: 9.911e-05, eta: 1 day, 6:00:53, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6413, decode.acc_seg: 77.1912, loss: 0.6413
2023-11-10 12:45:12,678 - mmseg - INFO - Iter [9650/160000]	lr: 9.911e-05, eta: 1 day, 6:00:04, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6048, decode.acc_seg: 78.2493, loss: 0.6048
2023-11-10 12:45:48,274 - mmseg - INFO - Iter [9700/160000]	lr: 9.910e-05, eta: 1 day, 5:59:23, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6115, decode.acc_seg: 78.0467, loss: 0.6115
2023-11-10 12:46:23,619 - mmseg - INFO - Iter [9750/160000]	lr: 9.909e-05, eta: 1 day, 5:58:38, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6195, decode.acc_seg: 77.5944, loss: 0.6195
2023-11-10 12:46:59,046 - mmseg - INFO - Iter [9800/160000]	lr: 9.908e-05, eta: 1 day, 5:57:55, time: 0.708, data_time: 0.011, memory: 23129, decode.loss_ce: 0.6271, decode.acc_seg: 77.3427, loss: 0.6271
2023-11-10 12:47:34,345 - mmseg - INFO - Iter [9850/160000]	lr: 9.907e-05, eta: 1 day, 5:57:09, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6475, decode.acc_seg: 76.7663, loss: 0.6475
2023-11-10 12:48:09,753 - mmseg - INFO - Iter [9900/160000]	lr: 9.906e-05, eta: 1 day, 5:56:26, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6199, decode.acc_seg: 77.7657, loss: 0.6199
2023-11-10 12:48:45,087 - mmseg - INFO - Iter [9950/160000]	lr: 9.905e-05, eta: 1 day, 5:55:41, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5994, decode.acc_seg: 78.5694, loss: 0.5994
2023-11-10 12:49:19,309 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 12:49:19,310 - mmseg - INFO - Iter [10000/160000]	lr: 9.904e-05, eta: 1 day, 5:54:40, time: 0.685, data_time: 0.011, memory: 23129, decode.loss_ce: 0.6096, decode.acc_seg: 77.8777, loss: 0.6096
2023-11-10 12:49:54,454 - mmseg - INFO - Iter [10050/160000]	lr: 9.903e-05, eta: 1 day, 5:53:53, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6152, decode.acc_seg: 77.6131, loss: 0.6152
2023-11-10 12:50:28,616 - mmseg - INFO - Iter [10100/160000]	lr: 9.902e-05, eta: 1 day, 5:52:52, time: 0.683, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6215, decode.acc_seg: 78.0735, loss: 0.6215
2023-11-10 12:51:00,248 - mmseg - INFO - Iter [10150/160000]	lr: 9.901e-05, eta: 1 day, 5:51:14, time: 0.634, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6222, decode.acc_seg: 77.3970, loss: 0.6222
2023-11-10 12:51:34,126 - mmseg - INFO - Iter [10200/160000]	lr: 9.900e-05, eta: 1 day, 5:50:08, time: 0.676, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5959, decode.acc_seg: 77.7190, loss: 0.5959
2023-11-10 12:52:06,515 - mmseg - INFO - Iter [10250/160000]	lr: 9.899e-05, eta: 1 day, 5:48:42, time: 0.647, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5855, decode.acc_seg: 78.6514, loss: 0.5855
2023-11-10 12:52:39,013 - mmseg - INFO - Iter [10300/160000]	lr: 9.898e-05, eta: 1 day, 5:47:18, time: 0.651, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5938, decode.acc_seg: 78.3960, loss: 0.5938
2023-11-10 12:53:12,735 - mmseg - INFO - Iter [10350/160000]	lr: 9.897e-05, eta: 1 day, 5:46:11, time: 0.673, data_time: 0.008, memory: 23129, decode.loss_ce: 0.5659, decode.acc_seg: 79.0845, loss: 0.5659
2023-11-10 12:53:48,289 - mmseg - INFO - Iter [10400/160000]	lr: 9.896e-05, eta: 1 day, 5:45:32, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5663, decode.acc_seg: 79.2107, loss: 0.5663
2023-11-10 12:54:23,247 - mmseg - INFO - Iter [10450/160000]	lr: 9.895e-05, eta: 1 day, 5:44:44, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5655, decode.acc_seg: 79.1080, loss: 0.5655
2023-11-10 12:54:58,756 - mmseg - INFO - Iter [10500/160000]	lr: 9.894e-05, eta: 1 day, 5:44:04, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6086, decode.acc_seg: 78.3207, loss: 0.6086
2023-11-10 12:55:32,760 - mmseg - INFO - Iter [10550/160000]	lr: 9.893e-05, eta: 1 day, 5:43:04, time: 0.681, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5903, decode.acc_seg: 78.6566, loss: 0.5903
2023-11-10 12:56:04,205 - mmseg - INFO - Iter [10600/160000]	lr: 9.892e-05, eta: 1 day, 5:41:26, time: 0.629, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5872, decode.acc_seg: 78.7870, loss: 0.5872
2023-11-10 12:56:38,800 - mmseg - INFO - Iter [10650/160000]	lr: 9.891e-05, eta: 1 day, 5:40:33, time: 0.691, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5967, decode.acc_seg: 78.0583, loss: 0.5967
2023-11-10 12:57:11,186 - mmseg - INFO - Iter [10700/160000]	lr: 9.890e-05, eta: 1 day, 5:39:11, time: 0.648, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5897, decode.acc_seg: 78.9597, loss: 0.5897
2023-11-10 12:57:42,578 - mmseg - INFO - Iter [10750/160000]	lr: 9.889e-05, eta: 1 day, 5:37:35, time: 0.628, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6399, decode.acc_seg: 76.5641, loss: 0.6399
2023-11-10 12:58:14,652 - mmseg - INFO - Iter [10800/160000]	lr: 9.888e-05, eta: 1 day, 5:36:09, time: 0.642, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5835, decode.acc_seg: 78.4291, loss: 0.5835
2023-11-10 12:58:48,437 - mmseg - INFO - Iter [10850/160000]	lr: 9.887e-05, eta: 1 day, 5:35:06, time: 0.675, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6338, decode.acc_seg: 77.2848, loss: 0.6338
2023-11-10 12:59:22,098 - mmseg - INFO - Iter [10900/160000]	lr: 9.886e-05, eta: 1 day, 5:34:02, time: 0.673, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5905, decode.acc_seg: 78.1442, loss: 0.5905
2023-11-10 12:59:56,730 - mmseg - INFO - Iter [10950/160000]	lr: 9.885e-05, eta: 1 day, 5:33:12, time: 0.693, data_time: 0.011, memory: 23129, decode.loss_ce: 0.6010, decode.acc_seg: 77.5354, loss: 0.6010
2023-11-10 13:00:31,053 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 13:00:31,054 - mmseg - INFO - Iter [11000/160000]	lr: 9.884e-05, eta: 1 day, 5:32:18, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6344, decode.acc_seg: 77.5072, loss: 0.6344
2023-11-10 13:01:06,379 - mmseg - INFO - Iter [11050/160000]	lr: 9.883e-05, eta: 1 day, 5:31:37, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6258, decode.acc_seg: 77.4305, loss: 0.6258
2023-11-10 13:01:38,316 - mmseg - INFO - Iter [11100/160000]	lr: 9.882e-05, eta: 1 day, 5:30:12, time: 0.640, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6228, decode.acc_seg: 77.6949, loss: 0.6228
2023-11-10 13:02:09,988 - mmseg - INFO - Iter [11150/160000]	lr: 9.881e-05, eta: 1 day, 5:28:43, time: 0.633, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6245, decode.acc_seg: 77.5900, loss: 0.6245
2023-11-10 13:02:43,708 - mmseg - INFO - Iter [11200/160000]	lr: 9.880e-05, eta: 1 day, 5:27:41, time: 0.673, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5880, decode.acc_seg: 78.5612, loss: 0.5880
2023-11-10 13:03:19,273 - mmseg - INFO - Iter [11250/160000]	lr: 9.879e-05, eta: 1 day, 5:27:04, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5965, decode.acc_seg: 78.4297, loss: 0.5965
2023-11-10 13:03:55,013 - mmseg - INFO - Iter [11300/160000]	lr: 9.877e-05, eta: 1 day, 5:26:30, time: 0.715, data_time: 0.011, memory: 23129, decode.loss_ce: 0.5676, decode.acc_seg: 79.0084, loss: 0.5676
2023-11-10 13:04:30,373 - mmseg - INFO - Iter [11350/160000]	lr: 9.876e-05, eta: 1 day, 5:25:51, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.6153, decode.acc_seg: 77.9526, loss: 0.6153
2023-11-10 13:05:05,846 - mmseg - INFO - Iter [11400/160000]	lr: 9.875e-05, eta: 1 day, 5:25:13, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5784, decode.acc_seg: 78.9166, loss: 0.5784
2023-11-10 13:05:40,924 - mmseg - INFO - Iter [11450/160000]	lr: 9.874e-05, eta: 1 day, 5:24:30, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5388, decode.acc_seg: 80.1192, loss: 0.5388
2023-11-10 13:06:15,934 - mmseg - INFO - Iter [11500/160000]	lr: 9.873e-05, eta: 1 day, 5:23:46, time: 0.700, data_time: 0.011, memory: 23129, decode.loss_ce: 0.5958, decode.acc_seg: 78.5501, loss: 0.5958
2023-11-10 13:06:50,935 - mmseg - INFO - Iter [11550/160000]	lr: 9.872e-05, eta: 1 day, 5:23:03, time: 0.700, data_time: 0.011, memory: 23129, decode.loss_ce: 0.6052, decode.acc_seg: 78.1464, loss: 0.6052
2023-11-10 13:07:26,631 - mmseg - INFO - Iter [11600/160000]	lr: 9.871e-05, eta: 1 day, 5:22:28, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5623, decode.acc_seg: 79.6419, loss: 0.5623
2023-11-10 13:08:03,029 - mmseg - INFO - Iter [11650/160000]	lr: 9.870e-05, eta: 1 day, 5:22:02, time: 0.728, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6065, decode.acc_seg: 77.9623, loss: 0.6065
2023-11-10 13:08:36,576 - mmseg - INFO - Iter [11700/160000]	lr: 9.869e-05, eta: 1 day, 5:21:01, time: 0.672, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5757, decode.acc_seg: 78.7365, loss: 0.5757
2023-11-10 13:09:12,026 - mmseg - INFO - Iter [11750/160000]	lr: 9.868e-05, eta: 1 day, 5:20:23, time: 0.709, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5947, decode.acc_seg: 78.2136, loss: 0.5947
2023-11-10 13:09:45,956 - mmseg - INFO - Iter [11800/160000]	lr: 9.866e-05, eta: 1 day, 5:19:25, time: 0.678, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5536, decode.acc_seg: 79.3448, loss: 0.5536
2023-11-10 13:10:21,139 - mmseg - INFO - Iter [11850/160000]	lr: 9.865e-05, eta: 1 day, 5:18:45, time: 0.705, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5364, decode.acc_seg: 80.2848, loss: 0.5364
2023-11-10 13:10:55,199 - mmseg - INFO - Iter [11900/160000]	lr: 9.864e-05, eta: 1 day, 5:17:50, time: 0.681, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6091, decode.acc_seg: 78.4007, loss: 0.6091
2023-11-10 13:11:28,489 - mmseg - INFO - Iter [11950/160000]	lr: 9.863e-05, eta: 1 day, 5:16:45, time: 0.666, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5594, decode.acc_seg: 79.7301, loss: 0.5594
2023-11-10 13:12:02,473 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 13:12:02,473 - mmseg - INFO - Iter [12000/160000]	lr: 9.862e-05, eta: 1 day, 5:15:50, time: 0.680, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5267, decode.acc_seg: 80.4721, loss: 0.5267
2023-11-10 13:12:37,418 - mmseg - INFO - Iter [12050/160000]	lr: 9.861e-05, eta: 1 day, 5:15:06, time: 0.698, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5347, decode.acc_seg: 80.1328, loss: 0.5347
2023-11-10 13:13:10,478 - mmseg - INFO - Iter [12100/160000]	lr: 9.860e-05, eta: 1 day, 5:13:59, time: 0.661, data_time: 0.011, memory: 23129, decode.loss_ce: 0.5497, decode.acc_seg: 79.8597, loss: 0.5497
2023-11-10 13:13:43,349 - mmseg - INFO - Iter [12150/160000]	lr: 9.858e-05, eta: 1 day, 5:12:51, time: 0.659, data_time: 0.011, memory: 23129, decode.loss_ce: 0.6005, decode.acc_seg: 78.5202, loss: 0.6005
2023-11-10 13:14:17,091 - mmseg - INFO - Iter [12200/160000]	lr: 9.857e-05, eta: 1 day, 5:11:53, time: 0.674, data_time: 0.009, memory: 23129, decode.loss_ce: 0.6084, decode.acc_seg: 77.4466, loss: 0.6084
2023-11-10 13:14:52,459 - mmseg - INFO - Iter [12250/160000]	lr: 9.856e-05, eta: 1 day, 5:11:15, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5694, decode.acc_seg: 79.5564, loss: 0.5694
2023-11-10 13:15:26,340 - mmseg - INFO - Iter [12300/160000]	lr: 9.855e-05, eta: 1 day, 5:10:19, time: 0.678, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5498, decode.acc_seg: 80.0130, loss: 0.5498
2023-11-10 13:16:01,248 - mmseg - INFO - Iter [12350/160000]	lr: 9.854e-05, eta: 1 day, 5:09:36, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5699, decode.acc_seg: 78.6631, loss: 0.5699
2023-11-10 13:16:35,150 - mmseg - INFO - Iter [12400/160000]	lr: 9.853e-05, eta: 1 day, 5:08:42, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5804, decode.acc_seg: 78.6044, loss: 0.5804
2023-11-10 13:17:07,261 - mmseg - INFO - Iter [12450/160000]	lr: 9.851e-05, eta: 1 day, 5:07:25, time: 0.641, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5631, decode.acc_seg: 79.6159, loss: 0.5631
2023-11-10 13:17:42,524 - mmseg - INFO - Iter [12500/160000]	lr: 9.850e-05, eta: 1 day, 5:06:46, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5568, decode.acc_seg: 80.0216, loss: 0.5568
2023-11-10 13:18:17,922 - mmseg - INFO - Iter [12550/160000]	lr: 9.849e-05, eta: 1 day, 5:06:09, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5958, decode.acc_seg: 78.5045, loss: 0.5958
2023-11-10 13:18:53,157 - mmseg - INFO - Iter [12600/160000]	lr: 9.848e-05, eta: 1 day, 5:05:30, time: 0.704, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5874, decode.acc_seg: 78.9788, loss: 0.5874
2023-11-10 13:19:28,137 - mmseg - INFO - Iter [12650/160000]	lr: 9.847e-05, eta: 1 day, 5:04:48, time: 0.700, data_time: 0.011, memory: 23129, decode.loss_ce: 0.5961, decode.acc_seg: 78.0368, loss: 0.5961
2023-11-10 13:20:03,125 - mmseg - INFO - Iter [12700/160000]	lr: 9.845e-05, eta: 1 day, 5:04:06, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5622, decode.acc_seg: 79.6233, loss: 0.5622
2023-11-10 13:20:38,262 - mmseg - INFO - Iter [12750/160000]	lr: 9.844e-05, eta: 1 day, 5:03:26, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5773, decode.acc_seg: 79.0884, loss: 0.5773
2023-11-10 13:21:13,409 - mmseg - INFO - Iter [12800/160000]	lr: 9.843e-05, eta: 1 day, 5:02:47, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5634, decode.acc_seg: 79.1279, loss: 0.5634
2023-11-10 13:21:45,014 - mmseg - INFO - Iter [12850/160000]	lr: 9.842e-05, eta: 1 day, 5:01:27, time: 0.632, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5865, decode.acc_seg: 78.4326, loss: 0.5865
2023-11-10 13:22:19,103 - mmseg - INFO - Iter [12900/160000]	lr: 9.840e-05, eta: 1 day, 5:00:35, time: 0.681, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5856, decode.acc_seg: 78.7865, loss: 0.5856
2023-11-10 13:22:54,140 - mmseg - INFO - Iter [12950/160000]	lr: 9.839e-05, eta: 1 day, 4:59:54, time: 0.701, data_time: 0.011, memory: 23129, decode.loss_ce: 0.5779, decode.acc_seg: 78.6666, loss: 0.5779
2023-11-10 13:23:28,749 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 13:23:28,750 - mmseg - INFO - Iter [13000/160000]	lr: 9.838e-05, eta: 1 day, 4:59:08, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5959, decode.acc_seg: 78.6206, loss: 0.5959
2023-11-10 13:24:04,097 - mmseg - INFO - Iter [13050/160000]	lr: 9.837e-05, eta: 1 day, 4:58:31, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5541, decode.acc_seg: 79.7196, loss: 0.5541
2023-11-10 13:24:39,443 - mmseg - INFO - Iter [13100/160000]	lr: 9.836e-05, eta: 1 day, 4:57:54, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5865, decode.acc_seg: 79.4120, loss: 0.5865
2023-11-10 13:25:14,710 - mmseg - INFO - Iter [13150/160000]	lr: 9.834e-05, eta: 1 day, 4:57:16, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5547, decode.acc_seg: 79.7156, loss: 0.5547
2023-11-10 13:25:47,733 - mmseg - INFO - Iter [13200/160000]	lr: 9.833e-05, eta: 1 day, 4:56:13, time: 0.661, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5671, decode.acc_seg: 79.4487, loss: 0.5671
2023-11-10 13:26:21,674 - mmseg - INFO - Iter [13250/160000]	lr: 9.832e-05, eta: 1 day, 4:55:21, time: 0.679, data_time: 0.011, memory: 23129, decode.loss_ce: 0.5708, decode.acc_seg: 78.6232, loss: 0.5708
2023-11-10 13:26:55,424 - mmseg - INFO - Iter [13300/160000]	lr: 9.831e-05, eta: 1 day, 4:54:26, time: 0.674, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5776, decode.acc_seg: 78.3893, loss: 0.5776
2023-11-10 13:27:29,998 - mmseg - INFO - Iter [13350/160000]	lr: 9.829e-05, eta: 1 day, 4:53:42, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5268, decode.acc_seg: 80.6393, loss: 0.5268
2023-11-10 13:28:04,201 - mmseg - INFO - Iter [13400/160000]	lr: 9.828e-05, eta: 1 day, 4:52:52, time: 0.683, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5636, decode.acc_seg: 79.4131, loss: 0.5636
2023-11-10 13:28:38,443 - mmseg - INFO - Iter [13450/160000]	lr: 9.827e-05, eta: 1 day, 4:52:03, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5425, decode.acc_seg: 79.7361, loss: 0.5425
2023-11-10 13:29:11,903 - mmseg - INFO - Iter [13500/160000]	lr: 9.825e-05, eta: 1 day, 4:51:06, time: 0.669, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5273, decode.acc_seg: 80.3527, loss: 0.5273
2023-11-10 13:29:44,698 - mmseg - INFO - Iter [13550/160000]	lr: 9.824e-05, eta: 1 day, 4:50:02, time: 0.655, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5655, decode.acc_seg: 79.2657, loss: 0.5655
2023-11-10 13:30:18,883 - mmseg - INFO - Iter [13600/160000]	lr: 9.823e-05, eta: 1 day, 4:49:13, time: 0.684, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5525, decode.acc_seg: 79.7184, loss: 0.5525
2023-11-10 13:30:54,145 - mmseg - INFO - Iter [13650/160000]	lr: 9.822e-05, eta: 1 day, 4:48:35, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5492, decode.acc_seg: 79.6572, loss: 0.5492
2023-11-10 13:31:29,025 - mmseg - INFO - Iter [13700/160000]	lr: 9.820e-05, eta: 1 day, 4:47:54, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5527, decode.acc_seg: 79.9672, loss: 0.5527
2023-11-10 13:32:02,296 - mmseg - INFO - Iter [13750/160000]	lr: 9.819e-05, eta: 1 day, 4:46:56, time: 0.666, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5480, decode.acc_seg: 79.8004, loss: 0.5480
2023-11-10 13:32:34,512 - mmseg - INFO - Iter [13800/160000]	lr: 9.818e-05, eta: 1 day, 4:45:47, time: 0.644, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5603, decode.acc_seg: 79.0002, loss: 0.5603
2023-11-10 13:33:10,315 - mmseg - INFO - Iter [13850/160000]	lr: 9.816e-05, eta: 1 day, 4:45:15, time: 0.715, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5747, decode.acc_seg: 79.0430, loss: 0.5747
2023-11-10 13:33:46,818 - mmseg - INFO - Iter [13900/160000]	lr: 9.815e-05, eta: 1 day, 4:44:51, time: 0.730, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5388, decode.acc_seg: 79.9736, loss: 0.5388
2023-11-10 13:34:23,554 - mmseg - INFO - Iter [13950/160000]	lr: 9.814e-05, eta: 1 day, 4:44:29, time: 0.735, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5641, decode.acc_seg: 79.2483, loss: 0.5641
2023-11-10 13:35:00,112 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 13:35:00,113 - mmseg - INFO - Iter [14000/160000]	lr: 9.812e-05, eta: 1 day, 4:44:06, time: 0.731, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5368, decode.acc_seg: 80.5071, loss: 0.5368
2023-11-10 13:35:35,979 - mmseg - INFO - Iter [14050/160000]	lr: 9.811e-05, eta: 1 day, 4:43:35, time: 0.718, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5353, decode.acc_seg: 80.0505, loss: 0.5353
2023-11-10 13:36:10,519 - mmseg - INFO - Iter [14100/160000]	lr: 9.810e-05, eta: 1 day, 4:42:50, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5748, decode.acc_seg: 78.6251, loss: 0.5748
2023-11-10 13:36:44,662 - mmseg - INFO - Iter [14150/160000]	lr: 9.808e-05, eta: 1 day, 4:42:01, time: 0.682, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5418, decode.acc_seg: 80.1013, loss: 0.5418
2023-11-10 13:37:20,022 - mmseg - INFO - Iter [14200/160000]	lr: 9.807e-05, eta: 1 day, 4:41:26, time: 0.708, data_time: 0.011, memory: 23129, decode.loss_ce: 0.5270, decode.acc_seg: 80.4828, loss: 0.5270
2023-11-10 13:37:53,471 - mmseg - INFO - Iter [14250/160000]	lr: 9.806e-05, eta: 1 day, 4:40:30, time: 0.670, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5219, decode.acc_seg: 80.6060, loss: 0.5219
2023-11-10 13:38:26,557 - mmseg - INFO - Iter [14300/160000]	lr: 9.804e-05, eta: 1 day, 4:39:31, time: 0.662, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5312, decode.acc_seg: 80.6910, loss: 0.5312
2023-11-10 13:38:58,118 - mmseg - INFO - Iter [14350/160000]	lr: 9.803e-05, eta: 1 day, 4:38:17, time: 0.631, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5814, decode.acc_seg: 79.2241, loss: 0.5814
2023-11-10 13:39:29,673 - mmseg - INFO - Iter [14400/160000]	lr: 9.801e-05, eta: 1 day, 4:37:02, time: 0.630, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5802, decode.acc_seg: 78.8849, loss: 0.5802
2023-11-10 13:40:05,258 - mmseg - INFO - Iter [14450/160000]	lr: 9.800e-05, eta: 1 day, 4:36:29, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5073, decode.acc_seg: 80.8943, loss: 0.5073
2023-11-10 13:40:41,015 - mmseg - INFO - Iter [14500/160000]	lr: 9.799e-05, eta: 1 day, 4:35:57, time: 0.715, data_time: 0.011, memory: 23129, decode.loss_ce: 0.5606, decode.acc_seg: 79.6595, loss: 0.5606
2023-11-10 13:41:16,383 - mmseg - INFO - Iter [14550/160000]	lr: 9.797e-05, eta: 1 day, 4:35:22, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5140, decode.acc_seg: 80.6234, loss: 0.5140
2023-11-10 13:41:50,465 - mmseg - INFO - Iter [14600/160000]	lr: 9.796e-05, eta: 1 day, 4:34:33, time: 0.682, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5740, decode.acc_seg: 78.5854, loss: 0.5740
2023-11-10 13:42:23,035 - mmseg - INFO - Iter [14650/160000]	lr: 9.795e-05, eta: 1 day, 4:33:31, time: 0.652, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5839, decode.acc_seg: 78.1966, loss: 0.5839
2023-11-10 13:42:56,018 - mmseg - INFO - Iter [14700/160000]	lr: 9.793e-05, eta: 1 day, 4:32:31, time: 0.659, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5531, decode.acc_seg: 79.8072, loss: 0.5531
2023-11-10 13:43:31,627 - mmseg - INFO - Iter [14750/160000]	lr: 9.792e-05, eta: 1 day, 4:31:58, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5508, decode.acc_seg: 79.6645, loss: 0.5508
2023-11-10 13:44:04,151 - mmseg - INFO - Iter [14800/160000]	lr: 9.790e-05, eta: 1 day, 4:30:55, time: 0.650, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5497, decode.acc_seg: 79.4495, loss: 0.5497
2023-11-10 13:44:39,236 - mmseg - INFO - Iter [14850/160000]	lr: 9.789e-05, eta: 1 day, 4:30:18, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5656, decode.acc_seg: 79.3282, loss: 0.5656
2023-11-10 13:45:12,485 - mmseg - INFO - Iter [14900/160000]	lr: 9.788e-05, eta: 1 day, 4:29:22, time: 0.664, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5599, decode.acc_seg: 79.5108, loss: 0.5599
2023-11-10 13:45:46,832 - mmseg - INFO - Iter [14950/160000]	lr: 9.786e-05, eta: 1 day, 4:28:37, time: 0.687, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5344, decode.acc_seg: 80.4533, loss: 0.5344
2023-11-10 13:46:18,390 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 13:46:18,391 - mmseg - INFO - Iter [15000/160000]	lr: 9.785e-05, eta: 1 day, 4:27:25, time: 0.631, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5205, decode.acc_seg: 80.1767, loss: 0.5205
2023-11-10 13:46:52,137 - mmseg - INFO - Iter [15050/160000]	lr: 9.783e-05, eta: 1 day, 4:26:34, time: 0.675, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5527, decode.acc_seg: 80.1669, loss: 0.5527
2023-11-10 13:47:25,291 - mmseg - INFO - Iter [15100/160000]	lr: 9.782e-05, eta: 1 day, 4:25:38, time: 0.664, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5438, decode.acc_seg: 79.7252, loss: 0.5438
2023-11-10 13:47:58,698 - mmseg - INFO - Iter [15150/160000]	lr: 9.780e-05, eta: 1 day, 4:24:44, time: 0.667, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5140, decode.acc_seg: 81.0314, loss: 0.5140
2023-11-10 13:48:30,678 - mmseg - INFO - Iter [15200/160000]	lr: 9.779e-05, eta: 1 day, 4:23:38, time: 0.641, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5625, decode.acc_seg: 79.3466, loss: 0.5625
2023-11-10 13:49:05,638 - mmseg - INFO - Iter [15250/160000]	lr: 9.778e-05, eta: 1 day, 4:22:59, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5665, decode.acc_seg: 78.9681, loss: 0.5665
2023-11-10 13:49:41,273 - mmseg - INFO - Iter [15300/160000]	lr: 9.776e-05, eta: 1 day, 4:22:27, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5750, decode.acc_seg: 78.8563, loss: 0.5750
2023-11-10 13:50:16,633 - mmseg - INFO - Iter [15350/160000]	lr: 9.775e-05, eta: 1 day, 4:21:52, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5822, decode.acc_seg: 79.0150, loss: 0.5822
2023-11-10 13:50:50,692 - mmseg - INFO - Iter [15400/160000]	lr: 9.773e-05, eta: 1 day, 4:21:05, time: 0.681, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5414, decode.acc_seg: 80.1329, loss: 0.5414
2023-11-10 13:51:23,999 - mmseg - INFO - Iter [15450/160000]	lr: 9.772e-05, eta: 1 day, 4:20:12, time: 0.667, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5269, decode.acc_seg: 80.0974, loss: 0.5269
2023-11-10 13:51:55,569 - mmseg - INFO - Iter [15500/160000]	lr: 9.770e-05, eta: 1 day, 4:19:02, time: 0.631, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5279, decode.acc_seg: 80.4300, loss: 0.5279
2023-11-10 13:52:27,943 - mmseg - INFO - Iter [15550/160000]	lr: 9.769e-05, eta: 1 day, 4:17:59, time: 0.646, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5301, decode.acc_seg: 80.1227, loss: 0.5301
2023-11-10 13:53:00,941 - mmseg - INFO - Iter [15600/160000]	lr: 9.767e-05, eta: 1 day, 4:17:03, time: 0.661, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5095, decode.acc_seg: 80.4346, loss: 0.5095
2023-11-10 13:53:35,839 - mmseg - INFO - Iter [15650/160000]	lr: 9.766e-05, eta: 1 day, 4:16:25, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5213, decode.acc_seg: 80.1702, loss: 0.5213
2023-11-10 13:54:07,753 - mmseg - INFO - Iter [15700/160000]	lr: 9.764e-05, eta: 1 day, 4:15:19, time: 0.638, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5286, decode.acc_seg: 79.9836, loss: 0.5286
2023-11-10 13:54:40,025 - mmseg - INFO - Iter [15750/160000]	lr: 9.763e-05, eta: 1 day, 4:14:16, time: 0.645, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5316, decode.acc_seg: 80.4089, loss: 0.5316
2023-11-10 13:55:14,863 - mmseg - INFO - Iter [15800/160000]	lr: 9.761e-05, eta: 1 day, 4:13:37, time: 0.697, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5335, decode.acc_seg: 80.0788, loss: 0.5335
2023-11-10 13:55:50,016 - mmseg - INFO - Iter [15850/160000]	lr: 9.760e-05, eta: 1 day, 4:13:01, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5455, decode.acc_seg: 80.4763, loss: 0.5455
2023-11-10 13:56:23,699 - mmseg - INFO - Iter [15900/160000]	lr: 9.758e-05, eta: 1 day, 4:12:12, time: 0.675, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5302, decode.acc_seg: 80.5962, loss: 0.5302
2023-11-10 13:56:56,437 - mmseg - INFO - Iter [15950/160000]	lr: 9.757e-05, eta: 1 day, 4:11:14, time: 0.654, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5007, decode.acc_seg: 81.3074, loss: 0.5007
2023-11-10 13:57:31,470 - mmseg - INFO - Saving checkpoint at 16000 iterations
2023-11-10 13:57:36,079 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 13:57:36,079 - mmseg - INFO - Iter [16000/160000]	lr: 9.755e-05, eta: 1 day, 4:11:19, time: 0.794, data_time: 0.011, memory: 23129, decode.loss_ce: 0.5347, decode.acc_seg: 80.7864, loss: 0.5347
2023-11-10 13:59:06,700 - mmseg - INFO - per class results:
2023-11-10 13:59:06,716 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 73.05 | 84.13 |
|       building      | 80.59 |  89.1 |
|         sky         | 93.43 | 97.42 |
|        floor        | 77.41 | 84.85 |
|         tree        | 71.69 | 89.35 |
|       ceiling       | 81.45 | 89.67 |
|         road        | 78.19 | 88.56 |
|         bed         | 84.47 | 93.62 |
|      windowpane     |  58.4 | 81.19 |
|        grass        | 67.73 | 85.56 |
|       cabinet       | 55.33 | 66.28 |
|       sidewalk      | 58.58 | 76.09 |
|        person       | 76.53 | 90.47 |
|        earth        | 32.08 | 43.62 |
|         door        | 39.94 | 49.96 |
|        table        |  52.6 | 69.62 |
|       mountain      | 48.14 | 58.23 |
|        plant        | 49.11 | 61.45 |
|       curtain       | 68.57 | 87.19 |
|        chair        | 47.71 |  68.3 |
|         car         | 77.97 | 90.48 |
|        water        | 48.42 | 58.35 |
|       painting      | 66.47 | 82.97 |
|         sofa        | 49.56 | 58.04 |
|        shelf        | 39.46 | 51.68 |
|        house        | 48.37 | 80.35 |
|         sea         | 50.18 | 77.83 |
|        mirror       | 49.29 | 52.67 |
|         rug         | 60.06 | 83.04 |
|        field        | 20.73 | 30.67 |
|       armchair      | 30.71 | 55.88 |
|         seat        | 62.44 | 77.35 |
|        fence        | 41.33 |  53.3 |
|         desk        | 36.07 | 59.15 |
|         rock        | 41.16 | 61.49 |
|       wardrobe      | 44.58 | 67.44 |
|         lamp        | 53.75 | 71.08 |
|       bathtub       | 67.51 | 72.94 |
|       railing       | 32.06 | 47.07 |
|       cushion       | 47.75 | 66.25 |
|         base        | 27.71 |  42.3 |
|         box         | 18.97 |  30.0 |
|        column       | 43.23 |  51.0 |
|      signboard      |  33.6 | 43.55 |
|   chest of drawers  | 41.86 |  61.7 |
|       counter       | 19.82 | 25.58 |
|         sand        | 35.68 |  54.7 |
|         sink        | 62.47 | 71.03 |
|      skyscraper     | 60.85 |  73.3 |
|      fireplace      | 52.37 |  90.5 |
|     refrigerator    | 66.04 | 83.91 |
|      grandstand     | 44.77 |  83.2 |
|         path        | 20.09 |  41.7 |
|        stairs       | 31.24 | 42.79 |
|        runway       | 69.42 |  89.7 |
|         case        | 52.33 | 66.78 |
|      pool table     | 89.29 | 97.02 |
|        pillow       | 49.95 | 70.16 |
|     screen door     | 48.18 | 53.03 |
|       stairway      | 28.03 | 45.51 |
|        river        | 14.26 | 32.28 |
|        bridge       | 57.78 | 79.05 |
|       bookcase      | 32.58 | 51.97 |
|        blind        | 38.68 | 41.85 |
|     coffee table    | 47.82 | 73.15 |
|        toilet       | 80.89 | 89.52 |
|        flower       | 34.11 | 63.05 |
|         book        | 36.35 | 63.63 |
|         hill        |  9.14 | 20.34 |
|        bench        | 37.36 |  49.7 |
|      countertop     | 51.55 | 70.77 |
|        stove        | 56.65 | 81.79 |
|         palm        | 43.05 | 57.66 |
|    kitchen island   | 30.59 | 68.14 |
|       computer      |  54.7 |  71.2 |
|     swivel chair    |  38.0 | 65.97 |
|         boat        |  58.9 | 86.09 |
|         bar         | 44.65 | 59.74 |
|    arcade machine   | 66.76 |  73.7 |
|        hovel        | 51.04 | 70.41 |
|         bus         |  80.1 | 91.28 |
|        towel        | 55.67 |  72.2 |
|        light        | 42.79 | 53.88 |
|        truck        |  31.2 |  44.9 |
|        tower        | 36.07 | 84.75 |
|      chandelier     | 52.53 |  87.2 |
|        awning       | 25.56 | 41.19 |
|     streetlight     |  16.8 | 20.17 |
|        booth        | 29.39 | 60.57 |
| television receiver | 66.79 | 76.34 |
|       airplane      | 54.73 | 64.59 |
|      dirt track     |  2.32 |  5.24 |
|       apparel       | 29.63 | 56.22 |
|         pole        | 13.67 | 16.26 |
|         land        |  0.04 |  0.05 |
|      bannister      |  5.7  |  7.59 |
|      escalator      |  6.46 |  7.28 |
|       ottoman       | 35.33 | 53.57 |
|        bottle       | 31.44 | 47.69 |
|        buffet       | 34.82 | 44.21 |
|        poster       | 15.16 | 19.23 |
|        stage        |  8.21 | 15.09 |
|         van         | 31.55 | 36.74 |
|         ship        |  5.03 |  5.95 |
|       fountain      | 21.29 |  22.1 |
|    conveyer belt    |  49.1 | 65.44 |
|        canopy       | 19.43 | 29.98 |
|        washer       | 71.35 |  77.6 |
|      plaything      | 16.41 | 51.42 |
|    swimming pool    | 55.81 | 82.25 |
|        stool        | 23.29 | 38.04 |
|        barrel       |  41.0 | 64.64 |
|        basket       |  28.4 | 48.89 |
|      waterfall      | 62.77 | 85.48 |
|         tent        | 40.71 | 98.54 |
|         bag         |  2.27 |  2.71 |
|       minibike      | 60.22 | 74.92 |
|        cradle       | 70.99 | 97.26 |
|         oven        | 23.32 | 49.43 |
|         ball        | 43.99 |  65.1 |
|         food        | 52.19 | 61.72 |
|         step        |  3.45 |  3.48 |
|         tank        | 45.95 | 49.76 |
|      trade name     | 22.31 | 24.05 |
|      microwave      | 61.26 |  72.1 |
|         pot         | 29.76 |  33.1 |
|        animal       |  53.3 | 55.86 |
|       bicycle       | 52.57 | 74.61 |
|         lake        |  0.0  |  0.0  |
|      dishwasher     |  17.0 | 17.41 |
|        screen       | 60.04 | 92.92 |
|       blanket       |  0.27 |  0.29 |
|      sculpture      | 44.14 | 62.23 |
|         hood        | 37.04 | 40.56 |
|        sconce       | 13.15 | 14.66 |
|         vase        | 25.08 | 35.04 |
|    traffic light    | 20.53 |  38.5 |
|         tray        |  0.16 |  0.16 |
|        ashcan       | 34.74 | 53.92 |
|         fan         | 51.78 | 64.22 |
|         pier        | 27.01 | 41.29 |
|      crt screen     |  0.0  |  0.0  |
|        plate        | 47.92 | 62.19 |
|       monitor       |  1.16 |  1.29 |
|    bulletin board   |  34.7 | 44.02 |
|        shower       |  0.0  |  0.0  |
|       radiator      | 47.52 |  50.9 |
|        glass        |  8.88 |  9.28 |
|        clock        | 23.54 | 33.58 |
|         flag        |  44.4 | 51.29 |
+---------------------+-------+-------+
2023-11-10 13:59:06,716 - mmseg - INFO - Summary:
2023-11-10 13:59:06,716 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 79.77 | 41.63 | 55.96 |
+-------+-------+-------+
2023-11-10 13:59:06,862 - mmseg - INFO - The previous best checkpoint /data/Next-ViT/segmentation/work_dirs/fpn_512_debi_base_160k/best_mIoU_iter_8000.pth was removed
2023-11-10 13:59:09,721 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_16000.pth.
2023-11-10 13:59:09,721 - mmseg - INFO - Best mIoU is 0.4163 at 16000 iter.
2023-11-10 13:59:09,756 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 13:59:09,757 - mmseg - INFO - Iter(val) [250]	aAcc: 0.7977, mIoU: 0.4163, mAcc: 0.5596, IoU.wall: 0.7305, IoU.building: 0.8059, IoU.sky: 0.9343, IoU.floor: 0.7741, IoU.tree: 0.7169, IoU.ceiling: 0.8145, IoU.road: 0.7819, IoU.bed : 0.8447, IoU.windowpane: 0.5840, IoU.grass: 0.6773, IoU.cabinet: 0.5533, IoU.sidewalk: 0.5858, IoU.person: 0.7653, IoU.earth: 0.3208, IoU.door: 0.3994, IoU.table: 0.5260, IoU.mountain: 0.4814, IoU.plant: 0.4911, IoU.curtain: 0.6857, IoU.chair: 0.4771, IoU.car: 0.7797, IoU.water: 0.4842, IoU.painting: 0.6647, IoU.sofa: 0.4956, IoU.shelf: 0.3946, IoU.house: 0.4837, IoU.sea: 0.5018, IoU.mirror: 0.4929, IoU.rug: 0.6006, IoU.field: 0.2073, IoU.armchair: 0.3071, IoU.seat: 0.6244, IoU.fence: 0.4133, IoU.desk: 0.3607, IoU.rock: 0.4116, IoU.wardrobe: 0.4458, IoU.lamp: 0.5375, IoU.bathtub: 0.6751, IoU.railing: 0.3206, IoU.cushion: 0.4775, IoU.base: 0.2771, IoU.box: 0.1897, IoU.column: 0.4323, IoU.signboard: 0.3360, IoU.chest of drawers: 0.4186, IoU.counter: 0.1982, IoU.sand: 0.3568, IoU.sink: 0.6247, IoU.skyscraper: 0.6085, IoU.fireplace: 0.5237, IoU.refrigerator: 0.6604, IoU.grandstand: 0.4477, IoU.path: 0.2009, IoU.stairs: 0.3124, IoU.runway: 0.6942, IoU.case: 0.5233, IoU.pool table: 0.8929, IoU.pillow: 0.4995, IoU.screen door: 0.4818, IoU.stairway: 0.2803, IoU.river: 0.1426, IoU.bridge: 0.5778, IoU.bookcase: 0.3258, IoU.blind: 0.3868, IoU.coffee table: 0.4782, IoU.toilet: 0.8089, IoU.flower: 0.3411, IoU.book: 0.3635, IoU.hill: 0.0914, IoU.bench: 0.3736, IoU.countertop: 0.5155, IoU.stove: 0.5665, IoU.palm: 0.4305, IoU.kitchen island: 0.3059, IoU.computer: 0.5470, IoU.swivel chair: 0.3800, IoU.boat: 0.5890, IoU.bar: 0.4465, IoU.arcade machine: 0.6676, IoU.hovel: 0.5104, IoU.bus: 0.8010, IoU.towel: 0.5567, IoU.light: 0.4279, IoU.truck: 0.3120, IoU.tower: 0.3607, IoU.chandelier: 0.5253, IoU.awning: 0.2556, IoU.streetlight: 0.1680, IoU.booth: 0.2939, IoU.television receiver: 0.6679, IoU.airplane: 0.5473, IoU.dirt track: 0.0232, IoU.apparel: 0.2963, IoU.pole: 0.1367, IoU.land: 0.0004, IoU.bannister: 0.0570, IoU.escalator: 0.0646, IoU.ottoman: 0.3533, IoU.bottle: 0.3144, IoU.buffet: 0.3482, IoU.poster: 0.1516, IoU.stage: 0.0821, IoU.van: 0.3155, IoU.ship: 0.0503, IoU.fountain: 0.2129, IoU.conveyer belt: 0.4910, IoU.canopy: 0.1943, IoU.washer: 0.7135, IoU.plaything: 0.1641, IoU.swimming pool: 0.5581, IoU.stool: 0.2329, IoU.barrel: 0.4100, IoU.basket: 0.2840, IoU.waterfall: 0.6277, IoU.tent: 0.4071, IoU.bag: 0.0227, IoU.minibike: 0.6022, IoU.cradle: 0.7099, IoU.oven: 0.2332, IoU.ball: 0.4399, IoU.food: 0.5219, IoU.step: 0.0345, IoU.tank: 0.4595, IoU.trade name: 0.2231, IoU.microwave: 0.6126, IoU.pot: 0.2976, IoU.animal: 0.5330, IoU.bicycle: 0.5257, IoU.lake: 0.0000, IoU.dishwasher: 0.1700, IoU.screen: 0.6004, IoU.blanket: 0.0027, IoU.sculpture: 0.4414, IoU.hood: 0.3704, IoU.sconce: 0.1315, IoU.vase: 0.2508, IoU.traffic light: 0.2053, IoU.tray: 0.0016, IoU.ashcan: 0.3474, IoU.fan: 0.5178, IoU.pier: 0.2701, IoU.crt screen: 0.0000, IoU.plate: 0.4792, IoU.monitor: 0.0116, IoU.bulletin board: 0.3470, IoU.shower: 0.0000, IoU.radiator: 0.4752, IoU.glass: 0.0888, IoU.clock: 0.2354, IoU.flag: 0.4440, Acc.wall: 0.8413, Acc.building: 0.8910, Acc.sky: 0.9742, Acc.floor: 0.8485, Acc.tree: 0.8935, Acc.ceiling: 0.8967, Acc.road: 0.8856, Acc.bed : 0.9362, Acc.windowpane: 0.8119, Acc.grass: 0.8556, Acc.cabinet: 0.6628, Acc.sidewalk: 0.7609, Acc.person: 0.9047, Acc.earth: 0.4362, Acc.door: 0.4996, Acc.table: 0.6962, Acc.mountain: 0.5823, Acc.plant: 0.6145, Acc.curtain: 0.8719, Acc.chair: 0.6830, Acc.car: 0.9048, Acc.water: 0.5835, Acc.painting: 0.8297, Acc.sofa: 0.5804, Acc.shelf: 0.5168, Acc.house: 0.8035, Acc.sea: 0.7783, Acc.mirror: 0.5267, Acc.rug: 0.8304, Acc.field: 0.3067, Acc.armchair: 0.5588, Acc.seat: 0.7735, Acc.fence: 0.5330, Acc.desk: 0.5915, Acc.rock: 0.6149, Acc.wardrobe: 0.6744, Acc.lamp: 0.7108, Acc.bathtub: 0.7294, Acc.railing: 0.4707, Acc.cushion: 0.6625, Acc.base: 0.4230, Acc.box: 0.3000, Acc.column: 0.5100, Acc.signboard: 0.4355, Acc.chest of drawers: 0.6170, Acc.counter: 0.2558, Acc.sand: 0.5470, Acc.sink: 0.7103, Acc.skyscraper: 0.7330, Acc.fireplace: 0.9050, Acc.refrigerator: 0.8391, Acc.grandstand: 0.8320, Acc.path: 0.4170, Acc.stairs: 0.4279, Acc.runway: 0.8970, Acc.case: 0.6678, Acc.pool table: 0.9702, Acc.pillow: 0.7016, Acc.screen door: 0.5303, Acc.stairway: 0.4551, Acc.river: 0.3228, Acc.bridge: 0.7905, Acc.bookcase: 0.5197, Acc.blind: 0.4185, Acc.coffee table: 0.7315, Acc.toilet: 0.8952, Acc.flower: 0.6305, Acc.book: 0.6363, Acc.hill: 0.2034, Acc.bench: 0.4970, Acc.countertop: 0.7077, Acc.stove: 0.8179, Acc.palm: 0.5766, Acc.kitchen island: 0.6814, Acc.computer: 0.7120, Acc.swivel chair: 0.6597, Acc.boat: 0.8609, Acc.bar: 0.5974, Acc.arcade machine: 0.7370, Acc.hovel: 0.7041, Acc.bus: 0.9128, Acc.towel: 0.7220, Acc.light: 0.5388, Acc.truck: 0.4490, Acc.tower: 0.8475, Acc.chandelier: 0.8720, Acc.awning: 0.4119, Acc.streetlight: 0.2017, Acc.booth: 0.6057, Acc.television receiver: 0.7634, Acc.airplane: 0.6459, Acc.dirt track: 0.0524, Acc.apparel: 0.5622, Acc.pole: 0.1626, Acc.land: 0.0005, Acc.bannister: 0.0759, Acc.escalator: 0.0728, Acc.ottoman: 0.5357, Acc.bottle: 0.4769, Acc.buffet: 0.4421, Acc.poster: 0.1923, Acc.stage: 0.1509, Acc.van: 0.3674, Acc.ship: 0.0595, Acc.fountain: 0.2210, Acc.conveyer belt: 0.6544, Acc.canopy: 0.2998, Acc.washer: 0.7760, Acc.plaything: 0.5142, Acc.swimming pool: 0.8225, Acc.stool: 0.3804, Acc.barrel: 0.6464, Acc.basket: 0.4889, Acc.waterfall: 0.8548, Acc.tent: 0.9854, Acc.bag: 0.0271, Acc.minibike: 0.7492, Acc.cradle: 0.9726, Acc.oven: 0.4943, Acc.ball: 0.6510, Acc.food: 0.6172, Acc.step: 0.0348, Acc.tank: 0.4976, Acc.trade name: 0.2405, Acc.microwave: 0.7210, Acc.pot: 0.3310, Acc.animal: 0.5586, Acc.bicycle: 0.7461, Acc.lake: 0.0000, Acc.dishwasher: 0.1741, Acc.screen: 0.9292, Acc.blanket: 0.0029, Acc.sculpture: 0.6223, Acc.hood: 0.4056, Acc.sconce: 0.1466, Acc.vase: 0.3504, Acc.traffic light: 0.3850, Acc.tray: 0.0016, Acc.ashcan: 0.5392, Acc.fan: 0.6422, Acc.pier: 0.4129, Acc.crt screen: 0.0000, Acc.plate: 0.6219, Acc.monitor: 0.0129, Acc.bulletin board: 0.4402, Acc.shower: 0.0000, Acc.radiator: 0.5090, Acc.glass: 0.0928, Acc.clock: 0.3358, Acc.flag: 0.5129
2023-11-10 13:59:45,099 - mmseg - INFO - Iter [16050/160000]	lr: 9.754e-05, eta: 1 day, 4:24:44, time: 2.579, data_time: 1.882, memory: 23129, decode.loss_ce: 0.5287, decode.acc_seg: 80.1223, loss: 0.5287
2023-11-10 14:00:20,731 - mmseg - INFO - Iter [16100/160000]	lr: 9.752e-05, eta: 1 day, 4:24:10, time: 0.713, data_time: 0.011, memory: 23129, decode.loss_ce: 0.5083, decode.acc_seg: 80.8153, loss: 0.5083
2023-11-10 14:00:56,081 - mmseg - INFO - Iter [16150/160000]	lr: 9.751e-05, eta: 1 day, 4:23:33, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5304, decode.acc_seg: 79.7609, loss: 0.5304
2023-11-10 14:01:31,081 - mmseg - INFO - Iter [16200/160000]	lr: 9.749e-05, eta: 1 day, 4:22:53, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5205, decode.acc_seg: 80.1934, loss: 0.5205
2023-11-10 14:02:04,521 - mmseg - INFO - Iter [16250/160000]	lr: 9.748e-05, eta: 1 day, 4:21:59, time: 0.670, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5322, decode.acc_seg: 80.5337, loss: 0.5322
2023-11-10 14:02:37,602 - mmseg - INFO - Iter [16300/160000]	lr: 9.746e-05, eta: 1 day, 4:21:02, time: 0.662, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5060, decode.acc_seg: 80.8857, loss: 0.5060
2023-11-10 14:03:12,816 - mmseg - INFO - Iter [16350/160000]	lr: 9.745e-05, eta: 1 day, 4:20:23, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5513, decode.acc_seg: 79.9087, loss: 0.5513
2023-11-10 14:03:47,600 - mmseg - INFO - Iter [16400/160000]	lr: 9.743e-05, eta: 1 day, 4:19:41, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5493, decode.acc_seg: 80.1966, loss: 0.5493
2023-11-10 14:04:22,728 - mmseg - INFO - Iter [16450/160000]	lr: 9.741e-05, eta: 1 day, 4:19:02, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5575, decode.acc_seg: 79.6810, loss: 0.5575
2023-11-10 14:04:57,203 - mmseg - INFO - Iter [16500/160000]	lr: 9.740e-05, eta: 1 day, 4:18:18, time: 0.690, data_time: 0.011, memory: 23129, decode.loss_ce: 0.5282, decode.acc_seg: 80.6379, loss: 0.5282
2023-11-10 14:05:31,639 - mmseg - INFO - Iter [16550/160000]	lr: 9.738e-05, eta: 1 day, 4:17:33, time: 0.689, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5091, decode.acc_seg: 80.7001, loss: 0.5091
2023-11-10 14:06:06,135 - mmseg - INFO - Iter [16600/160000]	lr: 9.737e-05, eta: 1 day, 4:16:49, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5268, decode.acc_seg: 80.8823, loss: 0.5268
2023-11-10 14:06:40,882 - mmseg - INFO - Iter [16650/160000]	lr: 9.735e-05, eta: 1 day, 4:16:07, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5300, decode.acc_seg: 80.8931, loss: 0.5300
2023-11-10 14:07:16,614 - mmseg - INFO - Iter [16700/160000]	lr: 9.734e-05, eta: 1 day, 4:15:34, time: 0.715, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4939, decode.acc_seg: 80.9767, loss: 0.4939
2023-11-10 14:07:51,908 - mmseg - INFO - Iter [16750/160000]	lr: 9.732e-05, eta: 1 day, 4:14:56, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5412, decode.acc_seg: 80.0251, loss: 0.5412
2023-11-10 14:08:26,297 - mmseg - INFO - Iter [16800/160000]	lr: 9.730e-05, eta: 1 day, 4:14:12, time: 0.688, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5196, decode.acc_seg: 80.3261, loss: 0.5196
2023-11-10 14:09:01,331 - mmseg - INFO - Iter [16850/160000]	lr: 9.729e-05, eta: 1 day, 4:13:32, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5140, decode.acc_seg: 80.8220, loss: 0.5140
2023-11-10 14:09:35,579 - mmseg - INFO - Iter [16900/160000]	lr: 9.727e-05, eta: 1 day, 4:12:47, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5135, decode.acc_seg: 80.7550, loss: 0.5135
2023-11-10 14:10:08,437 - mmseg - INFO - Iter [16950/160000]	lr: 9.726e-05, eta: 1 day, 4:11:49, time: 0.657, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5129, decode.acc_seg: 80.3881, loss: 0.5129
2023-11-10 14:10:40,446 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 14:10:40,447 - mmseg - INFO - Iter [17000/160000]	lr: 9.724e-05, eta: 1 day, 4:10:44, time: 0.639, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5162, decode.acc_seg: 80.5616, loss: 0.5162
2023-11-10 14:11:13,275 - mmseg - INFO - Iter [17050/160000]	lr: 9.722e-05, eta: 1 day, 4:09:46, time: 0.656, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5227, decode.acc_seg: 80.9065, loss: 0.5227
2023-11-10 14:11:48,635 - mmseg - INFO - Iter [17100/160000]	lr: 9.721e-05, eta: 1 day, 4:09:10, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5238, decode.acc_seg: 81.0634, loss: 0.5238
2023-11-10 14:12:24,117 - mmseg - INFO - Iter [17150/160000]	lr: 9.719e-05, eta: 1 day, 4:08:35, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4951, decode.acc_seg: 81.6186, loss: 0.4951
2023-11-10 14:12:59,596 - mmseg - INFO - Iter [17200/160000]	lr: 9.718e-05, eta: 1 day, 4:07:59, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5262, decode.acc_seg: 80.3918, loss: 0.5262
2023-11-10 14:13:32,778 - mmseg - INFO - Iter [17250/160000]	lr: 9.716e-05, eta: 1 day, 4:07:05, time: 0.663, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5156, decode.acc_seg: 81.2818, loss: 0.5156
2023-11-10 14:14:08,156 - mmseg - INFO - Iter [17300/160000]	lr: 9.714e-05, eta: 1 day, 4:06:29, time: 0.708, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5167, decode.acc_seg: 80.5316, loss: 0.5167
2023-11-10 14:14:42,892 - mmseg - INFO - Iter [17350/160000]	lr: 9.713e-05, eta: 1 day, 4:05:47, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5255, decode.acc_seg: 80.1660, loss: 0.5255
2023-11-10 14:15:16,713 - mmseg - INFO - Iter [17400/160000]	lr: 9.711e-05, eta: 1 day, 4:04:59, time: 0.678, data_time: 0.011, memory: 23129, decode.loss_ce: 0.5488, decode.acc_seg: 79.8952, loss: 0.5488
2023-11-10 14:15:48,990 - mmseg - INFO - Iter [17450/160000]	lr: 9.709e-05, eta: 1 day, 4:03:58, time: 0.645, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5169, decode.acc_seg: 80.2650, loss: 0.5169
2023-11-10 14:16:20,597 - mmseg - INFO - Iter [17500/160000]	lr: 9.708e-05, eta: 1 day, 4:02:51, time: 0.632, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5134, decode.acc_seg: 80.5323, loss: 0.5134
2023-11-10 14:16:52,862 - mmseg - INFO - Iter [17550/160000]	lr: 9.706e-05, eta: 1 day, 4:01:49, time: 0.644, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5202, decode.acc_seg: 80.9427, loss: 0.5202
2023-11-10 14:17:24,612 - mmseg - INFO - Iter [17600/160000]	lr: 9.704e-05, eta: 1 day, 4:00:45, time: 0.636, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5220, decode.acc_seg: 80.4139, loss: 0.5220
2023-11-10 14:17:56,427 - mmseg - INFO - Iter [17650/160000]	lr: 9.703e-05, eta: 1 day, 3:59:40, time: 0.636, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5171, decode.acc_seg: 80.8931, loss: 0.5171
2023-11-10 14:18:28,882 - mmseg - INFO - Iter [17700/160000]	lr: 9.701e-05, eta: 1 day, 3:58:41, time: 0.649, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5102, decode.acc_seg: 81.5032, loss: 0.5102
2023-11-10 14:19:03,275 - mmseg - INFO - Iter [17750/160000]	lr: 9.699e-05, eta: 1 day, 3:57:57, time: 0.687, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5323, decode.acc_seg: 80.6330, loss: 0.5323
2023-11-10 14:19:36,615 - mmseg - INFO - Iter [17800/160000]	lr: 9.698e-05, eta: 1 day, 3:57:06, time: 0.668, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5292, decode.acc_seg: 80.3621, loss: 0.5292
2023-11-10 14:20:11,516 - mmseg - INFO - Iter [17850/160000]	lr: 9.696e-05, eta: 1 day, 3:56:27, time: 0.697, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5143, decode.acc_seg: 80.2735, loss: 0.5143
2023-11-10 14:20:46,632 - mmseg - INFO - Iter [17900/160000]	lr: 9.694e-05, eta: 1 day, 3:55:49, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4920, decode.acc_seg: 81.9383, loss: 0.4920
2023-11-10 14:21:22,031 - mmseg - INFO - Iter [17950/160000]	lr: 9.693e-05, eta: 1 day, 3:55:14, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5114, decode.acc_seg: 81.4182, loss: 0.5114
2023-11-10 14:21:56,545 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 14:21:56,546 - mmseg - INFO - Iter [18000/160000]	lr: 9.691e-05, eta: 1 day, 3:54:31, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5159, decode.acc_seg: 80.6520, loss: 0.5159
2023-11-10 14:22:31,982 - mmseg - INFO - Iter [18050/160000]	lr: 9.689e-05, eta: 1 day, 3:53:57, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4751, decode.acc_seg: 82.0600, loss: 0.4751
2023-11-10 14:23:07,663 - mmseg - INFO - Iter [18100/160000]	lr: 9.688e-05, eta: 1 day, 3:53:24, time: 0.715, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4879, decode.acc_seg: 81.8995, loss: 0.4879
2023-11-10 14:23:42,014 - mmseg - INFO - Iter [18150/160000]	lr: 9.686e-05, eta: 1 day, 3:52:40, time: 0.686, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5296, decode.acc_seg: 80.7481, loss: 0.5296
2023-11-10 14:24:14,752 - mmseg - INFO - Iter [18200/160000]	lr: 9.684e-05, eta: 1 day, 3:51:44, time: 0.655, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5508, decode.acc_seg: 80.0558, loss: 0.5508
2023-11-10 14:24:50,058 - mmseg - INFO - Iter [18250/160000]	lr: 9.682e-05, eta: 1 day, 3:51:08, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5621, decode.acc_seg: 79.4459, loss: 0.5621
2023-11-10 14:25:25,429 - mmseg - INFO - Iter [18300/160000]	lr: 9.681e-05, eta: 1 day, 3:50:33, time: 0.707, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5090, decode.acc_seg: 80.5708, loss: 0.5090
2023-11-10 14:26:00,524 - mmseg - INFO - Iter [18350/160000]	lr: 9.679e-05, eta: 1 day, 3:49:56, time: 0.702, data_time: 0.011, memory: 23129, decode.loss_ce: 0.5178, decode.acc_seg: 80.5940, loss: 0.5178
2023-11-10 14:26:36,018 - mmseg - INFO - Iter [18400/160000]	lr: 9.677e-05, eta: 1 day, 3:49:21, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4944, decode.acc_seg: 81.1339, loss: 0.4944
2023-11-10 14:27:11,727 - mmseg - INFO - Iter [18450/160000]	lr: 9.676e-05, eta: 1 day, 3:48:48, time: 0.714, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5245, decode.acc_seg: 80.5855, loss: 0.5245
2023-11-10 14:27:45,277 - mmseg - INFO - Iter [18500/160000]	lr: 9.674e-05, eta: 1 day, 3:48:00, time: 0.672, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4879, decode.acc_seg: 81.4954, loss: 0.4879
2023-11-10 14:28:20,272 - mmseg - INFO - Iter [18550/160000]	lr: 9.672e-05, eta: 1 day, 3:47:21, time: 0.699, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4988, decode.acc_seg: 81.4514, loss: 0.4988
2023-11-10 14:28:54,099 - mmseg - INFO - Iter [18600/160000]	lr: 9.670e-05, eta: 1 day, 3:46:34, time: 0.677, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5091, decode.acc_seg: 80.8893, loss: 0.5091
2023-11-10 14:29:27,373 - mmseg - INFO - Iter [18650/160000]	lr: 9.669e-05, eta: 1 day, 3:45:43, time: 0.666, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4808, decode.acc_seg: 81.9120, loss: 0.4808
2023-11-10 14:29:59,933 - mmseg - INFO - Iter [18700/160000]	lr: 9.667e-05, eta: 1 day, 3:44:47, time: 0.651, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5297, decode.acc_seg: 80.8459, loss: 0.5297
2023-11-10 14:30:35,019 - mmseg - INFO - Iter [18750/160000]	lr: 9.665e-05, eta: 1 day, 3:44:09, time: 0.701, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4749, decode.acc_seg: 82.6356, loss: 0.4749
2023-11-10 14:31:07,320 - mmseg - INFO - Iter [18800/160000]	lr: 9.663e-05, eta: 1 day, 3:43:11, time: 0.646, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5054, decode.acc_seg: 80.8800, loss: 0.5054
2023-11-10 14:31:40,670 - mmseg - INFO - Iter [18850/160000]	lr: 9.661e-05, eta: 1 day, 3:42:20, time: 0.667, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4968, decode.acc_seg: 81.0316, loss: 0.4968
2023-11-10 14:32:15,794 - mmseg - INFO - Iter [18900/160000]	lr: 9.660e-05, eta: 1 day, 3:41:44, time: 0.704, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4792, decode.acc_seg: 82.2822, loss: 0.4792
2023-11-10 14:32:49,370 - mmseg - INFO - Iter [18950/160000]	lr: 9.658e-05, eta: 1 day, 3:40:55, time: 0.671, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4756, decode.acc_seg: 82.1799, loss: 0.4756
2023-11-10 14:33:24,053 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 14:33:24,054 - mmseg - INFO - Iter [19000/160000]	lr: 9.656e-05, eta: 1 day, 3:40:16, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5097, decode.acc_seg: 81.0555, loss: 0.5097
2023-11-10 14:33:55,505 - mmseg - INFO - Iter [19050/160000]	lr: 9.654e-05, eta: 1 day, 3:39:12, time: 0.629, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5160, decode.acc_seg: 81.4970, loss: 0.5160
2023-11-10 14:34:27,064 - mmseg - INFO - Iter [19100/160000]	lr: 9.653e-05, eta: 1 day, 3:38:09, time: 0.631, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5285, decode.acc_seg: 80.3395, loss: 0.5285
2023-11-10 14:35:01,940 - mmseg - INFO - Iter [19150/160000]	lr: 9.651e-05, eta: 1 day, 3:37:30, time: 0.696, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5126, decode.acc_seg: 80.4246, loss: 0.5126
2023-11-10 14:35:36,598 - mmseg - INFO - Iter [19200/160000]	lr: 9.649e-05, eta: 1 day, 3:36:50, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4912, decode.acc_seg: 81.4890, loss: 0.4912
2023-11-10 14:36:08,714 - mmseg - INFO - Iter [19250/160000]	lr: 9.647e-05, eta: 1 day, 3:35:51, time: 0.642, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4531, decode.acc_seg: 81.8923, loss: 0.4531
2023-11-10 14:36:42,784 - mmseg - INFO - Iter [19300/160000]	lr: 9.645e-05, eta: 1 day, 3:35:07, time: 0.681, data_time: 0.008, memory: 23129, decode.loss_ce: 0.4744, decode.acc_seg: 82.3930, loss: 0.4744
2023-11-10 14:37:15,176 - mmseg - INFO - Iter [19350/160000]	lr: 9.643e-05, eta: 1 day, 3:34:10, time: 0.648, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5246, decode.acc_seg: 80.3097, loss: 0.5246
2023-11-10 14:37:50,160 - mmseg - INFO - Iter [19400/160000]	lr: 9.642e-05, eta: 1 day, 3:33:33, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5283, decode.acc_seg: 80.7080, loss: 0.5283
2023-11-10 14:38:23,760 - mmseg - INFO - Iter [19450/160000]	lr: 9.640e-05, eta: 1 day, 3:32:46, time: 0.671, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4741, decode.acc_seg: 81.8319, loss: 0.4741
2023-11-10 14:38:57,734 - mmseg - INFO - Iter [19500/160000]	lr: 9.638e-05, eta: 1 day, 3:32:01, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5048, decode.acc_seg: 81.3855, loss: 0.5048
2023-11-10 14:39:33,047 - mmseg - INFO - Iter [19550/160000]	lr: 9.636e-05, eta: 1 day, 3:31:26, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4723, decode.acc_seg: 81.8717, loss: 0.4723
2023-11-10 14:40:08,092 - mmseg - INFO - Iter [19600/160000]	lr: 9.634e-05, eta: 1 day, 3:30:49, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5302, decode.acc_seg: 81.1959, loss: 0.5302
2023-11-10 14:40:43,140 - mmseg - INFO - Iter [19650/160000]	lr: 9.632e-05, eta: 1 day, 3:30:12, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5197, decode.acc_seg: 80.6489, loss: 0.5197
2023-11-10 14:41:17,887 - mmseg - INFO - Iter [19700/160000]	lr: 9.631e-05, eta: 1 day, 3:29:33, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4756, decode.acc_seg: 81.7444, loss: 0.4756
2023-11-10 14:41:51,962 - mmseg - INFO - Iter [19750/160000]	lr: 9.629e-05, eta: 1 day, 3:28:49, time: 0.680, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4738, decode.acc_seg: 81.9643, loss: 0.4738
2023-11-10 14:42:27,463 - mmseg - INFO - Iter [19800/160000]	lr: 9.627e-05, eta: 1 day, 3:28:16, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4733, decode.acc_seg: 82.0055, loss: 0.4733
2023-11-10 14:43:03,126 - mmseg - INFO - Iter [19850/160000]	lr: 9.625e-05, eta: 1 day, 3:27:43, time: 0.713, data_time: 0.011, memory: 23129, decode.loss_ce: 0.5093, decode.acc_seg: 80.9400, loss: 0.5093
2023-11-10 14:43:36,286 - mmseg - INFO - Iter [19900/160000]	lr: 9.623e-05, eta: 1 day, 3:26:53, time: 0.664, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4591, decode.acc_seg: 82.2172, loss: 0.4591
2023-11-10 14:44:07,639 - mmseg - INFO - Iter [19950/160000]	lr: 9.621e-05, eta: 1 day, 3:25:51, time: 0.627, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4831, decode.acc_seg: 81.8450, loss: 0.4831
2023-11-10 14:44:39,010 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 14:44:39,011 - mmseg - INFO - Iter [20000/160000]	lr: 9.619e-05, eta: 1 day, 3:24:48, time: 0.628, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4697, decode.acc_seg: 82.1560, loss: 0.4697
2023-11-10 14:45:12,380 - mmseg - INFO - Iter [20050/160000]	lr: 9.618e-05, eta: 1 day, 3:24:00, time: 0.666, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4605, decode.acc_seg: 82.7094, loss: 0.4605
2023-11-10 14:45:45,032 - mmseg - INFO - Iter [20100/160000]	lr: 9.616e-05, eta: 1 day, 3:23:06, time: 0.653, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4949, decode.acc_seg: 81.3423, loss: 0.4949
2023-11-10 14:46:20,178 - mmseg - INFO - Iter [20150/160000]	lr: 9.614e-05, eta: 1 day, 3:22:31, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4907, decode.acc_seg: 81.7071, loss: 0.4907
2023-11-10 14:46:53,133 - mmseg - INFO - Iter [20200/160000]	lr: 9.612e-05, eta: 1 day, 3:21:40, time: 0.659, data_time: 0.011, memory: 23129, decode.loss_ce: 0.5010, decode.acc_seg: 81.3112, loss: 0.5010
2023-11-10 14:47:28,433 - mmseg - INFO - Iter [20250/160000]	lr: 9.610e-05, eta: 1 day, 3:21:05, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4512, decode.acc_seg: 82.7524, loss: 0.4512
2023-11-10 14:48:02,864 - mmseg - INFO - Iter [20300/160000]	lr: 9.608e-05, eta: 1 day, 3:20:24, time: 0.689, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4713, decode.acc_seg: 81.8645, loss: 0.4713
2023-11-10 14:48:39,439 - mmseg - INFO - Iter [20350/160000]	lr: 9.606e-05, eta: 1 day, 3:19:58, time: 0.731, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4876, decode.acc_seg: 82.1115, loss: 0.4876
2023-11-10 14:49:15,285 - mmseg - INFO - Iter [20400/160000]	lr: 9.604e-05, eta: 1 day, 3:19:27, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5167, decode.acc_seg: 80.6966, loss: 0.5167
2023-11-10 14:49:47,679 - mmseg - INFO - Iter [20450/160000]	lr: 9.602e-05, eta: 1 day, 3:18:32, time: 0.648, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5077, decode.acc_seg: 81.0121, loss: 0.5077
2023-11-10 14:50:22,829 - mmseg - INFO - Iter [20500/160000]	lr: 9.600e-05, eta: 1 day, 3:17:57, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5000, decode.acc_seg: 81.4905, loss: 0.5000
2023-11-10 14:50:58,109 - mmseg - INFO - Iter [20550/160000]	lr: 9.599e-05, eta: 1 day, 3:17:22, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4640, decode.acc_seg: 82.3347, loss: 0.4640
2023-11-10 14:51:33,628 - mmseg - INFO - Iter [20600/160000]	lr: 9.597e-05, eta: 1 day, 3:16:48, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4472, decode.acc_seg: 82.9993, loss: 0.4472
2023-11-10 14:52:09,467 - mmseg - INFO - Iter [20650/160000]	lr: 9.595e-05, eta: 1 day, 3:16:17, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4880, decode.acc_seg: 81.8385, loss: 0.4880
2023-11-10 14:52:45,352 - mmseg - INFO - Iter [20700/160000]	lr: 9.593e-05, eta: 1 day, 3:15:47, time: 0.718, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5049, decode.acc_seg: 81.0376, loss: 0.5049
2023-11-10 14:53:20,985 - mmseg - INFO - Iter [20750/160000]	lr: 9.591e-05, eta: 1 day, 3:15:14, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5111, decode.acc_seg: 81.1969, loss: 0.5111
2023-11-10 14:53:54,970 - mmseg - INFO - Iter [20800/160000]	lr: 9.589e-05, eta: 1 day, 3:14:31, time: 0.680, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4803, decode.acc_seg: 82.8214, loss: 0.4803
2023-11-10 14:54:29,973 - mmseg - INFO - Iter [20850/160000]	lr: 9.587e-05, eta: 1 day, 3:13:54, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5002, decode.acc_seg: 81.3472, loss: 0.5002
2023-11-10 14:55:04,095 - mmseg - INFO - Iter [20900/160000]	lr: 9.585e-05, eta: 1 day, 3:13:11, time: 0.682, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5105, decode.acc_seg: 81.2592, loss: 0.5105
2023-11-10 14:55:39,381 - mmseg - INFO - Iter [20950/160000]	lr: 9.583e-05, eta: 1 day, 3:12:36, time: 0.706, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4879, decode.acc_seg: 81.8532, loss: 0.4879
2023-11-10 14:56:14,006 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 14:56:14,007 - mmseg - INFO - Iter [21000/160000]	lr: 9.581e-05, eta: 1 day, 3:11:57, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5161, decode.acc_seg: 80.8434, loss: 0.5161
2023-11-10 14:56:49,094 - mmseg - INFO - Iter [21050/160000]	lr: 9.579e-05, eta: 1 day, 3:11:21, time: 0.702, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4613, decode.acc_seg: 82.4259, loss: 0.4613
2023-11-10 14:57:24,672 - mmseg - INFO - Iter [21100/160000]	lr: 9.577e-05, eta: 1 day, 3:10:48, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4758, decode.acc_seg: 83.1785, loss: 0.4758
2023-11-10 14:57:59,899 - mmseg - INFO - Iter [21150/160000]	lr: 9.575e-05, eta: 1 day, 3:10:13, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5053, decode.acc_seg: 80.8525, loss: 0.5053
2023-11-10 14:58:35,131 - mmseg - INFO - Iter [21200/160000]	lr: 9.573e-05, eta: 1 day, 3:09:38, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.5166, decode.acc_seg: 81.4624, loss: 0.5166
2023-11-10 14:59:07,233 - mmseg - INFO - Iter [21250/160000]	lr: 9.571e-05, eta: 1 day, 3:08:42, time: 0.642, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4773, decode.acc_seg: 82.2410, loss: 0.4773
2023-11-10 14:59:39,903 - mmseg - INFO - Iter [21300/160000]	lr: 9.569e-05, eta: 1 day, 3:07:51, time: 0.653, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4578, decode.acc_seg: 82.1890, loss: 0.4578
2023-11-10 15:00:13,531 - mmseg - INFO - Iter [21350/160000]	lr: 9.567e-05, eta: 1 day, 3:07:05, time: 0.674, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4628, decode.acc_seg: 82.7101, loss: 0.4628
2023-11-10 15:00:46,386 - mmseg - INFO - Iter [21400/160000]	lr: 9.565e-05, eta: 1 day, 3:06:15, time: 0.657, data_time: 0.008, memory: 23129, decode.loss_ce: 0.4789, decode.acc_seg: 82.0760, loss: 0.4789
2023-11-10 15:01:19,212 - mmseg - INFO - Iter [21450/160000]	lr: 9.563e-05, eta: 1 day, 3:05:24, time: 0.657, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4952, decode.acc_seg: 81.3101, loss: 0.4952
2023-11-10 15:01:52,122 - mmseg - INFO - Iter [21500/160000]	lr: 9.561e-05, eta: 1 day, 3:04:34, time: 0.658, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4907, decode.acc_seg: 81.4300, loss: 0.4907
2023-11-10 15:02:26,696 - mmseg - INFO - Iter [21550/160000]	lr: 9.559e-05, eta: 1 day, 3:03:55, time: 0.691, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4748, decode.acc_seg: 82.1916, loss: 0.4748
2023-11-10 15:03:01,515 - mmseg - INFO - Iter [21600/160000]	lr: 9.557e-05, eta: 1 day, 3:03:18, time: 0.697, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4788, decode.acc_seg: 82.0053, loss: 0.4788
2023-11-10 15:03:36,085 - mmseg - INFO - Iter [21650/160000]	lr: 9.555e-05, eta: 1 day, 3:02:38, time: 0.690, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4911, decode.acc_seg: 81.7009, loss: 0.4911
2023-11-10 15:04:10,009 - mmseg - INFO - Iter [21700/160000]	lr: 9.553e-05, eta: 1 day, 3:01:55, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4680, decode.acc_seg: 81.9202, loss: 0.4680
2023-11-10 15:04:45,218 - mmseg - INFO - Iter [21750/160000]	lr: 9.551e-05, eta: 1 day, 3:01:20, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4532, decode.acc_seg: 82.6167, loss: 0.4532
2023-11-10 15:05:19,166 - mmseg - INFO - Iter [21800/160000]	lr: 9.549e-05, eta: 1 day, 3:00:37, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4579, decode.acc_seg: 82.7769, loss: 0.4579
2023-11-10 15:05:54,170 - mmseg - INFO - Iter [21850/160000]	lr: 9.547e-05, eta: 1 day, 3:00:01, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4681, decode.acc_seg: 82.3174, loss: 0.4681
2023-11-10 15:06:29,272 - mmseg - INFO - Iter [21900/160000]	lr: 9.545e-05, eta: 1 day, 2:59:25, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4719, decode.acc_seg: 82.5419, loss: 0.4719
2023-11-10 15:07:04,415 - mmseg - INFO - Iter [21950/160000]	lr: 9.543e-05, eta: 1 day, 2:58:50, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4897, decode.acc_seg: 82.1525, loss: 0.4897
2023-11-10 15:07:39,552 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 15:07:39,553 - mmseg - INFO - Iter [22000/160000]	lr: 9.541e-05, eta: 1 day, 2:58:14, time: 0.702, data_time: 0.008, memory: 23129, decode.loss_ce: 0.4706, decode.acc_seg: 82.3432, loss: 0.4706
2023-11-10 15:08:15,155 - mmseg - INFO - Iter [22050/160000]	lr: 9.539e-05, eta: 1 day, 2:57:42, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4751, decode.acc_seg: 82.5065, loss: 0.4751
2023-11-10 15:08:50,190 - mmseg - INFO - Iter [22100/160000]	lr: 9.537e-05, eta: 1 day, 2:57:06, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4690, decode.acc_seg: 82.8162, loss: 0.4690
2023-11-10 15:09:24,686 - mmseg - INFO - Iter [22150/160000]	lr: 9.535e-05, eta: 1 day, 2:56:26, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4588, decode.acc_seg: 82.6115, loss: 0.4588
2023-11-10 15:09:59,483 - mmseg - INFO - Iter [22200/160000]	lr: 9.533e-05, eta: 1 day, 2:55:49, time: 0.695, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4698, decode.acc_seg: 82.7777, loss: 0.4698
2023-11-10 15:10:34,898 - mmseg - INFO - Iter [22250/160000]	lr: 9.530e-05, eta: 1 day, 2:55:15, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4858, decode.acc_seg: 81.2567, loss: 0.4858
2023-11-10 15:11:10,172 - mmseg - INFO - Iter [22300/160000]	lr: 9.528e-05, eta: 1 day, 2:54:40, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4431, decode.acc_seg: 83.8399, loss: 0.4431
2023-11-10 15:11:45,307 - mmseg - INFO - Iter [22350/160000]	lr: 9.526e-05, eta: 1 day, 2:54:05, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4558, decode.acc_seg: 82.4252, loss: 0.4558
2023-11-10 15:12:21,308 - mmseg - INFO - Iter [22400/160000]	lr: 9.524e-05, eta: 1 day, 2:53:35, time: 0.720, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4658, decode.acc_seg: 82.6677, loss: 0.4658
2023-11-10 15:12:53,797 - mmseg - INFO - Iter [22450/160000]	lr: 9.522e-05, eta: 1 day, 2:52:43, time: 0.650, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4904, decode.acc_seg: 82.1083, loss: 0.4904
2023-11-10 15:13:25,638 - mmseg - INFO - Iter [22500/160000]	lr: 9.520e-05, eta: 1 day, 2:51:48, time: 0.636, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4606, decode.acc_seg: 83.0803, loss: 0.4606
2023-11-10 15:14:01,248 - mmseg - INFO - Iter [22550/160000]	lr: 9.518e-05, eta: 1 day, 2:51:15, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4224, decode.acc_seg: 83.7363, loss: 0.4224
2023-11-10 15:14:34,550 - mmseg - INFO - Iter [22600/160000]	lr: 9.516e-05, eta: 1 day, 2:50:29, time: 0.667, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4744, decode.acc_seg: 82.1795, loss: 0.4744
2023-11-10 15:15:09,001 - mmseg - INFO - Iter [22650/160000]	lr: 9.514e-05, eta: 1 day, 2:49:49, time: 0.688, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4979, decode.acc_seg: 82.0732, loss: 0.4979
2023-11-10 15:15:44,219 - mmseg - INFO - Iter [22700/160000]	lr: 9.512e-05, eta: 1 day, 2:49:14, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4481, decode.acc_seg: 82.9256, loss: 0.4481
2023-11-10 15:16:19,519 - mmseg - INFO - Iter [22750/160000]	lr: 9.509e-05, eta: 1 day, 2:48:40, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4884, decode.acc_seg: 81.2318, loss: 0.4884
2023-11-10 15:16:54,768 - mmseg - INFO - Iter [22800/160000]	lr: 9.507e-05, eta: 1 day, 2:48:05, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4847, decode.acc_seg: 81.4741, loss: 0.4847
2023-11-10 15:17:30,638 - mmseg - INFO - Iter [22850/160000]	lr: 9.505e-05, eta: 1 day, 2:47:34, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4743, decode.acc_seg: 82.3095, loss: 0.4743
2023-11-10 15:18:06,285 - mmseg - INFO - Iter [22900/160000]	lr: 9.503e-05, eta: 1 day, 2:47:02, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4509, decode.acc_seg: 83.0420, loss: 0.4509
2023-11-10 15:18:40,913 - mmseg - INFO - Iter [22950/160000]	lr: 9.501e-05, eta: 1 day, 2:46:24, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4296, decode.acc_seg: 83.9371, loss: 0.4296
2023-11-10 15:19:15,273 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 15:19:15,273 - mmseg - INFO - Iter [23000/160000]	lr: 9.499e-05, eta: 1 day, 2:45:44, time: 0.688, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4476, decode.acc_seg: 83.1461, loss: 0.4476
2023-11-10 15:19:48,296 - mmseg - INFO - Iter [23050/160000]	lr: 9.497e-05, eta: 1 day, 2:44:56, time: 0.660, data_time: 0.009, memory: 23129, decode.loss_ce: 0.5048, decode.acc_seg: 81.2346, loss: 0.5048
2023-11-10 15:20:21,354 - mmseg - INFO - Iter [23100/160000]	lr: 9.494e-05, eta: 1 day, 2:44:09, time: 0.661, data_time: 0.008, memory: 23129, decode.loss_ce: 0.4808, decode.acc_seg: 81.7629, loss: 0.4808
2023-11-10 15:20:56,193 - mmseg - INFO - Iter [23150/160000]	lr: 9.492e-05, eta: 1 day, 2:43:31, time: 0.696, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4827, decode.acc_seg: 82.0915, loss: 0.4827
2023-11-10 15:21:31,956 - mmseg - INFO - Iter [23200/160000]	lr: 9.490e-05, eta: 1 day, 2:43:00, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4648, decode.acc_seg: 82.7008, loss: 0.4648
2023-11-10 15:22:06,619 - mmseg - INFO - Iter [23250/160000]	lr: 9.488e-05, eta: 1 day, 2:42:22, time: 0.694, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4582, decode.acc_seg: 81.8185, loss: 0.4582
2023-11-10 15:22:41,559 - mmseg - INFO - Iter [23300/160000]	lr: 9.486e-05, eta: 1 day, 2:41:45, time: 0.698, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4893, decode.acc_seg: 82.2255, loss: 0.4893
2023-11-10 15:23:14,687 - mmseg - INFO - Iter [23350/160000]	lr: 9.484e-05, eta: 1 day, 2:40:59, time: 0.664, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4459, decode.acc_seg: 82.7631, loss: 0.4459
2023-11-10 15:23:46,988 - mmseg - INFO - Iter [23400/160000]	lr: 9.482e-05, eta: 1 day, 2:40:07, time: 0.645, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4631, decode.acc_seg: 82.4776, loss: 0.4631
2023-11-10 15:24:21,752 - mmseg - INFO - Iter [23450/160000]	lr: 9.479e-05, eta: 1 day, 2:39:29, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4525, decode.acc_seg: 82.4287, loss: 0.4525
2023-11-10 15:24:54,567 - mmseg - INFO - Iter [23500/160000]	lr: 9.477e-05, eta: 1 day, 2:38:40, time: 0.656, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4572, decode.acc_seg: 82.5371, loss: 0.4572
2023-11-10 15:25:28,128 - mmseg - INFO - Iter [23550/160000]	lr: 9.475e-05, eta: 1 day, 2:37:56, time: 0.672, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4477, decode.acc_seg: 83.2327, loss: 0.4477
2023-11-10 15:26:01,369 - mmseg - INFO - Iter [23600/160000]	lr: 9.473e-05, eta: 1 day, 2:37:10, time: 0.664, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4707, decode.acc_seg: 82.4072, loss: 0.4707
2023-11-10 15:26:36,101 - mmseg - INFO - Iter [23650/160000]	lr: 9.471e-05, eta: 1 day, 2:36:33, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4603, decode.acc_seg: 83.1736, loss: 0.4603
2023-11-10 15:27:09,908 - mmseg - INFO - Iter [23700/160000]	lr: 9.468e-05, eta: 1 day, 2:35:50, time: 0.676, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4637, decode.acc_seg: 81.9870, loss: 0.4637
2023-11-10 15:27:46,160 - mmseg - INFO - Iter [23750/160000]	lr: 9.466e-05, eta: 1 day, 2:35:21, time: 0.725, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4327, decode.acc_seg: 83.3761, loss: 0.4327
2023-11-10 15:28:21,459 - mmseg - INFO - Iter [23800/160000]	lr: 9.464e-05, eta: 1 day, 2:34:48, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4390, decode.acc_seg: 83.3492, loss: 0.4390
2023-11-10 15:28:52,904 - mmseg - INFO - Iter [23850/160000]	lr: 9.462e-05, eta: 1 day, 2:33:51, time: 0.629, data_time: 0.008, memory: 23129, decode.loss_ce: 0.4805, decode.acc_seg: 81.9367, loss: 0.4805
2023-11-10 15:29:27,510 - mmseg - INFO - Iter [23900/160000]	lr: 9.460e-05, eta: 1 day, 2:33:13, time: 0.692, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4477, decode.acc_seg: 82.3515, loss: 0.4477
2023-11-10 15:30:02,289 - mmseg - INFO - Iter [23950/160000]	lr: 9.457e-05, eta: 1 day, 2:32:36, time: 0.695, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4646, decode.acc_seg: 82.6677, loss: 0.4646
2023-11-10 15:30:37,600 - mmseg - INFO - Saving checkpoint at 24000 iterations
2023-11-10 15:30:42,452 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 15:30:42,452 - mmseg - INFO - Iter [24000/160000]	lr: 9.455e-05, eta: 1 day, 2:32:30, time: 0.804, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4421, decode.acc_seg: 83.1142, loss: 0.4421
2023-11-10 15:32:26,601 - mmseg - INFO - per class results:
2023-11-10 15:32:26,615 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 73.87 | 84.71 |
|       building      | 80.36 | 87.46 |
|         sky         | 93.93 |  97.6 |
|        floor        | 80.58 | 88.09 |
|         tree        | 73.95 | 89.06 |
|       ceiling       | 80.79 | 94.47 |
|         road        | 78.53 | 83.07 |
|         bed         | 83.86 | 94.19 |
|      windowpane     | 58.15 | 76.68 |
|        grass        | 62.72 | 77.65 |
|       cabinet       |  57.0 | 69.56 |
|       sidewalk      | 58.69 | 84.85 |
|        person       | 77.08 | 92.25 |
|        earth        | 34.97 | 49.59 |
|         door        | 42.34 | 53.56 |
|        table        | 53.77 | 71.35 |
|       mountain      | 53.99 | 68.35 |
|        plant        | 50.93 | 61.42 |
|       curtain       | 67.77 | 85.06 |
|        chair        | 49.95 | 64.16 |
|         car         | 80.92 | 90.25 |
|        water        | 45.41 | 53.56 |
|       painting      | 67.81 | 86.33 |
|         sofa        | 58.13 | 80.84 |
|        shelf        | 40.02 | 57.73 |
|        house        | 39.04 | 74.28 |
|         sea         |  54.4 | 83.64 |
|        mirror       | 61.05 | 73.42 |
|         rug         | 64.87 | 83.43 |
|        field        | 28.18 | 55.65 |
|       armchair      | 30.83 | 44.88 |
|         seat        | 59.19 | 73.14 |
|        fence        | 42.16 |  57.4 |
|         desk        | 40.43 | 50.48 |
|         rock        | 38.87 | 64.25 |
|       wardrobe      | 46.85 | 61.23 |
|         lamp        | 56.89 | 71.12 |
|       bathtub       | 76.27 | 82.33 |
|       railing       |  33.9 | 48.04 |
|       cushion       | 52.01 | 66.62 |
|         base        | 19.99 | 33.62 |
|         box         | 19.25 | 23.95 |
|        column       | 43.13 | 55.18 |
|      signboard      | 34.72 | 47.76 |
|   chest of drawers  |  36.9 | 48.05 |
|       counter       | 27.68 |  38.2 |
|         sand        | 30.63 | 48.79 |
|         sink        | 62.28 | 74.58 |
|      skyscraper     | 54.62 | 60.82 |
|      fireplace      | 72.42 | 86.19 |
|     refrigerator    | 58.84 |  84.5 |
|      grandstand     |  38.2 | 64.16 |
|         path        | 23.57 | 31.43 |
|        stairs       | 33.04 | 44.34 |
|        runway       |  71.4 | 95.93 |
|         case        | 54.81 |  74.6 |
|      pool table     | 91.23 | 96.67 |
|        pillow       |  53.7 | 71.85 |
|     screen door     | 71.56 | 77.27 |
|       stairway      | 28.27 | 33.95 |
|        river        | 12.04 | 36.08 |
|        bridge       |  60.6 | 78.82 |
|       bookcase      | 35.55 |  60.4 |
|        blind        | 38.05 | 41.73 |
|     coffee table    | 46.29 | 83.85 |
|        toilet       |  80.0 |  85.8 |
|        flower       | 28.79 |  49.1 |
|         book        | 40.41 | 52.12 |
|         hill        |  5.87 | 12.42 |
|        bench        | 35.59 | 52.94 |
|      countertop     |  39.9 | 44.75 |
|        stove        | 59.08 | 69.44 |
|         palm        | 43.11 | 52.57 |
|    kitchen island   | 33.78 | 80.59 |
|       computer      | 66.82 | 80.63 |
|     swivel chair    | 41.05 | 61.29 |
|         boat        | 69.16 | 79.95 |
|         bar         | 45.18 | 66.87 |
|    arcade machine   | 48.01 |  49.8 |
|        hovel        |  46.6 | 66.07 |
|         bus         | 82.95 | 93.37 |
|        towel        |  58.0 | 64.29 |
|        light        | 42.99 | 52.43 |
|        truck        | 24.35 | 42.28 |
|        tower        |  26.5 | 49.29 |
|      chandelier     | 64.16 | 78.93 |
|        awning       | 23.05 | 28.64 |
|     streetlight     | 18.87 | 24.94 |
|        booth        | 39.85 | 73.49 |
| television receiver |  61.4 | 80.63 |
|       airplane      | 41.26 | 65.36 |
|      dirt track     |  0.0  |  0.0  |
|       apparel       | 30.69 | 41.57 |
|         pole        | 14.91 | 18.94 |
|         land        |  0.01 |  0.02 |
|      bannister      |  10.1 |  14.7 |
|      escalator      |  5.68 |  5.68 |
|       ottoman       | 37.17 | 67.07 |
|        bottle       | 19.79 | 22.95 |
|        buffet       | 36.95 | 47.71 |
|        poster       | 22.74 | 27.12 |
|        stage        | 10.88 | 24.17 |
|         van         | 36.43 | 50.36 |
|         ship        | 79.43 |  82.2 |
|       fountain      |  9.48 |  9.57 |
|    conveyer belt    | 65.63 | 91.31 |
|        canopy       |  5.85 |  7.29 |
|        washer       |  69.5 | 75.99 |
|      plaything      | 15.74 | 27.01 |
|    swimming pool    | 49.66 | 50.11 |
|        stool        |  27.1 | 40.66 |
|        barrel       | 53.28 | 63.49 |
|        basket       |  23.7 | 25.79 |
|      waterfall      |  60.5 | 93.68 |
|         tent        | 56.61 | 97.94 |
|         bag         |  4.01 |  4.16 |
|       minibike      | 66.61 | 78.51 |
|        cradle       | 69.86 | 97.65 |
|         oven        | 24.04 |  58.0 |
|         ball        | 48.28 | 62.32 |
|         food        | 48.92 | 56.33 |
|         step        |  8.78 | 11.16 |
|         tank        |  28.3 | 31.51 |
|      trade name     | 23.74 | 26.73 |
|      microwave      | 54.79 | 61.47 |
|         pot         | 37.94 | 43.11 |
|        animal       | 59.27 | 61.88 |
|       bicycle       |  57.4 |  77.2 |
|         lake        | 46.91 | 72.47 |
|      dishwasher     | 39.15 | 77.87 |
|        screen       | 58.71 |  91.4 |
|       blanket       |  1.03 |  1.12 |
|      sculpture      |  60.8 | 69.38 |
|         hood        | 45.88 | 61.35 |
|        sconce       | 21.51 | 24.74 |
|         vase        | 32.95 | 48.32 |
|    traffic light    | 21.25 |  38.3 |
|         tray        |  1.46 |  1.93 |
|        ashcan       | 34.65 | 48.15 |
|         fan         | 51.81 | 67.78 |
|         pier        | 26.29 | 30.85 |
|      crt screen     |  4.69 | 10.42 |
|        plate        | 50.76 | 67.61 |
|       monitor       | 17.29 | 24.32 |
|    bulletin board   | 35.17 | 71.63 |
|        shower       |  0.0  |  0.0  |
|       radiator      | 50.61 | 54.02 |
|        glass        |  6.96 |  7.27 |
|        clock        | 13.47 |  14.5 |
|         flag        | 39.95 | 48.36 |
+---------------------+-------+-------+
2023-11-10 15:32:26,615 - mmseg - INFO - Summary:
2023-11-10 15:32:26,616 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 80.45 | 43.67 | 57.13 |
+-------+-------+-------+
2023-11-10 15:32:26,769 - mmseg - INFO - The previous best checkpoint /data/Next-ViT/segmentation/work_dirs/fpn_512_debi_base_160k/best_mIoU_iter_16000.pth was removed
2023-11-10 15:32:29,644 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_24000.pth.
2023-11-10 15:32:29,645 - mmseg - INFO - Best mIoU is 0.4367 at 24000 iter.
2023-11-10 15:32:29,685 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 15:32:29,685 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8045, mIoU: 0.4367, mAcc: 0.5713, IoU.wall: 0.7387, IoU.building: 0.8036, IoU.sky: 0.9393, IoU.floor: 0.8058, IoU.tree: 0.7395, IoU.ceiling: 0.8079, IoU.road: 0.7853, IoU.bed : 0.8386, IoU.windowpane: 0.5815, IoU.grass: 0.6272, IoU.cabinet: 0.5700, IoU.sidewalk: 0.5869, IoU.person: 0.7708, IoU.earth: 0.3497, IoU.door: 0.4234, IoU.table: 0.5377, IoU.mountain: 0.5399, IoU.plant: 0.5093, IoU.curtain: 0.6777, IoU.chair: 0.4995, IoU.car: 0.8092, IoU.water: 0.4541, IoU.painting: 0.6781, IoU.sofa: 0.5813, IoU.shelf: 0.4002, IoU.house: 0.3904, IoU.sea: 0.5440, IoU.mirror: 0.6105, IoU.rug: 0.6487, IoU.field: 0.2818, IoU.armchair: 0.3083, IoU.seat: 0.5919, IoU.fence: 0.4216, IoU.desk: 0.4043, IoU.rock: 0.3887, IoU.wardrobe: 0.4685, IoU.lamp: 0.5689, IoU.bathtub: 0.7627, IoU.railing: 0.3390, IoU.cushion: 0.5201, IoU.base: 0.1999, IoU.box: 0.1925, IoU.column: 0.4313, IoU.signboard: 0.3472, IoU.chest of drawers: 0.3690, IoU.counter: 0.2768, IoU.sand: 0.3063, IoU.sink: 0.6228, IoU.skyscraper: 0.5462, IoU.fireplace: 0.7242, IoU.refrigerator: 0.5884, IoU.grandstand: 0.3820, IoU.path: 0.2357, IoU.stairs: 0.3304, IoU.runway: 0.7140, IoU.case: 0.5481, IoU.pool table: 0.9123, IoU.pillow: 0.5370, IoU.screen door: 0.7156, IoU.stairway: 0.2827, IoU.river: 0.1204, IoU.bridge: 0.6060, IoU.bookcase: 0.3555, IoU.blind: 0.3805, IoU.coffee table: 0.4629, IoU.toilet: 0.8000, IoU.flower: 0.2879, IoU.book: 0.4041, IoU.hill: 0.0587, IoU.bench: 0.3559, IoU.countertop: 0.3990, IoU.stove: 0.5908, IoU.palm: 0.4311, IoU.kitchen island: 0.3378, IoU.computer: 0.6682, IoU.swivel chair: 0.4105, IoU.boat: 0.6916, IoU.bar: 0.4518, IoU.arcade machine: 0.4801, IoU.hovel: 0.4660, IoU.bus: 0.8295, IoU.towel: 0.5800, IoU.light: 0.4299, IoU.truck: 0.2435, IoU.tower: 0.2650, IoU.chandelier: 0.6416, IoU.awning: 0.2305, IoU.streetlight: 0.1887, IoU.booth: 0.3985, IoU.television receiver: 0.6140, IoU.airplane: 0.4126, IoU.dirt track: 0.0000, IoU.apparel: 0.3069, IoU.pole: 0.1491, IoU.land: 0.0001, IoU.bannister: 0.1010, IoU.escalator: 0.0568, IoU.ottoman: 0.3717, IoU.bottle: 0.1979, IoU.buffet: 0.3695, IoU.poster: 0.2274, IoU.stage: 0.1088, IoU.van: 0.3643, IoU.ship: 0.7943, IoU.fountain: 0.0948, IoU.conveyer belt: 0.6563, IoU.canopy: 0.0585, IoU.washer: 0.6950, IoU.plaything: 0.1574, IoU.swimming pool: 0.4966, IoU.stool: 0.2710, IoU.barrel: 0.5328, IoU.basket: 0.2370, IoU.waterfall: 0.6050, IoU.tent: 0.5661, IoU.bag: 0.0401, IoU.minibike: 0.6661, IoU.cradle: 0.6986, IoU.oven: 0.2404, IoU.ball: 0.4828, IoU.food: 0.4892, IoU.step: 0.0878, IoU.tank: 0.2830, IoU.trade name: 0.2374, IoU.microwave: 0.5479, IoU.pot: 0.3794, IoU.animal: 0.5927, IoU.bicycle: 0.5740, IoU.lake: 0.4691, IoU.dishwasher: 0.3915, IoU.screen: 0.5871, IoU.blanket: 0.0103, IoU.sculpture: 0.6080, IoU.hood: 0.4588, IoU.sconce: 0.2151, IoU.vase: 0.3295, IoU.traffic light: 0.2125, IoU.tray: 0.0146, IoU.ashcan: 0.3465, IoU.fan: 0.5181, IoU.pier: 0.2629, IoU.crt screen: 0.0469, IoU.plate: 0.5076, IoU.monitor: 0.1729, IoU.bulletin board: 0.3517, IoU.shower: 0.0000, IoU.radiator: 0.5061, IoU.glass: 0.0696, IoU.clock: 0.1347, IoU.flag: 0.3995, Acc.wall: 0.8471, Acc.building: 0.8746, Acc.sky: 0.9760, Acc.floor: 0.8809, Acc.tree: 0.8906, Acc.ceiling: 0.9447, Acc.road: 0.8307, Acc.bed : 0.9419, Acc.windowpane: 0.7668, Acc.grass: 0.7765, Acc.cabinet: 0.6956, Acc.sidewalk: 0.8485, Acc.person: 0.9225, Acc.earth: 0.4959, Acc.door: 0.5356, Acc.table: 0.7135, Acc.mountain: 0.6835, Acc.plant: 0.6142, Acc.curtain: 0.8506, Acc.chair: 0.6416, Acc.car: 0.9025, Acc.water: 0.5356, Acc.painting: 0.8633, Acc.sofa: 0.8084, Acc.shelf: 0.5773, Acc.house: 0.7428, Acc.sea: 0.8364, Acc.mirror: 0.7342, Acc.rug: 0.8343, Acc.field: 0.5565, Acc.armchair: 0.4488, Acc.seat: 0.7314, Acc.fence: 0.5740, Acc.desk: 0.5048, Acc.rock: 0.6425, Acc.wardrobe: 0.6123, Acc.lamp: 0.7112, Acc.bathtub: 0.8233, Acc.railing: 0.4804, Acc.cushion: 0.6662, Acc.base: 0.3362, Acc.box: 0.2395, Acc.column: 0.5518, Acc.signboard: 0.4776, Acc.chest of drawers: 0.4805, Acc.counter: 0.3820, Acc.sand: 0.4879, Acc.sink: 0.7458, Acc.skyscraper: 0.6082, Acc.fireplace: 0.8619, Acc.refrigerator: 0.8450, Acc.grandstand: 0.6416, Acc.path: 0.3143, Acc.stairs: 0.4434, Acc.runway: 0.9593, Acc.case: 0.7460, Acc.pool table: 0.9667, Acc.pillow: 0.7185, Acc.screen door: 0.7727, Acc.stairway: 0.3395, Acc.river: 0.3608, Acc.bridge: 0.7882, Acc.bookcase: 0.6040, Acc.blind: 0.4173, Acc.coffee table: 0.8385, Acc.toilet: 0.8580, Acc.flower: 0.4910, Acc.book: 0.5212, Acc.hill: 0.1242, Acc.bench: 0.5294, Acc.countertop: 0.4475, Acc.stove: 0.6944, Acc.palm: 0.5257, Acc.kitchen island: 0.8059, Acc.computer: 0.8063, Acc.swivel chair: 0.6129, Acc.boat: 0.7995, Acc.bar: 0.6687, Acc.arcade machine: 0.4980, Acc.hovel: 0.6607, Acc.bus: 0.9337, Acc.towel: 0.6429, Acc.light: 0.5243, Acc.truck: 0.4228, Acc.tower: 0.4929, Acc.chandelier: 0.7893, Acc.awning: 0.2864, Acc.streetlight: 0.2494, Acc.booth: 0.7349, Acc.television receiver: 0.8063, Acc.airplane: 0.6536, Acc.dirt track: 0.0000, Acc.apparel: 0.4157, Acc.pole: 0.1894, Acc.land: 0.0002, Acc.bannister: 0.1470, Acc.escalator: 0.0568, Acc.ottoman: 0.6707, Acc.bottle: 0.2295, Acc.buffet: 0.4771, Acc.poster: 0.2712, Acc.stage: 0.2417, Acc.van: 0.5036, Acc.ship: 0.8220, Acc.fountain: 0.0957, Acc.conveyer belt: 0.9131, Acc.canopy: 0.0729, Acc.washer: 0.7599, Acc.plaything: 0.2701, Acc.swimming pool: 0.5011, Acc.stool: 0.4066, Acc.barrel: 0.6349, Acc.basket: 0.2579, Acc.waterfall: 0.9368, Acc.tent: 0.9794, Acc.bag: 0.0416, Acc.minibike: 0.7851, Acc.cradle: 0.9765, Acc.oven: 0.5800, Acc.ball: 0.6232, Acc.food: 0.5633, Acc.step: 0.1116, Acc.tank: 0.3151, Acc.trade name: 0.2673, Acc.microwave: 0.6147, Acc.pot: 0.4311, Acc.animal: 0.6188, Acc.bicycle: 0.7720, Acc.lake: 0.7247, Acc.dishwasher: 0.7787, Acc.screen: 0.9140, Acc.blanket: 0.0112, Acc.sculpture: 0.6938, Acc.hood: 0.6135, Acc.sconce: 0.2474, Acc.vase: 0.4832, Acc.traffic light: 0.3830, Acc.tray: 0.0193, Acc.ashcan: 0.4815, Acc.fan: 0.6778, Acc.pier: 0.3085, Acc.crt screen: 0.1042, Acc.plate: 0.6761, Acc.monitor: 0.2432, Acc.bulletin board: 0.7163, Acc.shower: 0.0000, Acc.radiator: 0.5402, Acc.glass: 0.0727, Acc.clock: 0.1450, Acc.flag: 0.4836
2023-11-10 15:33:01,881 - mmseg - INFO - Iter [24050/160000]	lr: 9.453e-05, eta: 1 day, 2:41:44, time: 2.788, data_time: 2.153, memory: 23129, decode.loss_ce: 0.4526, decode.acc_seg: 82.6104, loss: 0.4526
2023-11-10 15:33:35,372 - mmseg - INFO - Iter [24100/160000]	lr: 9.451e-05, eta: 1 day, 2:40:58, time: 0.669, data_time: 0.008, memory: 23129, decode.loss_ce: 0.4819, decode.acc_seg: 82.5390, loss: 0.4819
2023-11-10 15:34:09,470 - mmseg - INFO - Iter [24150/160000]	lr: 9.448e-05, eta: 1 day, 2:40:16, time: 0.682, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4609, decode.acc_seg: 82.5162, loss: 0.4609
2023-11-10 15:34:41,012 - mmseg - INFO - Iter [24200/160000]	lr: 9.446e-05, eta: 1 day, 2:39:19, time: 0.630, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4714, decode.acc_seg: 82.6475, loss: 0.4714
2023-11-10 15:35:15,092 - mmseg - INFO - Iter [24250/160000]	lr: 9.444e-05, eta: 1 day, 2:38:37, time: 0.682, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4113, decode.acc_seg: 84.4126, loss: 0.4113
2023-11-10 15:35:49,602 - mmseg - INFO - Iter [24300/160000]	lr: 9.442e-05, eta: 1 day, 2:37:57, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4602, decode.acc_seg: 82.4551, loss: 0.4602
2023-11-10 15:36:24,581 - mmseg - INFO - Iter [24350/160000]	lr: 9.439e-05, eta: 1 day, 2:37:19, time: 0.698, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4569, decode.acc_seg: 82.6501, loss: 0.4569
2023-11-10 15:36:59,923 - mmseg - INFO - Iter [24400/160000]	lr: 9.437e-05, eta: 1 day, 2:36:44, time: 0.707, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4473, decode.acc_seg: 82.7801, loss: 0.4473
2023-11-10 15:37:35,152 - mmseg - INFO - Iter [24450/160000]	lr: 9.435e-05, eta: 1 day, 2:36:08, time: 0.705, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4520, decode.acc_seg: 82.7965, loss: 0.4520
2023-11-10 15:38:10,644 - mmseg - INFO - Iter [24500/160000]	lr: 9.433e-05, eta: 1 day, 2:35:34, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4378, decode.acc_seg: 83.3774, loss: 0.4378
2023-11-10 15:38:42,473 - mmseg - INFO - Iter [24550/160000]	lr: 9.430e-05, eta: 1 day, 2:34:40, time: 0.638, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4089, decode.acc_seg: 83.9969, loss: 0.4089
2023-11-10 15:39:14,560 - mmseg - INFO - Iter [24600/160000]	lr: 9.428e-05, eta: 1 day, 2:33:46, time: 0.641, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4583, decode.acc_seg: 82.8811, loss: 0.4583
2023-11-10 15:39:49,776 - mmseg - INFO - Iter [24650/160000]	lr: 9.426e-05, eta: 1 day, 2:33:10, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4511, decode.acc_seg: 82.6901, loss: 0.4511
2023-11-10 15:40:25,203 - mmseg - INFO - Iter [24700/160000]	lr: 9.423e-05, eta: 1 day, 2:32:36, time: 0.709, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4439, decode.acc_seg: 83.3021, loss: 0.4439
2023-11-10 15:41:00,514 - mmseg - INFO - Iter [24750/160000]	lr: 9.421e-05, eta: 1 day, 2:32:00, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4490, decode.acc_seg: 83.2507, loss: 0.4490
2023-11-10 15:41:35,709 - mmseg - INFO - Iter [24800/160000]	lr: 9.419e-05, eta: 1 day, 2:31:24, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4513, decode.acc_seg: 82.7451, loss: 0.4513
2023-11-10 15:42:10,360 - mmseg - INFO - Iter [24850/160000]	lr: 9.417e-05, eta: 1 day, 2:30:46, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4535, decode.acc_seg: 82.5765, loss: 0.4535
2023-11-10 15:42:45,500 - mmseg - INFO - Iter [24900/160000]	lr: 9.414e-05, eta: 1 day, 2:30:09, time: 0.702, data_time: 0.008, memory: 23129, decode.loss_ce: 0.4434, decode.acc_seg: 82.6167, loss: 0.4434
2023-11-10 15:43:20,237 - mmseg - INFO - Iter [24950/160000]	lr: 9.412e-05, eta: 1 day, 2:29:31, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4440, decode.acc_seg: 83.6794, loss: 0.4440
2023-11-10 15:43:51,675 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 15:43:51,675 - mmseg - INFO - Iter [25000/160000]	lr: 9.410e-05, eta: 1 day, 2:28:35, time: 0.629, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4581, decode.acc_seg: 82.9841, loss: 0.4581
2023-11-10 15:44:24,258 - mmseg - INFO - Iter [25050/160000]	lr: 9.407e-05, eta: 1 day, 2:27:45, time: 0.651, data_time: 0.008, memory: 23129, decode.loss_ce: 0.4544, decode.acc_seg: 82.9820, loss: 0.4544
2023-11-10 15:44:59,357 - mmseg - INFO - Iter [25100/160000]	lr: 9.405e-05, eta: 1 day, 2:27:08, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4578, decode.acc_seg: 83.0650, loss: 0.4578
2023-11-10 15:45:34,662 - mmseg - INFO - Iter [25150/160000]	lr: 9.403e-05, eta: 1 day, 2:26:33, time: 0.706, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4576, decode.acc_seg: 82.7173, loss: 0.4576
2023-11-10 15:46:10,148 - mmseg - INFO - Iter [25200/160000]	lr: 9.400e-05, eta: 1 day, 2:25:59, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4538, decode.acc_seg: 82.8439, loss: 0.4538
2023-11-10 15:46:43,763 - mmseg - INFO - Iter [25250/160000]	lr: 9.398e-05, eta: 1 day, 2:25:15, time: 0.673, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4272, decode.acc_seg: 82.9176, loss: 0.4272
2023-11-10 15:47:18,706 - mmseg - INFO - Iter [25300/160000]	lr: 9.396e-05, eta: 1 day, 2:24:37, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4252, decode.acc_seg: 83.9175, loss: 0.4252
2023-11-10 15:47:52,651 - mmseg - INFO - Iter [25350/160000]	lr: 9.393e-05, eta: 1 day, 2:23:55, time: 0.680, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4485, decode.acc_seg: 83.3959, loss: 0.4485
2023-11-10 15:48:25,880 - mmseg - INFO - Iter [25400/160000]	lr: 9.391e-05, eta: 1 day, 2:23:09, time: 0.664, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4610, decode.acc_seg: 81.8964, loss: 0.4610
2023-11-10 15:49:00,535 - mmseg - INFO - Iter [25450/160000]	lr: 9.389e-05, eta: 1 day, 2:22:30, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4503, decode.acc_seg: 82.6551, loss: 0.4503
2023-11-10 15:49:35,382 - mmseg - INFO - Iter [25500/160000]	lr: 9.386e-05, eta: 1 day, 2:21:53, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4448, decode.acc_seg: 83.2446, loss: 0.4448
2023-11-10 15:50:09,245 - mmseg - INFO - Iter [25550/160000]	lr: 9.384e-05, eta: 1 day, 2:21:10, time: 0.677, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4513, decode.acc_seg: 83.0746, loss: 0.4513
2023-11-10 15:50:41,986 - mmseg - INFO - Iter [25600/160000]	lr: 9.382e-05, eta: 1 day, 2:20:21, time: 0.656, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4243, decode.acc_seg: 83.5698, loss: 0.4243
2023-11-10 15:51:13,411 - mmseg - INFO - Iter [25650/160000]	lr: 9.379e-05, eta: 1 day, 2:19:26, time: 0.629, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4282, decode.acc_seg: 83.4967, loss: 0.4282
2023-11-10 15:51:46,930 - mmseg - INFO - Iter [25700/160000]	lr: 9.377e-05, eta: 1 day, 2:18:41, time: 0.670, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4307, decode.acc_seg: 83.4956, loss: 0.4307
2023-11-10 15:52:22,021 - mmseg - INFO - Iter [25750/160000]	lr: 9.374e-05, eta: 1 day, 2:18:05, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4508, decode.acc_seg: 83.5226, loss: 0.4508
2023-11-10 15:52:56,381 - mmseg - INFO - Iter [25800/160000]	lr: 9.372e-05, eta: 1 day, 2:17:25, time: 0.687, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4602, decode.acc_seg: 82.7468, loss: 0.4602
2023-11-10 15:53:31,570 - mmseg - INFO - Iter [25850/160000]	lr: 9.370e-05, eta: 1 day, 2:16:49, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4890, decode.acc_seg: 81.7328, loss: 0.4890
2023-11-10 15:54:05,975 - mmseg - INFO - Iter [25900/160000]	lr: 9.367e-05, eta: 1 day, 2:16:10, time: 0.688, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4215, decode.acc_seg: 83.9990, loss: 0.4215
2023-11-10 15:54:37,903 - mmseg - INFO - Iter [25950/160000]	lr: 9.365e-05, eta: 1 day, 2:15:18, time: 0.640, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4153, decode.acc_seg: 83.9168, loss: 0.4153
2023-11-10 15:55:09,796 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 15:55:09,797 - mmseg - INFO - Iter [26000/160000]	lr: 9.363e-05, eta: 1 day, 2:14:25, time: 0.638, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4407, decode.acc_seg: 83.1780, loss: 0.4407
2023-11-10 15:55:43,417 - mmseg - INFO - Iter [26050/160000]	lr: 9.360e-05, eta: 1 day, 2:13:41, time: 0.671, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4445, decode.acc_seg: 82.6357, loss: 0.4445
2023-11-10 15:56:16,163 - mmseg - INFO - Iter [26100/160000]	lr: 9.358e-05, eta: 1 day, 2:12:53, time: 0.656, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4574, decode.acc_seg: 82.5735, loss: 0.4574
2023-11-10 15:56:49,942 - mmseg - INFO - Iter [26150/160000]	lr: 9.355e-05, eta: 1 day, 2:12:11, time: 0.675, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4152, decode.acc_seg: 84.2250, loss: 0.4152
2023-11-10 15:57:24,491 - mmseg - INFO - Iter [26200/160000]	lr: 9.353e-05, eta: 1 day, 2:11:32, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4403, decode.acc_seg: 83.4380, loss: 0.4403
2023-11-10 15:57:59,850 - mmseg - INFO - Iter [26250/160000]	lr: 9.350e-05, eta: 1 day, 2:10:57, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4427, decode.acc_seg: 83.4366, loss: 0.4427
2023-11-10 15:58:33,791 - mmseg - INFO - Iter [26300/160000]	lr: 9.348e-05, eta: 1 day, 2:10:15, time: 0.679, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4401, decode.acc_seg: 83.3131, loss: 0.4401
2023-11-10 15:59:08,261 - mmseg - INFO - Iter [26350/160000]	lr: 9.346e-05, eta: 1 day, 2:09:36, time: 0.690, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4484, decode.acc_seg: 83.0470, loss: 0.4484
2023-11-10 15:59:44,792 - mmseg - INFO - Iter [26400/160000]	lr: 9.343e-05, eta: 1 day, 2:09:08, time: 0.731, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4344, decode.acc_seg: 83.2769, loss: 0.4344
2023-11-10 16:00:18,890 - mmseg - INFO - Iter [26450/160000]	lr: 9.341e-05, eta: 1 day, 2:08:26, time: 0.681, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4322, decode.acc_seg: 83.5998, loss: 0.4322
2023-11-10 16:00:54,083 - mmseg - INFO - Iter [26500/160000]	lr: 9.338e-05, eta: 1 day, 2:07:51, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4473, decode.acc_seg: 82.7453, loss: 0.4473
2023-11-10 16:01:25,533 - mmseg - INFO - Iter [26550/160000]	lr: 9.336e-05, eta: 1 day, 2:06:57, time: 0.629, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4487, decode.acc_seg: 83.2090, loss: 0.4487
2023-11-10 16:02:00,758 - mmseg - INFO - Iter [26600/160000]	lr: 9.333e-05, eta: 1 day, 2:06:21, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4880, decode.acc_seg: 82.2928, loss: 0.4880
2023-11-10 16:02:35,383 - mmseg - INFO - Iter [26650/160000]	lr: 9.331e-05, eta: 1 day, 2:05:43, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4510, decode.acc_seg: 83.3838, loss: 0.4510
2023-11-10 16:03:09,307 - mmseg - INFO - Iter [26700/160000]	lr: 9.329e-05, eta: 1 day, 2:05:01, time: 0.677, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4774, decode.acc_seg: 82.0589, loss: 0.4774
2023-11-10 16:03:44,418 - mmseg - INFO - Iter [26750/160000]	lr: 9.326e-05, eta: 1 day, 2:04:26, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4647, decode.acc_seg: 82.8125, loss: 0.4647
2023-11-10 16:04:16,782 - mmseg - INFO - Iter [26800/160000]	lr: 9.324e-05, eta: 1 day, 2:03:36, time: 0.647, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4515, decode.acc_seg: 83.0916, loss: 0.4515
2023-11-10 16:04:52,223 - mmseg - INFO - Iter [26850/160000]	lr: 9.321e-05, eta: 1 day, 2:03:02, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4108, decode.acc_seg: 84.6464, loss: 0.4108
2023-11-10 16:05:27,086 - mmseg - INFO - Iter [26900/160000]	lr: 9.319e-05, eta: 1 day, 2:02:25, time: 0.697, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4344, decode.acc_seg: 83.2182, loss: 0.4344
2023-11-10 16:06:01,397 - mmseg - INFO - Iter [26950/160000]	lr: 9.316e-05, eta: 1 day, 2:01:46, time: 0.687, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4583, decode.acc_seg: 82.3398, loss: 0.4583
2023-11-10 16:06:36,819 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 16:06:36,819 - mmseg - INFO - Iter [27000/160000]	lr: 9.314e-05, eta: 1 day, 2:01:11, time: 0.708, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4481, decode.acc_seg: 82.7741, loss: 0.4481
2023-11-10 16:07:11,238 - mmseg - INFO - Iter [27050/160000]	lr: 9.311e-05, eta: 1 day, 2:00:32, time: 0.689, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4349, decode.acc_seg: 83.6400, loss: 0.4349
2023-11-10 16:07:45,124 - mmseg - INFO - Iter [27100/160000]	lr: 9.309e-05, eta: 1 day, 1:59:51, time: 0.677, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4256, decode.acc_seg: 83.7072, loss: 0.4256
2023-11-10 16:08:20,508 - mmseg - INFO - Iter [27150/160000]	lr: 9.306e-05, eta: 1 day, 1:59:16, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4485, decode.acc_seg: 82.9685, loss: 0.4485
2023-11-10 16:08:55,161 - mmseg - INFO - Iter [27200/160000]	lr: 9.304e-05, eta: 1 day, 1:58:38, time: 0.693, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4296, decode.acc_seg: 82.6857, loss: 0.4296
2023-11-10 16:09:29,380 - mmseg - INFO - Iter [27250/160000]	lr: 9.301e-05, eta: 1 day, 1:57:58, time: 0.685, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4473, decode.acc_seg: 82.7234, loss: 0.4473
2023-11-10 16:10:04,493 - mmseg - INFO - Iter [27300/160000]	lr: 9.299e-05, eta: 1 day, 1:57:22, time: 0.701, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4449, decode.acc_seg: 83.0460, loss: 0.4449
2023-11-10 16:10:39,988 - mmseg - INFO - Iter [27350/160000]	lr: 9.296e-05, eta: 1 day, 1:56:49, time: 0.710, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4540, decode.acc_seg: 82.8410, loss: 0.4540
2023-11-10 16:11:15,020 - mmseg - INFO - Iter [27400/160000]	lr: 9.294e-05, eta: 1 day, 1:56:13, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4212, decode.acc_seg: 83.2170, loss: 0.4212
2023-11-10 16:11:50,019 - mmseg - INFO - Iter [27450/160000]	lr: 9.291e-05, eta: 1 day, 1:55:36, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4423, decode.acc_seg: 83.6074, loss: 0.4423
2023-11-10 16:12:25,482 - mmseg - INFO - Iter [27500/160000]	lr: 9.289e-05, eta: 1 day, 1:55:02, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4577, decode.acc_seg: 82.2377, loss: 0.4577
2023-11-10 16:13:01,040 - mmseg - INFO - Iter [27550/160000]	lr: 9.286e-05, eta: 1 day, 1:54:29, time: 0.711, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4173, decode.acc_seg: 83.8477, loss: 0.4173
2023-11-10 16:13:36,212 - mmseg - INFO - Iter [27600/160000]	lr: 9.284e-05, eta: 1 day, 1:53:53, time: 0.704, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3867, decode.acc_seg: 84.8722, loss: 0.3867
2023-11-10 16:14:11,565 - mmseg - INFO - Iter [27650/160000]	lr: 9.281e-05, eta: 1 day, 1:53:19, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4306, decode.acc_seg: 83.8169, loss: 0.4306
2023-11-10 16:14:47,135 - mmseg - INFO - Iter [27700/160000]	lr: 9.279e-05, eta: 1 day, 1:52:45, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4521, decode.acc_seg: 82.8541, loss: 0.4521
2023-11-10 16:15:22,606 - mmseg - INFO - Iter [27750/160000]	lr: 9.276e-05, eta: 1 day, 1:52:12, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4763, decode.acc_seg: 81.8023, loss: 0.4763
2023-11-10 16:15:57,834 - mmseg - INFO - Iter [27800/160000]	lr: 9.273e-05, eta: 1 day, 1:51:36, time: 0.705, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4265, decode.acc_seg: 83.8945, loss: 0.4265
2023-11-10 16:16:32,998 - mmseg - INFO - Iter [27850/160000]	lr: 9.271e-05, eta: 1 day, 1:51:01, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4141, decode.acc_seg: 84.0668, loss: 0.4141
2023-11-10 16:17:08,882 - mmseg - INFO - Iter [27900/160000]	lr: 9.268e-05, eta: 1 day, 1:50:29, time: 0.718, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4598, decode.acc_seg: 82.5739, loss: 0.4598
2023-11-10 16:17:44,129 - mmseg - INFO - Iter [27950/160000]	lr: 9.266e-05, eta: 1 day, 1:49:54, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4470, decode.acc_seg: 83.4843, loss: 0.4470
2023-11-10 16:18:18,644 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 16:18:18,644 - mmseg - INFO - Iter [28000/160000]	lr: 9.263e-05, eta: 1 day, 1:49:15, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4355, decode.acc_seg: 83.7227, loss: 0.4355
2023-11-10 16:18:53,946 - mmseg - INFO - Iter [28050/160000]	lr: 9.261e-05, eta: 1 day, 1:48:41, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4427, decode.acc_seg: 83.6929, loss: 0.4427
2023-11-10 16:19:27,933 - mmseg - INFO - Iter [28100/160000]	lr: 9.258e-05, eta: 1 day, 1:48:00, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4408, decode.acc_seg: 83.1116, loss: 0.4408
2023-11-10 16:20:03,497 - mmseg - INFO - Iter [28150/160000]	lr: 9.256e-05, eta: 1 day, 1:47:26, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4412, decode.acc_seg: 83.5559, loss: 0.4412
2023-11-10 16:20:38,267 - mmseg - INFO - Iter [28200/160000]	lr: 9.253e-05, eta: 1 day, 1:46:49, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4925, decode.acc_seg: 81.7999, loss: 0.4925
2023-11-10 16:21:10,695 - mmseg - INFO - Iter [28250/160000]	lr: 9.250e-05, eta: 1 day, 1:46:01, time: 0.650, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4393, decode.acc_seg: 83.4358, loss: 0.4393
2023-11-10 16:21:43,753 - mmseg - INFO - Iter [28300/160000]	lr: 9.248e-05, eta: 1 day, 1:45:15, time: 0.660, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4331, decode.acc_seg: 83.5851, loss: 0.4331
2023-11-10 16:22:19,511 - mmseg - INFO - Iter [28350/160000]	lr: 9.245e-05, eta: 1 day, 1:44:43, time: 0.715, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4057, decode.acc_seg: 84.3203, loss: 0.4057
2023-11-10 16:22:52,714 - mmseg - INFO - Iter [28400/160000]	lr: 9.243e-05, eta: 1 day, 1:43:59, time: 0.665, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4268, decode.acc_seg: 84.2586, loss: 0.4268
2023-11-10 16:23:25,312 - mmseg - INFO - Iter [28450/160000]	lr: 9.240e-05, eta: 1 day, 1:43:11, time: 0.651, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3957, decode.acc_seg: 84.7097, loss: 0.3957
2023-11-10 16:24:00,586 - mmseg - INFO - Iter [28500/160000]	lr: 9.237e-05, eta: 1 day, 1:42:36, time: 0.707, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4122, decode.acc_seg: 84.7595, loss: 0.4122
2023-11-10 16:24:31,858 - mmseg - INFO - Iter [28550/160000]	lr: 9.235e-05, eta: 1 day, 1:41:43, time: 0.625, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4469, decode.acc_seg: 83.0917, loss: 0.4469
2023-11-10 16:25:06,427 - mmseg - INFO - Iter [28600/160000]	lr: 9.232e-05, eta: 1 day, 1:41:05, time: 0.692, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4173, decode.acc_seg: 84.1338, loss: 0.4173
2023-11-10 16:25:37,582 - mmseg - INFO - Iter [28650/160000]	lr: 9.230e-05, eta: 1 day, 1:40:12, time: 0.623, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4214, decode.acc_seg: 83.7419, loss: 0.4214
2023-11-10 16:26:11,876 - mmseg - INFO - Iter [28700/160000]	lr: 9.227e-05, eta: 1 day, 1:39:32, time: 0.686, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4048, decode.acc_seg: 84.9873, loss: 0.4048
2023-11-10 16:26:44,625 - mmseg - INFO - Iter [28750/160000]	lr: 9.224e-05, eta: 1 day, 1:38:46, time: 0.654, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4284, decode.acc_seg: 83.6868, loss: 0.4284
2023-11-10 16:27:18,671 - mmseg - INFO - Iter [28800/160000]	lr: 9.222e-05, eta: 1 day, 1:38:06, time: 0.681, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4308, decode.acc_seg: 84.0269, loss: 0.4308
2023-11-10 16:27:53,565 - mmseg - INFO - Iter [28850/160000]	lr: 9.219e-05, eta: 1 day, 1:37:29, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4112, decode.acc_seg: 84.1332, loss: 0.4112
2023-11-10 16:28:28,906 - mmseg - INFO - Iter [28900/160000]	lr: 9.216e-05, eta: 1 day, 1:36:55, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4200, decode.acc_seg: 84.1077, loss: 0.4200
2023-11-10 16:29:02,885 - mmseg - INFO - Iter [28950/160000]	lr: 9.214e-05, eta: 1 day, 1:36:14, time: 0.680, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4250, decode.acc_seg: 83.4232, loss: 0.4250
2023-11-10 16:29:38,054 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 16:29:38,055 - mmseg - INFO - Iter [29000/160000]	lr: 9.211e-05, eta: 1 day, 1:35:39, time: 0.703, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4453, decode.acc_seg: 82.3894, loss: 0.4453
2023-11-10 16:30:11,190 - mmseg - INFO - Iter [29050/160000]	lr: 9.208e-05, eta: 1 day, 1:34:55, time: 0.663, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4358, decode.acc_seg: 83.6076, loss: 0.4358
2023-11-10 16:30:46,105 - mmseg - INFO - Iter [29100/160000]	lr: 9.206e-05, eta: 1 day, 1:34:18, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4179, decode.acc_seg: 84.0329, loss: 0.4179
2023-11-10 16:31:21,183 - mmseg - INFO - Iter [29150/160000]	lr: 9.203e-05, eta: 1 day, 1:33:43, time: 0.702, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4360, decode.acc_seg: 83.6911, loss: 0.4360
2023-11-10 16:31:54,972 - mmseg - INFO - Iter [29200/160000]	lr: 9.201e-05, eta: 1 day, 1:33:02, time: 0.677, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4099, decode.acc_seg: 84.1245, loss: 0.4099
2023-11-10 16:32:26,769 - mmseg - INFO - Iter [29250/160000]	lr: 9.198e-05, eta: 1 day, 1:32:12, time: 0.636, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4094, decode.acc_seg: 85.2276, loss: 0.4094
2023-11-10 16:33:00,898 - mmseg - INFO - Iter [29300/160000]	lr: 9.195e-05, eta: 1 day, 1:31:32, time: 0.681, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4215, decode.acc_seg: 83.8969, loss: 0.4215
2023-11-10 16:33:34,684 - mmseg - INFO - Iter [29350/160000]	lr: 9.193e-05, eta: 1 day, 1:30:51, time: 0.677, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4260, decode.acc_seg: 83.9643, loss: 0.4260
2023-11-10 16:34:06,886 - mmseg - INFO - Iter [29400/160000]	lr: 9.190e-05, eta: 1 day, 1:30:02, time: 0.643, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4122, decode.acc_seg: 84.3039, loss: 0.4122
2023-11-10 16:34:41,890 - mmseg - INFO - Iter [29450/160000]	lr: 9.187e-05, eta: 1 day, 1:29:26, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3976, decode.acc_seg: 84.6911, loss: 0.3976
2023-11-10 16:35:17,215 - mmseg - INFO - Iter [29500/160000]	lr: 9.184e-05, eta: 1 day, 1:28:52, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4197, decode.acc_seg: 84.0239, loss: 0.4197
2023-11-10 16:35:52,618 - mmseg - INFO - Iter [29550/160000]	lr: 9.182e-05, eta: 1 day, 1:28:18, time: 0.708, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4187, decode.acc_seg: 83.7761, loss: 0.4187
2023-11-10 16:36:28,208 - mmseg - INFO - Iter [29600/160000]	lr: 9.179e-05, eta: 1 day, 1:27:45, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4320, decode.acc_seg: 83.4501, loss: 0.4320
2023-11-10 16:37:03,707 - mmseg - INFO - Iter [29650/160000]	lr: 9.176e-05, eta: 1 day, 1:27:11, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4227, decode.acc_seg: 83.9298, loss: 0.4227
2023-11-10 16:37:38,632 - mmseg - INFO - Iter [29700/160000]	lr: 9.174e-05, eta: 1 day, 1:26:35, time: 0.699, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4101, decode.acc_seg: 84.1566, loss: 0.4101
2023-11-10 16:38:13,408 - mmseg - INFO - Iter [29750/160000]	lr: 9.171e-05, eta: 1 day, 1:25:58, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4269, decode.acc_seg: 83.9934, loss: 0.4269
2023-11-10 16:38:48,093 - mmseg - INFO - Iter [29800/160000]	lr: 9.168e-05, eta: 1 day, 1:25:21, time: 0.692, data_time: 0.008, memory: 23129, decode.loss_ce: 0.4169, decode.acc_seg: 84.3328, loss: 0.4169
2023-11-10 16:39:19,991 - mmseg - INFO - Iter [29850/160000]	lr: 9.166e-05, eta: 1 day, 1:24:32, time: 0.638, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4242, decode.acc_seg: 83.7170, loss: 0.4242
2023-11-10 16:39:55,041 - mmseg - INFO - Iter [29900/160000]	lr: 9.163e-05, eta: 1 day, 1:23:56, time: 0.701, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4255, decode.acc_seg: 83.7969, loss: 0.4255
2023-11-10 16:40:30,270 - mmseg - INFO - Iter [29950/160000]	lr: 9.160e-05, eta: 1 day, 1:23:21, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4077, decode.acc_seg: 84.4071, loss: 0.4077
2023-11-10 16:41:05,328 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 16:41:05,328 - mmseg - INFO - Iter [30000/160000]	lr: 9.157e-05, eta: 1 day, 1:22:46, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4096, decode.acc_seg: 83.9628, loss: 0.4096
2023-11-10 16:41:40,921 - mmseg - INFO - Iter [30050/160000]	lr: 9.155e-05, eta: 1 day, 1:22:13, time: 0.711, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4136, decode.acc_seg: 84.1504, loss: 0.4136
2023-11-10 16:42:16,178 - mmseg - INFO - Iter [30100/160000]	lr: 9.152e-05, eta: 1 day, 1:21:38, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4033, decode.acc_seg: 84.6473, loss: 0.4033
2023-11-10 16:42:50,966 - mmseg - INFO - Iter [30150/160000]	lr: 9.149e-05, eta: 1 day, 1:21:01, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4027, decode.acc_seg: 84.3520, loss: 0.4027
2023-11-10 16:43:24,675 - mmseg - INFO - Iter [30200/160000]	lr: 9.146e-05, eta: 1 day, 1:20:20, time: 0.674, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4263, decode.acc_seg: 83.8225, loss: 0.4263
2023-11-10 16:44:00,158 - mmseg - INFO - Iter [30250/160000]	lr: 9.144e-05, eta: 1 day, 1:19:47, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4529, decode.acc_seg: 83.1739, loss: 0.4529
2023-11-10 16:44:35,717 - mmseg - INFO - Iter [30300/160000]	lr: 9.141e-05, eta: 1 day, 1:19:13, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4118, decode.acc_seg: 84.2187, loss: 0.4118
2023-11-10 16:45:11,366 - mmseg - INFO - Iter [30350/160000]	lr: 9.138e-05, eta: 1 day, 1:18:40, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4267, decode.acc_seg: 83.7419, loss: 0.4267
2023-11-10 16:45:45,042 - mmseg - INFO - Iter [30400/160000]	lr: 9.135e-05, eta: 1 day, 1:17:59, time: 0.674, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4336, decode.acc_seg: 83.9658, loss: 0.4336
2023-11-10 16:46:17,553 - mmseg - INFO - Iter [30450/160000]	lr: 9.133e-05, eta: 1 day, 1:17:13, time: 0.649, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4419, decode.acc_seg: 83.1139, loss: 0.4419
2023-11-10 16:46:51,268 - mmseg - INFO - Iter [30500/160000]	lr: 9.130e-05, eta: 1 day, 1:16:32, time: 0.675, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4499, decode.acc_seg: 82.7066, loss: 0.4499
2023-11-10 16:47:25,733 - mmseg - INFO - Iter [30550/160000]	lr: 9.127e-05, eta: 1 day, 1:15:53, time: 0.688, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4204, decode.acc_seg: 84.2347, loss: 0.4204
2023-11-10 16:48:01,185 - mmseg - INFO - Iter [30600/160000]	lr: 9.124e-05, eta: 1 day, 1:15:20, time: 0.709, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4333, decode.acc_seg: 84.1331, loss: 0.4333
2023-11-10 16:48:35,840 - mmseg - INFO - Iter [30650/160000]	lr: 9.122e-05, eta: 1 day, 1:14:42, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4194, decode.acc_seg: 84.1548, loss: 0.4194
2023-11-10 16:49:09,244 - mmseg - INFO - Iter [30700/160000]	lr: 9.119e-05, eta: 1 day, 1:14:00, time: 0.669, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3924, decode.acc_seg: 84.8614, loss: 0.3924
2023-11-10 16:49:42,816 - mmseg - INFO - Iter [30750/160000]	lr: 9.116e-05, eta: 1 day, 1:13:18, time: 0.670, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4138, decode.acc_seg: 83.8941, loss: 0.4138
2023-11-10 16:50:14,723 - mmseg - INFO - Iter [30800/160000]	lr: 9.113e-05, eta: 1 day, 1:12:30, time: 0.639, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4273, decode.acc_seg: 83.7482, loss: 0.4273
2023-11-10 16:50:50,425 - mmseg - INFO - Iter [30850/160000]	lr: 9.110e-05, eta: 1 day, 1:11:57, time: 0.713, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3928, decode.acc_seg: 85.2578, loss: 0.3928
2023-11-10 16:51:25,822 - mmseg - INFO - Iter [30900/160000]	lr: 9.108e-05, eta: 1 day, 1:11:23, time: 0.709, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4065, decode.acc_seg: 84.5105, loss: 0.4065
2023-11-10 16:51:58,240 - mmseg - INFO - Iter [30950/160000]	lr: 9.105e-05, eta: 1 day, 1:10:37, time: 0.649, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4193, decode.acc_seg: 83.9647, loss: 0.4193
2023-11-10 16:52:33,762 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 16:52:33,763 - mmseg - INFO - Iter [31000/160000]	lr: 9.102e-05, eta: 1 day, 1:10:03, time: 0.709, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4284, decode.acc_seg: 83.6885, loss: 0.4284
2023-11-10 16:53:09,024 - mmseg - INFO - Iter [31050/160000]	lr: 9.099e-05, eta: 1 day, 1:09:29, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4191, decode.acc_seg: 83.8942, loss: 0.4191
2023-11-10 16:53:44,117 - mmseg - INFO - Iter [31100/160000]	lr: 9.096e-05, eta: 1 day, 1:08:54, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3982, decode.acc_seg: 84.5439, loss: 0.3982
2023-11-10 16:54:15,596 - mmseg - INFO - Iter [31150/160000]	lr: 9.094e-05, eta: 1 day, 1:08:04, time: 0.630, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4106, decode.acc_seg: 84.3325, loss: 0.4106
2023-11-10 16:54:47,394 - mmseg - INFO - Iter [31200/160000]	lr: 9.091e-05, eta: 1 day, 1:07:15, time: 0.636, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4070, decode.acc_seg: 84.8680, loss: 0.4070
2023-11-10 16:55:19,805 - mmseg - INFO - Iter [31250/160000]	lr: 9.088e-05, eta: 1 day, 1:06:28, time: 0.647, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4221, decode.acc_seg: 83.9061, loss: 0.4221
2023-11-10 16:55:54,245 - mmseg - INFO - Iter [31300/160000]	lr: 9.085e-05, eta: 1 day, 1:05:51, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4127, decode.acc_seg: 84.4683, loss: 0.4127
2023-11-10 16:56:27,041 - mmseg - INFO - Iter [31350/160000]	lr: 9.082e-05, eta: 1 day, 1:05:06, time: 0.656, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4071, decode.acc_seg: 84.1763, loss: 0.4071
2023-11-10 16:56:59,496 - mmseg - INFO - Iter [31400/160000]	lr: 9.079e-05, eta: 1 day, 1:04:20, time: 0.648, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3765, decode.acc_seg: 85.4597, loss: 0.3765
2023-11-10 16:57:33,091 - mmseg - INFO - Iter [31450/160000]	lr: 9.077e-05, eta: 1 day, 1:03:39, time: 0.672, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4138, decode.acc_seg: 84.2574, loss: 0.4138
2023-11-10 16:58:06,580 - mmseg - INFO - Iter [31500/160000]	lr: 9.074e-05, eta: 1 day, 1:02:57, time: 0.670, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3809, decode.acc_seg: 85.3611, loss: 0.3809
2023-11-10 16:58:40,464 - mmseg - INFO - Iter [31550/160000]	lr: 9.071e-05, eta: 1 day, 1:02:17, time: 0.677, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3865, decode.acc_seg: 85.2114, loss: 0.3865
2023-11-10 16:59:16,157 - mmseg - INFO - Iter [31600/160000]	lr: 9.068e-05, eta: 1 day, 1:01:45, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3843, decode.acc_seg: 84.5774, loss: 0.3843
2023-11-10 16:59:50,396 - mmseg - INFO - Iter [31650/160000]	lr: 9.065e-05, eta: 1 day, 1:01:06, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3842, decode.acc_seg: 85.1025, loss: 0.3842
2023-11-10 17:00:23,884 - mmseg - INFO - Iter [31700/160000]	lr: 9.062e-05, eta: 1 day, 1:00:24, time: 0.669, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3938, decode.acc_seg: 84.8652, loss: 0.3938
2023-11-10 17:00:58,520 - mmseg - INFO - Iter [31750/160000]	lr: 9.060e-05, eta: 1 day, 0:59:48, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4046, decode.acc_seg: 84.8484, loss: 0.4046
2023-11-10 17:01:32,997 - mmseg - INFO - Iter [31800/160000]	lr: 9.057e-05, eta: 1 day, 0:59:10, time: 0.689, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4169, decode.acc_seg: 84.3755, loss: 0.4169
2023-11-10 17:02:06,803 - mmseg - INFO - Iter [31850/160000]	lr: 9.054e-05, eta: 1 day, 0:58:30, time: 0.677, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4068, decode.acc_seg: 84.8791, loss: 0.4068
2023-11-10 17:02:38,762 - mmseg - INFO - Iter [31900/160000]	lr: 9.051e-05, eta: 1 day, 0:57:42, time: 0.639, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3990, decode.acc_seg: 84.8429, loss: 0.3990
2023-11-10 17:03:13,590 - mmseg - INFO - Iter [31950/160000]	lr: 9.048e-05, eta: 1 day, 0:57:06, time: 0.696, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4016, decode.acc_seg: 85.0327, loss: 0.4016
2023-11-10 17:03:50,673 - mmseg - INFO - Saving checkpoint at 32000 iterations
2023-11-10 17:03:55,240 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 17:03:55,241 - mmseg - INFO - Iter [32000/160000]	lr: 9.045e-05, eta: 1 day, 0:56:58, time: 0.834, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4308, decode.acc_seg: 83.4397, loss: 0.4308
2023-11-10 17:05:26,038 - mmseg - INFO - per class results:
2023-11-10 17:05:26,052 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 74.37 | 85.49 |
|       building      | 81.11 |  90.9 |
|         sky         | 93.84 | 97.53 |
|        floor        | 77.88 | 82.99 |
|         tree        |  73.9 | 88.58 |
|       ceiling       | 81.93 | 90.27 |
|         road        | 80.52 | 86.88 |
|         bed         | 86.44 |  93.6 |
|      windowpane     | 60.01 | 77.78 |
|        grass        | 68.83 | 89.21 |
|       cabinet       | 54.58 | 71.48 |
|       sidewalk      | 59.67 | 78.18 |
|        person       | 76.94 | 91.82 |
|        earth        | 30.68 | 46.86 |
|         door        | 41.82 |  49.4 |
|        table        | 53.26 | 73.17 |
|       mountain      | 55.69 | 73.27 |
|        plant        | 50.37 | 56.94 |
|       curtain       | 70.65 |  83.4 |
|        chair        | 50.25 | 65.36 |
|         car         | 82.33 | 91.19 |
|        water        | 56.99 | 72.85 |
|       painting      | 67.61 | 87.73 |
|         sofa        | 58.83 | 80.97 |
|        shelf        | 40.34 | 59.26 |
|        house        |  45.7 | 67.84 |
|         sea         | 51.78 | 69.02 |
|        mirror       | 59.58 | 66.54 |
|         rug         | 61.89 | 82.56 |
|        field        | 26.21 | 32.42 |
|       armchair      |  23.3 | 27.69 |
|         seat        | 58.74 | 78.44 |
|        fence        | 29.09 | 35.73 |
|         desk        | 48.17 | 71.41 |
|         rock        | 36.96 | 52.27 |
|       wardrobe      | 45.38 | 76.67 |
|         lamp        | 55.38 | 76.85 |
|       bathtub       | 75.96 |  81.0 |
|       railing       | 31.67 | 46.27 |
|       cushion       | 52.46 | 71.55 |
|         base        | 28.23 | 37.69 |
|         box         | 20.08 | 24.74 |
|        column       | 45.71 | 58.88 |
|      signboard      | 34.67 | 50.83 |
|   chest of drawers  | 37.01 | 52.23 |
|       counter       | 26.23 | 32.42 |
|         sand        |  34.0 | 77.48 |
|         sink        | 64.85 | 71.59 |
|      skyscraper     | 38.36 | 46.29 |
|      fireplace      | 59.56 | 93.38 |
|     refrigerator    | 60.91 | 85.94 |
|      grandstand     | 39.66 | 66.85 |
|         path        | 18.48 | 35.46 |
|        stairs       | 30.17 | 36.08 |
|        runway       |  66.6 | 86.33 |
|         case        | 41.08 | 46.63 |
|      pool table     | 91.12 | 95.76 |
|        pillow       |  55.7 | 68.43 |
|     screen door     | 64.03 | 79.91 |
|       stairway      | 26.96 | 44.88 |
|        river        | 19.18 |  37.1 |
|        bridge       | 43.26 | 61.73 |
|       bookcase      | 39.48 | 57.07 |
|        blind        | 31.14 | 34.93 |
|     coffee table    | 42.96 | 84.03 |
|        toilet       | 79.34 | 92.17 |
|        flower       | 31.71 |  48.9 |
|         book        | 41.44 | 69.36 |
|         hill        |  5.59 | 10.16 |
|        bench        | 39.79 | 52.64 |
|      countertop     | 54.56 | 64.59 |
|        stove        | 60.22 |  76.9 |
|         palm        | 42.98 | 69.58 |
|    kitchen island   | 32.76 |  87.8 |
|       computer      | 56.53 | 74.06 |
|     swivel chair    | 42.28 |  67.1 |
|         boat        | 27.98 | 65.61 |
|         bar         | 25.33 | 44.86 |
|    arcade machine   | 71.31 | 78.54 |
|        hovel        | 39.35 |  42.0 |
|         bus         | 82.83 |  94.6 |
|        towel        | 64.34 | 75.12 |
|        light        | 46.27 | 55.63 |
|        truck        | 27.39 | 36.44 |
|        tower        | 24.88 | 40.18 |
|      chandelier     | 57.42 | 84.01 |
|        awning       |  21.5 | 31.47 |
|     streetlight     | 20.58 | 30.14 |
|        booth        | 44.16 | 71.07 |
| television receiver | 67.79 | 76.43 |
|       airplane      | 52.57 | 63.56 |
|      dirt track     |  4.8  | 18.67 |
|       apparel       | 37.27 | 53.71 |
|         pole        | 13.85 | 17.76 |
|         land        |  2.39 |  5.82 |
|      bannister      |  9.35 | 13.55 |
|      escalator      | 20.11 | 20.73 |
|       ottoman       | 36.49 | 51.71 |
|        bottle       | 31.29 | 65.94 |
|        buffet       | 44.74 | 64.11 |
|        poster       | 25.49 | 36.16 |
|        stage        | 19.58 | 24.55 |
|         van         | 38.78 | 52.82 |
|         ship        | 28.03 | 35.31 |
|       fountain      | 20.93 | 21.23 |
|    conveyer belt    | 63.52 | 72.09 |
|        canopy       | 20.34 | 31.64 |
|        washer       |  69.9 | 73.57 |
|      plaything      | 23.47 | 35.51 |
|    swimming pool    | 59.62 | 75.73 |
|        stool        | 23.31 | 44.29 |
|        barrel       | 42.27 |  65.1 |
|        basket       | 36.06 | 45.67 |
|      waterfall      | 56.61 | 88.72 |
|         tent        | 81.21 | 98.19 |
|         bag         | 10.65 | 15.92 |
|       minibike      | 48.86 | 58.98 |
|        cradle       | 63.78 | 82.25 |
|         oven        | 12.92 |  35.8 |
|         ball        |  37.5 |  65.6 |
|         food        | 54.36 | 66.56 |
|         step        |  2.58 |  2.88 |
|         tank        | 53.38 | 66.84 |
|      trade name     | 17.69 | 18.16 |
|      microwave      | 35.77 |  43.5 |
|         pot         | 34.45 |  38.9 |
|        animal       |  62.6 | 68.63 |
|       bicycle       | 50.51 | 77.56 |
|         lake        |  0.0  |  0.0  |
|      dishwasher     | 39.52 | 49.58 |
|        screen       | 68.59 | 85.16 |
|       blanket       |  6.89 |  8.86 |
|      sculpture      | 45.68 | 56.85 |
|         hood        | 49.12 | 60.26 |
|        sconce       | 30.71 | 36.17 |
|         vase        | 30.36 | 46.21 |
|    traffic light    | 24.38 | 45.45 |
|         tray        |  5.59 |  7.05 |
|        ashcan       | 30.79 |  48.2 |
|         fan         | 53.14 |  75.1 |
|         pier        | 25.66 | 53.19 |
|      crt screen     |  5.62 |  7.93 |
|        plate        |  47.2 | 60.13 |
|       monitor       | 37.37 | 65.18 |
|    bulletin board   | 47.35 | 54.74 |
|        shower       |  0.0  |  0.0  |
|       radiator      | 45.08 | 49.14 |
|        glass        | 12.91 | 17.57 |
|        clock        | 29.41 | 37.35 |
|         flag        | 41.01 | 45.12 |
+---------------------+-------+-------+
2023-11-10 17:05:26,052 - mmseg - INFO - Summary:
2023-11-10 17:05:26,052 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 80.59 | 43.48 | 57.56 |
+-------+-------+-------+
2023-11-10 17:05:26,063 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 17:05:26,063 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8059, mIoU: 0.4348, mAcc: 0.5756, IoU.wall: 0.7437, IoU.building: 0.8111, IoU.sky: 0.9384, IoU.floor: 0.7788, IoU.tree: 0.7390, IoU.ceiling: 0.8193, IoU.road: 0.8052, IoU.bed : 0.8644, IoU.windowpane: 0.6001, IoU.grass: 0.6883, IoU.cabinet: 0.5458, IoU.sidewalk: 0.5967, IoU.person: 0.7694, IoU.earth: 0.3068, IoU.door: 0.4182, IoU.table: 0.5326, IoU.mountain: 0.5569, IoU.plant: 0.5037, IoU.curtain: 0.7065, IoU.chair: 0.5025, IoU.car: 0.8233, IoU.water: 0.5699, IoU.painting: 0.6761, IoU.sofa: 0.5883, IoU.shelf: 0.4034, IoU.house: 0.4570, IoU.sea: 0.5178, IoU.mirror: 0.5958, IoU.rug: 0.6189, IoU.field: 0.2621, IoU.armchair: 0.2330, IoU.seat: 0.5874, IoU.fence: 0.2909, IoU.desk: 0.4817, IoU.rock: 0.3696, IoU.wardrobe: 0.4538, IoU.lamp: 0.5538, IoU.bathtub: 0.7596, IoU.railing: 0.3167, IoU.cushion: 0.5246, IoU.base: 0.2823, IoU.box: 0.2008, IoU.column: 0.4571, IoU.signboard: 0.3467, IoU.chest of drawers: 0.3701, IoU.counter: 0.2623, IoU.sand: 0.3400, IoU.sink: 0.6485, IoU.skyscraper: 0.3836, IoU.fireplace: 0.5956, IoU.refrigerator: 0.6091, IoU.grandstand: 0.3966, IoU.path: 0.1848, IoU.stairs: 0.3017, IoU.runway: 0.6660, IoU.case: 0.4108, IoU.pool table: 0.9112, IoU.pillow: 0.5570, IoU.screen door: 0.6403, IoU.stairway: 0.2696, IoU.river: 0.1918, IoU.bridge: 0.4326, IoU.bookcase: 0.3948, IoU.blind: 0.3114, IoU.coffee table: 0.4296, IoU.toilet: 0.7934, IoU.flower: 0.3171, IoU.book: 0.4144, IoU.hill: 0.0559, IoU.bench: 0.3979, IoU.countertop: 0.5456, IoU.stove: 0.6022, IoU.palm: 0.4298, IoU.kitchen island: 0.3276, IoU.computer: 0.5653, IoU.swivel chair: 0.4228, IoU.boat: 0.2798, IoU.bar: 0.2533, IoU.arcade machine: 0.7131, IoU.hovel: 0.3935, IoU.bus: 0.8283, IoU.towel: 0.6434, IoU.light: 0.4627, IoU.truck: 0.2739, IoU.tower: 0.2488, IoU.chandelier: 0.5742, IoU.awning: 0.2150, IoU.streetlight: 0.2058, IoU.booth: 0.4416, IoU.television receiver: 0.6779, IoU.airplane: 0.5257, IoU.dirt track: 0.0480, IoU.apparel: 0.3727, IoU.pole: 0.1385, IoU.land: 0.0239, IoU.bannister: 0.0935, IoU.escalator: 0.2011, IoU.ottoman: 0.3649, IoU.bottle: 0.3129, IoU.buffet: 0.4474, IoU.poster: 0.2549, IoU.stage: 0.1958, IoU.van: 0.3878, IoU.ship: 0.2803, IoU.fountain: 0.2093, IoU.conveyer belt: 0.6352, IoU.canopy: 0.2034, IoU.washer: 0.6990, IoU.plaything: 0.2347, IoU.swimming pool: 0.5962, IoU.stool: 0.2331, IoU.barrel: 0.4227, IoU.basket: 0.3606, IoU.waterfall: 0.5661, IoU.tent: 0.8121, IoU.bag: 0.1065, IoU.minibike: 0.4886, IoU.cradle: 0.6378, IoU.oven: 0.1292, IoU.ball: 0.3750, IoU.food: 0.5436, IoU.step: 0.0258, IoU.tank: 0.5338, IoU.trade name: 0.1769, IoU.microwave: 0.3577, IoU.pot: 0.3445, IoU.animal: 0.6260, IoU.bicycle: 0.5051, IoU.lake: 0.0000, IoU.dishwasher: 0.3952, IoU.screen: 0.6859, IoU.blanket: 0.0689, IoU.sculpture: 0.4568, IoU.hood: 0.4912, IoU.sconce: 0.3071, IoU.vase: 0.3036, IoU.traffic light: 0.2438, IoU.tray: 0.0559, IoU.ashcan: 0.3079, IoU.fan: 0.5314, IoU.pier: 0.2566, IoU.crt screen: 0.0562, IoU.plate: 0.4720, IoU.monitor: 0.3737, IoU.bulletin board: 0.4735, IoU.shower: 0.0000, IoU.radiator: 0.4508, IoU.glass: 0.1291, IoU.clock: 0.2941, IoU.flag: 0.4101, Acc.wall: 0.8549, Acc.building: 0.9090, Acc.sky: 0.9753, Acc.floor: 0.8299, Acc.tree: 0.8858, Acc.ceiling: 0.9027, Acc.road: 0.8688, Acc.bed : 0.9360, Acc.windowpane: 0.7778, Acc.grass: 0.8921, Acc.cabinet: 0.7148, Acc.sidewalk: 0.7818, Acc.person: 0.9182, Acc.earth: 0.4686, Acc.door: 0.4940, Acc.table: 0.7317, Acc.mountain: 0.7327, Acc.plant: 0.5694, Acc.curtain: 0.8340, Acc.chair: 0.6536, Acc.car: 0.9119, Acc.water: 0.7285, Acc.painting: 0.8773, Acc.sofa: 0.8097, Acc.shelf: 0.5926, Acc.house: 0.6784, Acc.sea: 0.6902, Acc.mirror: 0.6654, Acc.rug: 0.8256, Acc.field: 0.3242, Acc.armchair: 0.2769, Acc.seat: 0.7844, Acc.fence: 0.3573, Acc.desk: 0.7141, Acc.rock: 0.5227, Acc.wardrobe: 0.7667, Acc.lamp: 0.7685, Acc.bathtub: 0.8100, Acc.railing: 0.4627, Acc.cushion: 0.7155, Acc.base: 0.3769, Acc.box: 0.2474, Acc.column: 0.5888, Acc.signboard: 0.5083, Acc.chest of drawers: 0.5223, Acc.counter: 0.3242, Acc.sand: 0.7748, Acc.sink: 0.7159, Acc.skyscraper: 0.4629, Acc.fireplace: 0.9338, Acc.refrigerator: 0.8594, Acc.grandstand: 0.6685, Acc.path: 0.3546, Acc.stairs: 0.3608, Acc.runway: 0.8633, Acc.case: 0.4663, Acc.pool table: 0.9576, Acc.pillow: 0.6843, Acc.screen door: 0.7991, Acc.stairway: 0.4488, Acc.river: 0.3710, Acc.bridge: 0.6173, Acc.bookcase: 0.5707, Acc.blind: 0.3493, Acc.coffee table: 0.8403, Acc.toilet: 0.9217, Acc.flower: 0.4890, Acc.book: 0.6936, Acc.hill: 0.1016, Acc.bench: 0.5264, Acc.countertop: 0.6459, Acc.stove: 0.7690, Acc.palm: 0.6958, Acc.kitchen island: 0.8780, Acc.computer: 0.7406, Acc.swivel chair: 0.6710, Acc.boat: 0.6561, Acc.bar: 0.4486, Acc.arcade machine: 0.7854, Acc.hovel: 0.4200, Acc.bus: 0.9460, Acc.towel: 0.7512, Acc.light: 0.5563, Acc.truck: 0.3644, Acc.tower: 0.4018, Acc.chandelier: 0.8401, Acc.awning: 0.3147, Acc.streetlight: 0.3014, Acc.booth: 0.7107, Acc.television receiver: 0.7643, Acc.airplane: 0.6356, Acc.dirt track: 0.1867, Acc.apparel: 0.5371, Acc.pole: 0.1776, Acc.land: 0.0582, Acc.bannister: 0.1355, Acc.escalator: 0.2073, Acc.ottoman: 0.5171, Acc.bottle: 0.6594, Acc.buffet: 0.6411, Acc.poster: 0.3616, Acc.stage: 0.2455, Acc.van: 0.5282, Acc.ship: 0.3531, Acc.fountain: 0.2123, Acc.conveyer belt: 0.7209, Acc.canopy: 0.3164, Acc.washer: 0.7357, Acc.plaything: 0.3551, Acc.swimming pool: 0.7573, Acc.stool: 0.4429, Acc.barrel: 0.6510, Acc.basket: 0.4567, Acc.waterfall: 0.8872, Acc.tent: 0.9819, Acc.bag: 0.1592, Acc.minibike: 0.5898, Acc.cradle: 0.8225, Acc.oven: 0.3580, Acc.ball: 0.6560, Acc.food: 0.6656, Acc.step: 0.0288, Acc.tank: 0.6684, Acc.trade name: 0.1816, Acc.microwave: 0.4350, Acc.pot: 0.3890, Acc.animal: 0.6863, Acc.bicycle: 0.7756, Acc.lake: 0.0000, Acc.dishwasher: 0.4958, Acc.screen: 0.8516, Acc.blanket: 0.0886, Acc.sculpture: 0.5685, Acc.hood: 0.6026, Acc.sconce: 0.3617, Acc.vase: 0.4621, Acc.traffic light: 0.4545, Acc.tray: 0.0705, Acc.ashcan: 0.4820, Acc.fan: 0.7510, Acc.pier: 0.5319, Acc.crt screen: 0.0793, Acc.plate: 0.6013, Acc.monitor: 0.6518, Acc.bulletin board: 0.5474, Acc.shower: 0.0000, Acc.radiator: 0.4914, Acc.glass: 0.1757, Acc.clock: 0.3735, Acc.flag: 0.4512
2023-11-10 17:06:00,612 - mmseg - INFO - Iter [32050/160000]	lr: 9.042e-05, eta: 1 day, 1:02:23, time: 2.506, data_time: 1.825, memory: 23129, decode.loss_ce: 0.4127, decode.acc_seg: 84.3136, loss: 0.4127
2023-11-10 17:06:34,414 - mmseg - INFO - Iter [32100/160000]	lr: 9.039e-05, eta: 1 day, 1:01:42, time: 0.676, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4307, decode.acc_seg: 83.6001, loss: 0.4307
2023-11-10 17:07:09,898 - mmseg - INFO - Iter [32150/160000]	lr: 9.036e-05, eta: 1 day, 1:01:08, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4139, decode.acc_seg: 84.3879, loss: 0.4139
2023-11-10 17:07:43,426 - mmseg - INFO - Iter [32200/160000]	lr: 9.034e-05, eta: 1 day, 1:00:26, time: 0.671, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3645, decode.acc_seg: 85.7409, loss: 0.3645
2023-11-10 17:08:17,676 - mmseg - INFO - Iter [32250/160000]	lr: 9.031e-05, eta: 1 day, 0:59:47, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4296, decode.acc_seg: 83.9831, loss: 0.4296
2023-11-10 17:08:49,356 - mmseg - INFO - Iter [32300/160000]	lr: 9.028e-05, eta: 1 day, 0:58:57, time: 0.633, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4321, decode.acc_seg: 83.9935, loss: 0.4321
2023-11-10 17:09:24,087 - mmseg - INFO - Iter [32350/160000]	lr: 9.025e-05, eta: 1 day, 0:58:20, time: 0.695, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4188, decode.acc_seg: 84.0494, loss: 0.4188
2023-11-10 17:09:59,214 - mmseg - INFO - Iter [32400/160000]	lr: 9.022e-05, eta: 1 day, 0:57:45, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4131, decode.acc_seg: 84.1025, loss: 0.4131
2023-11-10 17:10:34,357 - mmseg - INFO - Iter [32450/160000]	lr: 9.019e-05, eta: 1 day, 0:57:09, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4083, decode.acc_seg: 84.6809, loss: 0.4083
2023-11-10 17:11:09,331 - mmseg - INFO - Iter [32500/160000]	lr: 9.016e-05, eta: 1 day, 0:56:33, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4031, decode.acc_seg: 84.2196, loss: 0.4031
2023-11-10 17:11:44,734 - mmseg - INFO - Iter [32550/160000]	lr: 9.013e-05, eta: 1 day, 0:55:59, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3898, decode.acc_seg: 84.7740, loss: 0.3898
2023-11-10 17:12:19,404 - mmseg - INFO - Iter [32600/160000]	lr: 9.010e-05, eta: 1 day, 0:55:21, time: 0.693, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4038, decode.acc_seg: 84.4663, loss: 0.4038
2023-11-10 17:12:52,733 - mmseg - INFO - Iter [32650/160000]	lr: 9.007e-05, eta: 1 day, 0:54:39, time: 0.668, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4211, decode.acc_seg: 83.8554, loss: 0.4211
2023-11-10 17:13:24,255 - mmseg - INFO - Iter [32700/160000]	lr: 9.004e-05, eta: 1 day, 0:53:49, time: 0.630, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4396, decode.acc_seg: 83.7595, loss: 0.4396
2023-11-10 17:13:58,845 - mmseg - INFO - Iter [32750/160000]	lr: 9.001e-05, eta: 1 day, 0:53:12, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4199, decode.acc_seg: 84.1183, loss: 0.4199
2023-11-10 17:14:33,528 - mmseg - INFO - Iter [32800/160000]	lr: 8.998e-05, eta: 1 day, 0:52:34, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3872, decode.acc_seg: 85.0757, loss: 0.3872
2023-11-10 17:15:08,192 - mmseg - INFO - Iter [32850/160000]	lr: 8.996e-05, eta: 1 day, 0:51:57, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4128, decode.acc_seg: 84.0730, loss: 0.4128
2023-11-10 17:15:42,824 - mmseg - INFO - Iter [32900/160000]	lr: 8.993e-05, eta: 1 day, 0:51:20, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4129, decode.acc_seg: 83.8865, loss: 0.4129
2023-11-10 17:16:17,558 - mmseg - INFO - Iter [32950/160000]	lr: 8.990e-05, eta: 1 day, 0:50:43, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3985, decode.acc_seg: 84.8508, loss: 0.3985
2023-11-10 17:16:51,204 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 17:16:51,204 - mmseg - INFO - Iter [33000/160000]	lr: 8.987e-05, eta: 1 day, 0:50:02, time: 0.674, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4368, decode.acc_seg: 84.0058, loss: 0.4368
2023-11-10 17:17:23,638 - mmseg - INFO - Iter [33050/160000]	lr: 8.984e-05, eta: 1 day, 0:49:16, time: 0.648, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3957, decode.acc_seg: 84.7206, loss: 0.3957
2023-11-10 17:17:59,189 - mmseg - INFO - Iter [33100/160000]	lr: 8.981e-05, eta: 1 day, 0:48:42, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4063, decode.acc_seg: 84.6916, loss: 0.4063
2023-11-10 17:18:31,393 - mmseg - INFO - Iter [33150/160000]	lr: 8.978e-05, eta: 1 day, 0:47:55, time: 0.645, data_time: 0.011, memory: 23129, decode.loss_ce: 0.4049, decode.acc_seg: 84.7124, loss: 0.4049
2023-11-10 17:19:04,656 - mmseg - INFO - Iter [33200/160000]	lr: 8.975e-05, eta: 1 day, 0:47:13, time: 0.665, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4153, decode.acc_seg: 84.3070, loss: 0.4153
2023-11-10 17:19:37,105 - mmseg - INFO - Iter [33250/160000]	lr: 8.972e-05, eta: 1 day, 0:46:27, time: 0.648, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4243, decode.acc_seg: 83.8610, loss: 0.4243
2023-11-10 17:20:12,013 - mmseg - INFO - Iter [33300/160000]	lr: 8.969e-05, eta: 1 day, 0:45:51, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3991, decode.acc_seg: 84.9134, loss: 0.3991
2023-11-10 17:20:46,739 - mmseg - INFO - Iter [33350/160000]	lr: 8.966e-05, eta: 1 day, 0:45:14, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4007, decode.acc_seg: 84.9831, loss: 0.4007
2023-11-10 17:21:19,138 - mmseg - INFO - Iter [33400/160000]	lr: 8.963e-05, eta: 1 day, 0:44:28, time: 0.649, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4004, decode.acc_seg: 84.6925, loss: 0.4004
2023-11-10 17:21:50,700 - mmseg - INFO - Iter [33450/160000]	lr: 8.960e-05, eta: 1 day, 0:43:39, time: 0.631, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4050, decode.acc_seg: 84.5612, loss: 0.4050
2023-11-10 17:22:22,693 - mmseg - INFO - Iter [33500/160000]	lr: 8.957e-05, eta: 1 day, 0:42:52, time: 0.640, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4222, decode.acc_seg: 83.8479, loss: 0.4222
2023-11-10 17:22:54,628 - mmseg - INFO - Iter [33550/160000]	lr: 8.954e-05, eta: 1 day, 0:42:05, time: 0.639, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3965, decode.acc_seg: 84.7573, loss: 0.3965
2023-11-10 17:23:28,452 - mmseg - INFO - Iter [33600/160000]	lr: 8.951e-05, eta: 1 day, 0:41:25, time: 0.675, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4176, decode.acc_seg: 84.3822, loss: 0.4176
2023-11-10 17:24:03,258 - mmseg - INFO - Iter [33650/160000]	lr: 8.948e-05, eta: 1 day, 0:40:48, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4225, decode.acc_seg: 84.2569, loss: 0.4225
2023-11-10 17:24:38,024 - mmseg - INFO - Iter [33700/160000]	lr: 8.945e-05, eta: 1 day, 0:40:11, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4118, decode.acc_seg: 84.1853, loss: 0.4118
2023-11-10 17:25:12,842 - mmseg - INFO - Iter [33750/160000]	lr: 8.942e-05, eta: 1 day, 0:39:35, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3800, decode.acc_seg: 85.5434, loss: 0.3800
2023-11-10 17:25:47,812 - mmseg - INFO - Iter [33800/160000]	lr: 8.939e-05, eta: 1 day, 0:38:59, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4241, decode.acc_seg: 84.4437, loss: 0.4241
2023-11-10 17:26:22,539 - mmseg - INFO - Iter [33850/160000]	lr: 8.936e-05, eta: 1 day, 0:38:22, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3872, decode.acc_seg: 85.2845, loss: 0.3872
2023-11-10 17:26:58,229 - mmseg - INFO - Iter [33900/160000]	lr: 8.933e-05, eta: 1 day, 0:37:49, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3817, decode.acc_seg: 84.9948, loss: 0.3817
2023-11-10 17:27:33,155 - mmseg - INFO - Iter [33950/160000]	lr: 8.930e-05, eta: 1 day, 0:37:13, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3950, decode.acc_seg: 84.6371, loss: 0.3950
2023-11-10 17:28:07,080 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 17:28:07,080 - mmseg - INFO - Iter [34000/160000]	lr: 8.927e-05, eta: 1 day, 0:36:33, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3949, decode.acc_seg: 84.9391, loss: 0.3949
2023-11-10 17:28:41,885 - mmseg - INFO - Iter [34050/160000]	lr: 8.924e-05, eta: 1 day, 0:35:57, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3945, decode.acc_seg: 85.0711, loss: 0.3945
2023-11-10 17:29:17,080 - mmseg - INFO - Iter [34100/160000]	lr: 8.921e-05, eta: 1 day, 0:35:22, time: 0.704, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4131, decode.acc_seg: 84.5940, loss: 0.4131
2023-11-10 17:29:50,617 - mmseg - INFO - Iter [34150/160000]	lr: 8.918e-05, eta: 1 day, 0:34:41, time: 0.671, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3974, decode.acc_seg: 84.6637, loss: 0.3974
2023-11-10 17:30:25,766 - mmseg - INFO - Iter [34200/160000]	lr: 8.914e-05, eta: 1 day, 0:34:06, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4181, decode.acc_seg: 84.2762, loss: 0.4181
2023-11-10 17:30:59,049 - mmseg - INFO - Iter [34250/160000]	lr: 8.911e-05, eta: 1 day, 0:33:24, time: 0.667, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3746, decode.acc_seg: 85.7412, loss: 0.3746
2023-11-10 17:31:32,708 - mmseg - INFO - Iter [34300/160000]	lr: 8.908e-05, eta: 1 day, 0:32:43, time: 0.672, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4109, decode.acc_seg: 84.0859, loss: 0.4109
2023-11-10 17:32:08,119 - mmseg - INFO - Iter [34350/160000]	lr: 8.905e-05, eta: 1 day, 0:32:09, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3777, decode.acc_seg: 85.3766, loss: 0.3777
2023-11-10 17:32:42,931 - mmseg - INFO - Iter [34400/160000]	lr: 8.902e-05, eta: 1 day, 0:31:32, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3802, decode.acc_seg: 85.4777, loss: 0.3802
2023-11-10 17:33:17,228 - mmseg - INFO - Iter [34450/160000]	lr: 8.899e-05, eta: 1 day, 0:30:54, time: 0.687, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3738, decode.acc_seg: 85.5417, loss: 0.3738
2023-11-10 17:33:49,392 - mmseg - INFO - Iter [34500/160000]	lr: 8.896e-05, eta: 1 day, 0:30:08, time: 0.642, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3700, decode.acc_seg: 85.7450, loss: 0.3700
2023-11-10 17:34:24,176 - mmseg - INFO - Iter [34550/160000]	lr: 8.893e-05, eta: 1 day, 0:29:32, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3905, decode.acc_seg: 84.8911, loss: 0.3905
2023-11-10 17:34:56,902 - mmseg - INFO - Iter [34600/160000]	lr: 8.890e-05, eta: 1 day, 0:28:48, time: 0.655, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3845, decode.acc_seg: 84.9241, loss: 0.3845
2023-11-10 17:35:32,363 - mmseg - INFO - Iter [34650/160000]	lr: 8.887e-05, eta: 1 day, 0:28:14, time: 0.708, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3952, decode.acc_seg: 84.3937, loss: 0.3952
2023-11-10 17:36:07,666 - mmseg - INFO - Iter [34700/160000]	lr: 8.884e-05, eta: 1 day, 0:27:39, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3791, decode.acc_seg: 85.4993, loss: 0.3791
2023-11-10 17:36:42,099 - mmseg - INFO - Iter [34750/160000]	lr: 8.881e-05, eta: 1 day, 0:27:02, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4128, decode.acc_seg: 84.8633, loss: 0.4128
2023-11-10 17:37:13,560 - mmseg - INFO - Iter [34800/160000]	lr: 8.878e-05, eta: 1 day, 0:26:13, time: 0.629, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4280, decode.acc_seg: 84.1039, loss: 0.4280
2023-11-10 17:37:44,865 - mmseg - INFO - Iter [34850/160000]	lr: 8.874e-05, eta: 1 day, 0:25:25, time: 0.626, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4162, decode.acc_seg: 83.9376, loss: 0.4162
2023-11-10 17:38:17,441 - mmseg - INFO - Iter [34900/160000]	lr: 8.871e-05, eta: 1 day, 0:24:40, time: 0.651, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3867, decode.acc_seg: 84.9952, loss: 0.3867
2023-11-10 17:38:50,306 - mmseg - INFO - Iter [34950/160000]	lr: 8.868e-05, eta: 1 day, 0:23:57, time: 0.657, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4153, decode.acc_seg: 84.3431, loss: 0.4153
2023-11-10 17:39:25,164 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 17:39:25,165 - mmseg - INFO - Iter [35000/160000]	lr: 8.865e-05, eta: 1 day, 0:23:21, time: 0.696, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3870, decode.acc_seg: 84.7869, loss: 0.3870
2023-11-10 17:40:00,673 - mmseg - INFO - Iter [35050/160000]	lr: 8.862e-05, eta: 1 day, 0:22:47, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4077, decode.acc_seg: 84.8584, loss: 0.4077
2023-11-10 17:40:36,281 - mmseg - INFO - Iter [35100/160000]	lr: 8.859e-05, eta: 1 day, 0:22:14, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3744, decode.acc_seg: 85.6955, loss: 0.3744
2023-11-10 17:41:11,432 - mmseg - INFO - Iter [35150/160000]	lr: 8.856e-05, eta: 1 day, 0:21:39, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3691, decode.acc_seg: 85.8310, loss: 0.3691
2023-11-10 17:41:46,724 - mmseg - INFO - Iter [35200/160000]	lr: 8.853e-05, eta: 1 day, 0:21:04, time: 0.706, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3841, decode.acc_seg: 85.3933, loss: 0.3841
2023-11-10 17:42:22,150 - mmseg - INFO - Iter [35250/160000]	lr: 8.849e-05, eta: 1 day, 0:20:30, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3726, decode.acc_seg: 85.7165, loss: 0.3726
2023-11-10 17:42:56,088 - mmseg - INFO - Iter [35300/160000]	lr: 8.846e-05, eta: 1 day, 0:19:51, time: 0.680, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3736, decode.acc_seg: 85.7422, loss: 0.3736
2023-11-10 17:43:29,678 - mmseg - INFO - Iter [35350/160000]	lr: 8.843e-05, eta: 1 day, 0:19:10, time: 0.671, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3618, decode.acc_seg: 85.5262, loss: 0.3618
2023-11-10 17:44:04,780 - mmseg - INFO - Iter [35400/160000]	lr: 8.840e-05, eta: 1 day, 0:18:35, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3929, decode.acc_seg: 84.6383, loss: 0.3929
2023-11-10 17:44:40,203 - mmseg - INFO - Iter [35450/160000]	lr: 8.837e-05, eta: 1 day, 0:18:01, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3812, decode.acc_seg: 85.3113, loss: 0.3812
2023-11-10 17:45:14,905 - mmseg - INFO - Iter [35500/160000]	lr: 8.834e-05, eta: 1 day, 0:17:24, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4305, decode.acc_seg: 83.7917, loss: 0.4305
2023-11-10 17:45:48,392 - mmseg - INFO - Iter [35550/160000]	lr: 8.831e-05, eta: 1 day, 0:16:44, time: 0.670, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3986, decode.acc_seg: 84.6984, loss: 0.3986
2023-11-10 17:46:23,728 - mmseg - INFO - Iter [35600/160000]	lr: 8.827e-05, eta: 1 day, 0:16:09, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3819, decode.acc_seg: 85.2554, loss: 0.3819
2023-11-10 17:46:56,406 - mmseg - INFO - Iter [35650/160000]	lr: 8.824e-05, eta: 1 day, 0:15:26, time: 0.654, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3932, decode.acc_seg: 85.1699, loss: 0.3932
2023-11-10 17:47:28,861 - mmseg - INFO - Iter [35700/160000]	lr: 8.821e-05, eta: 1 day, 0:14:41, time: 0.650, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3851, decode.acc_seg: 85.6723, loss: 0.3851
2023-11-10 17:48:01,942 - mmseg - INFO - Iter [35750/160000]	lr: 8.818e-05, eta: 1 day, 0:13:59, time: 0.660, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4071, decode.acc_seg: 84.5247, loss: 0.4071
2023-11-10 17:48:36,422 - mmseg - INFO - Iter [35800/160000]	lr: 8.815e-05, eta: 1 day, 0:13:22, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3939, decode.acc_seg: 84.7309, loss: 0.3939
2023-11-10 17:49:11,099 - mmseg - INFO - Iter [35850/160000]	lr: 8.812e-05, eta: 1 day, 0:12:45, time: 0.693, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3766, decode.acc_seg: 85.3221, loss: 0.3766
2023-11-10 17:49:42,803 - mmseg - INFO - Iter [35900/160000]	lr: 8.808e-05, eta: 1 day, 0:11:59, time: 0.634, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3936, decode.acc_seg: 84.9618, loss: 0.3936
2023-11-10 17:50:17,783 - mmseg - INFO - Iter [35950/160000]	lr: 8.805e-05, eta: 1 day, 0:11:23, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3993, decode.acc_seg: 85.0342, loss: 0.3993
2023-11-10 17:50:52,990 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 17:50:52,990 - mmseg - INFO - Iter [36000/160000]	lr: 8.802e-05, eta: 1 day, 0:10:48, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3635, decode.acc_seg: 85.7714, loss: 0.3635
2023-11-10 17:51:28,192 - mmseg - INFO - Iter [36050/160000]	lr: 8.799e-05, eta: 1 day, 0:10:14, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3989, decode.acc_seg: 84.7610, loss: 0.3989
2023-11-10 17:52:02,451 - mmseg - INFO - Iter [36100/160000]	lr: 8.796e-05, eta: 1 day, 0:09:35, time: 0.684, data_time: 0.008, memory: 23129, decode.loss_ce: 0.3998, decode.acc_seg: 84.8159, loss: 0.3998
2023-11-10 17:52:38,424 - mmseg - INFO - Iter [36150/160000]	lr: 8.793e-05, eta: 1 day, 0:09:03, time: 0.719, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3880, decode.acc_seg: 85.1743, loss: 0.3880
2023-11-10 17:53:13,914 - mmseg - INFO - Iter [36200/160000]	lr: 8.789e-05, eta: 1 day, 0:08:29, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4086, decode.acc_seg: 84.4938, loss: 0.4086
2023-11-10 17:53:47,051 - mmseg - INFO - Iter [36250/160000]	lr: 8.786e-05, eta: 1 day, 0:07:48, time: 0.664, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3985, decode.acc_seg: 85.1876, loss: 0.3985
2023-11-10 17:54:20,065 - mmseg - INFO - Iter [36300/160000]	lr: 8.783e-05, eta: 1 day, 0:07:06, time: 0.660, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3876, decode.acc_seg: 85.3171, loss: 0.3876
2023-11-10 17:54:54,182 - mmseg - INFO - Iter [36350/160000]	lr: 8.780e-05, eta: 1 day, 0:06:27, time: 0.681, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4022, decode.acc_seg: 84.6607, loss: 0.4022
2023-11-10 17:55:28,681 - mmseg - INFO - Iter [36400/160000]	lr: 8.776e-05, eta: 1 day, 0:05:50, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3862, decode.acc_seg: 85.2131, loss: 0.3862
2023-11-10 17:56:02,718 - mmseg - INFO - Iter [36450/160000]	lr: 8.773e-05, eta: 1 day, 0:05:11, time: 0.681, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3969, decode.acc_seg: 84.8527, loss: 0.3969
2023-11-10 17:56:34,509 - mmseg - INFO - Iter [36500/160000]	lr: 8.770e-05, eta: 1 day, 0:04:25, time: 0.636, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3724, decode.acc_seg: 85.6684, loss: 0.3724
2023-11-10 17:57:06,278 - mmseg - INFO - Iter [36550/160000]	lr: 8.767e-05, eta: 1 day, 0:03:39, time: 0.635, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3852, decode.acc_seg: 85.2249, loss: 0.3852
2023-11-10 17:57:41,289 - mmseg - INFO - Iter [36600/160000]	lr: 8.764e-05, eta: 1 day, 0:03:03, time: 0.699, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3824, decode.acc_seg: 85.5509, loss: 0.3824
2023-11-10 17:58:16,435 - mmseg - INFO - Iter [36650/160000]	lr: 8.760e-05, eta: 1 day, 0:02:28, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3751, decode.acc_seg: 85.4820, loss: 0.3751
2023-11-10 17:58:50,782 - mmseg - INFO - Iter [36700/160000]	lr: 8.757e-05, eta: 1 day, 0:01:51, time: 0.687, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3851, decode.acc_seg: 85.1402, loss: 0.3851
2023-11-10 17:59:25,704 - mmseg - INFO - Iter [36750/160000]	lr: 8.754e-05, eta: 1 day, 0:01:16, time: 0.699, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3790, decode.acc_seg: 85.4996, loss: 0.3790
2023-11-10 17:59:57,664 - mmseg - INFO - Iter [36800/160000]	lr: 8.751e-05, eta: 1 day, 0:00:30, time: 0.638, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3927, decode.acc_seg: 84.6654, loss: 0.3927
2023-11-10 18:00:31,875 - mmseg - INFO - Iter [36850/160000]	lr: 8.747e-05, eta: 23:59:52, time: 0.686, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3618, decode.acc_seg: 85.8252, loss: 0.3618
2023-11-10 18:01:04,899 - mmseg - INFO - Iter [36900/160000]	lr: 8.744e-05, eta: 23:59:10, time: 0.659, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4077, decode.acc_seg: 84.0443, loss: 0.4077
2023-11-10 18:01:39,797 - mmseg - INFO - Iter [36950/160000]	lr: 8.741e-05, eta: 23:58:34, time: 0.698, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3974, decode.acc_seg: 85.2904, loss: 0.3974
2023-11-10 18:02:14,431 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 18:02:14,431 - mmseg - INFO - Iter [37000/160000]	lr: 8.738e-05, eta: 23:57:58, time: 0.692, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3833, decode.acc_seg: 85.1603, loss: 0.3833
2023-11-10 18:02:48,121 - mmseg - INFO - Iter [37050/160000]	lr: 8.734e-05, eta: 23:57:18, time: 0.675, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3999, decode.acc_seg: 84.7102, loss: 0.3999
2023-11-10 18:03:22,571 - mmseg - INFO - Iter [37100/160000]	lr: 8.731e-05, eta: 23:56:41, time: 0.689, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3833, decode.acc_seg: 85.5577, loss: 0.3833
2023-11-10 18:03:54,343 - mmseg - INFO - Iter [37150/160000]	lr: 8.728e-05, eta: 23:55:55, time: 0.635, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3801, decode.acc_seg: 85.0182, loss: 0.3801
2023-11-10 18:04:25,845 - mmseg - INFO - Iter [37200/160000]	lr: 8.725e-05, eta: 23:55:08, time: 0.631, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3851, decode.acc_seg: 85.4738, loss: 0.3851
2023-11-10 18:04:59,977 - mmseg - INFO - Iter [37250/160000]	lr: 8.721e-05, eta: 23:54:30, time: 0.683, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3890, decode.acc_seg: 84.7679, loss: 0.3890
2023-11-10 18:05:31,501 - mmseg - INFO - Iter [37300/160000]	lr: 8.718e-05, eta: 23:53:43, time: 0.630, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3762, decode.acc_seg: 85.7512, loss: 0.3762
2023-11-10 18:06:06,477 - mmseg - INFO - Iter [37350/160000]	lr: 8.715e-05, eta: 23:53:08, time: 0.700, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3872, decode.acc_seg: 85.0390, loss: 0.3872
2023-11-10 18:06:41,797 - mmseg - INFO - Iter [37400/160000]	lr: 8.711e-05, eta: 23:52:34, time: 0.705, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3709, decode.acc_seg: 85.8349, loss: 0.3709
2023-11-10 18:07:17,180 - mmseg - INFO - Iter [37450/160000]	lr: 8.708e-05, eta: 23:52:00, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3727, decode.acc_seg: 85.3959, loss: 0.3727
2023-11-10 18:07:53,008 - mmseg - INFO - Iter [37500/160000]	lr: 8.705e-05, eta: 23:51:27, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3601, decode.acc_seg: 85.8098, loss: 0.3601
2023-11-10 18:08:28,263 - mmseg - INFO - Iter [37550/160000]	lr: 8.702e-05, eta: 23:50:53, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3603, decode.acc_seg: 86.1735, loss: 0.3603
2023-11-10 18:09:03,302 - mmseg - INFO - Iter [37600/160000]	lr: 8.698e-05, eta: 23:50:18, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3642, decode.acc_seg: 86.4783, loss: 0.3642
2023-11-10 18:09:38,315 - mmseg - INFO - Iter [37650/160000]	lr: 8.695e-05, eta: 23:49:43, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3775, decode.acc_seg: 86.0667, loss: 0.3775
2023-11-10 18:10:13,479 - mmseg - INFO - Iter [37700/160000]	lr: 8.692e-05, eta: 23:49:08, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3739, decode.acc_seg: 85.7755, loss: 0.3739
2023-11-10 18:10:46,601 - mmseg - INFO - Iter [37750/160000]	lr: 8.688e-05, eta: 23:48:27, time: 0.664, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3708, decode.acc_seg: 85.7794, loss: 0.3708
2023-11-10 18:11:18,371 - mmseg - INFO - Iter [37800/160000]	lr: 8.685e-05, eta: 23:47:41, time: 0.635, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3563, decode.acc_seg: 86.0071, loss: 0.3563
2023-11-10 18:11:51,542 - mmseg - INFO - Iter [37850/160000]	lr: 8.682e-05, eta: 23:47:00, time: 0.662, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3722, decode.acc_seg: 85.9070, loss: 0.3722
2023-11-10 18:12:26,371 - mmseg - INFO - Iter [37900/160000]	lr: 8.678e-05, eta: 23:46:24, time: 0.697, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3774, decode.acc_seg: 85.2376, loss: 0.3774
2023-11-10 18:13:01,782 - mmseg - INFO - Iter [37950/160000]	lr: 8.675e-05, eta: 23:45:50, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3669, decode.acc_seg: 85.6607, loss: 0.3669
2023-11-10 18:13:33,840 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 18:13:33,840 - mmseg - INFO - Iter [38000/160000]	lr: 8.672e-05, eta: 23:45:06, time: 0.642, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3679, decode.acc_seg: 85.8903, loss: 0.3679
2023-11-10 18:14:08,704 - mmseg - INFO - Iter [38050/160000]	lr: 8.668e-05, eta: 23:44:30, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3806, decode.acc_seg: 85.3354, loss: 0.3806
2023-11-10 18:14:43,545 - mmseg - INFO - Iter [38100/160000]	lr: 8.665e-05, eta: 23:43:54, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3607, decode.acc_seg: 85.9134, loss: 0.3607
2023-11-10 18:15:17,155 - mmseg - INFO - Iter [38150/160000]	lr: 8.662e-05, eta: 23:43:15, time: 0.671, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3418, decode.acc_seg: 86.3615, loss: 0.3418
2023-11-10 18:15:52,698 - mmseg - INFO - Iter [38200/160000]	lr: 8.658e-05, eta: 23:42:41, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3547, decode.acc_seg: 86.2849, loss: 0.3547
2023-11-10 18:16:26,834 - mmseg - INFO - Iter [38250/160000]	lr: 8.655e-05, eta: 23:42:03, time: 0.684, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4039, decode.acc_seg: 84.3852, loss: 0.4039
2023-11-10 18:17:00,163 - mmseg - INFO - Iter [38300/160000]	lr: 8.652e-05, eta: 23:41:23, time: 0.666, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3956, decode.acc_seg: 85.1670, loss: 0.3956
2023-11-10 18:17:33,826 - mmseg - INFO - Iter [38350/160000]	lr: 8.648e-05, eta: 23:40:43, time: 0.673, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3843, decode.acc_seg: 85.3744, loss: 0.3843
2023-11-10 18:18:06,624 - mmseg - INFO - Iter [38400/160000]	lr: 8.645e-05, eta: 23:40:01, time: 0.656, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3921, decode.acc_seg: 84.8609, loss: 0.3921
2023-11-10 18:18:41,789 - mmseg - INFO - Iter [38450/160000]	lr: 8.642e-05, eta: 23:39:27, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3929, decode.acc_seg: 85.0531, loss: 0.3929
2023-11-10 18:19:15,554 - mmseg - INFO - Iter [38500/160000]	lr: 8.638e-05, eta: 23:38:48, time: 0.676, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3663, decode.acc_seg: 85.2886, loss: 0.3663
2023-11-10 18:19:47,159 - mmseg - INFO - Iter [38550/160000]	lr: 8.635e-05, eta: 23:38:02, time: 0.632, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3591, decode.acc_seg: 86.4111, loss: 0.3591
2023-11-10 18:20:19,521 - mmseg - INFO - Iter [38600/160000]	lr: 8.631e-05, eta: 23:37:18, time: 0.647, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3578, decode.acc_seg: 86.2273, loss: 0.3578
2023-11-10 18:20:52,521 - mmseg - INFO - Iter [38650/160000]	lr: 8.628e-05, eta: 23:36:37, time: 0.659, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3990, decode.acc_seg: 84.8674, loss: 0.3990
2023-11-10 18:21:26,484 - mmseg - INFO - Iter [38700/160000]	lr: 8.625e-05, eta: 23:35:59, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3773, decode.acc_seg: 85.3588, loss: 0.3773
2023-11-10 18:22:02,270 - mmseg - INFO - Iter [38750/160000]	lr: 8.621e-05, eta: 23:35:26, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3695, decode.acc_seg: 85.6770, loss: 0.3695
2023-11-10 18:22:38,077 - mmseg - INFO - Iter [38800/160000]	lr: 8.618e-05, eta: 23:34:53, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4016, decode.acc_seg: 84.7443, loss: 0.4016
2023-11-10 18:23:13,694 - mmseg - INFO - Iter [38850/160000]	lr: 8.615e-05, eta: 23:34:20, time: 0.713, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3883, decode.acc_seg: 85.1278, loss: 0.3883
2023-11-10 18:23:46,479 - mmseg - INFO - Iter [38900/160000]	lr: 8.611e-05, eta: 23:33:38, time: 0.657, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3761, decode.acc_seg: 86.0703, loss: 0.3761
2023-11-10 18:24:20,587 - mmseg - INFO - Iter [38950/160000]	lr: 8.608e-05, eta: 23:33:00, time: 0.681, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3969, decode.acc_seg: 84.9052, loss: 0.3969
2023-11-10 18:24:55,544 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 18:24:55,545 - mmseg - INFO - Iter [39000/160000]	lr: 8.604e-05, eta: 23:32:25, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3580, decode.acc_seg: 86.5362, loss: 0.3580
2023-11-10 18:25:30,403 - mmseg - INFO - Iter [39050/160000]	lr: 8.601e-05, eta: 23:31:50, time: 0.698, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3773, decode.acc_seg: 85.3447, loss: 0.3773
2023-11-10 18:26:02,288 - mmseg - INFO - Iter [39100/160000]	lr: 8.598e-05, eta: 23:31:05, time: 0.638, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3984, decode.acc_seg: 84.8060, loss: 0.3984
2023-11-10 18:26:35,701 - mmseg - INFO - Iter [39150/160000]	lr: 8.594e-05, eta: 23:30:25, time: 0.668, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3711, decode.acc_seg: 85.5877, loss: 0.3711
2023-11-10 18:27:09,481 - mmseg - INFO - Iter [39200/160000]	lr: 8.591e-05, eta: 23:29:46, time: 0.675, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4053, decode.acc_seg: 84.8245, loss: 0.4053
2023-11-10 18:27:45,500 - mmseg - INFO - Iter [39250/160000]	lr: 8.587e-05, eta: 23:29:14, time: 0.720, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3770, decode.acc_seg: 85.4994, loss: 0.3770
2023-11-10 18:28:20,846 - mmseg - INFO - Iter [39300/160000]	lr: 8.584e-05, eta: 23:28:40, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3791, decode.acc_seg: 85.5228, loss: 0.3791
2023-11-10 18:28:56,407 - mmseg - INFO - Iter [39350/160000]	lr: 8.580e-05, eta: 23:28:07, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3558, decode.acc_seg: 86.2073, loss: 0.3558
2023-11-10 18:29:31,937 - mmseg - INFO - Iter [39400/160000]	lr: 8.577e-05, eta: 23:27:33, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3533, decode.acc_seg: 85.8472, loss: 0.3533
2023-11-10 18:30:04,747 - mmseg - INFO - Iter [39450/160000]	lr: 8.574e-05, eta: 23:26:52, time: 0.657, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3749, decode.acc_seg: 85.4703, loss: 0.3749
2023-11-10 18:30:40,142 - mmseg - INFO - Iter [39500/160000]	lr: 8.570e-05, eta: 23:26:18, time: 0.707, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3568, decode.acc_seg: 85.9075, loss: 0.3568
2023-11-10 18:31:15,389 - mmseg - INFO - Iter [39550/160000]	lr: 8.567e-05, eta: 23:25:44, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3865, decode.acc_seg: 85.0603, loss: 0.3865
2023-11-10 18:31:47,210 - mmseg - INFO - Iter [39600/160000]	lr: 8.563e-05, eta: 23:24:59, time: 0.636, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3604, decode.acc_seg: 85.7726, loss: 0.3604
2023-11-10 18:32:22,032 - mmseg - INFO - Iter [39650/160000]	lr: 8.560e-05, eta: 23:24:23, time: 0.696, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3785, decode.acc_seg: 85.4957, loss: 0.3785
2023-11-10 18:32:57,212 - mmseg - INFO - Iter [39700/160000]	lr: 8.556e-05, eta: 23:23:49, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3792, decode.acc_seg: 85.6693, loss: 0.3792
2023-11-10 18:33:30,888 - mmseg - INFO - Iter [39750/160000]	lr: 8.553e-05, eta: 23:23:10, time: 0.674, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3581, decode.acc_seg: 85.7706, loss: 0.3581
2023-11-10 18:34:05,609 - mmseg - INFO - Iter [39800/160000]	lr: 8.549e-05, eta: 23:22:34, time: 0.693, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3752, decode.acc_seg: 85.4262, loss: 0.3752
2023-11-10 18:34:41,788 - mmseg - INFO - Iter [39850/160000]	lr: 8.546e-05, eta: 23:22:02, time: 0.723, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3614, decode.acc_seg: 85.9117, loss: 0.3614
2023-11-10 18:35:18,444 - mmseg - INFO - Iter [39900/160000]	lr: 8.543e-05, eta: 23:21:32, time: 0.734, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3688, decode.acc_seg: 85.5262, loss: 0.3688
2023-11-10 18:35:51,892 - mmseg - INFO - Iter [39950/160000]	lr: 8.539e-05, eta: 23:20:53, time: 0.669, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3900, decode.acc_seg: 85.2560, loss: 0.3900
2023-11-10 18:36:24,354 - mmseg - INFO - Saving checkpoint at 40000 iterations
2023-11-10 18:36:29,017 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 18:36:29,017 - mmseg - INFO - Iter [40000/160000]	lr: 8.536e-05, eta: 23:20:24, time: 0.744, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3760, decode.acc_seg: 85.2014, loss: 0.3760
2023-11-10 18:38:25,956 - mmseg - INFO - per class results:
2023-11-10 18:38:25,970 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 74.67 | 86.75 |
|       building      | 82.24 | 91.37 |
|         sky         | 93.99 | 97.99 |
|        floor        | 81.47 | 88.73 |
|         tree        | 72.88 |  84.9 |
|       ceiling       | 81.78 | 91.39 |
|         road        |  81.7 | 90.03 |
|         bed         | 87.76 | 94.14 |
|      windowpane     | 58.47 | 76.55 |
|        grass        | 66.83 | 81.66 |
|       cabinet       |  55.8 | 67.62 |
|       sidewalk      | 64.18 | 77.09 |
|        person       |  78.4 | 92.23 |
|        earth        | 32.15 | 43.16 |
|         door        | 43.23 | 62.83 |
|        table        |  52.2 |  69.5 |
|       mountain      | 57.89 | 72.12 |
|        plant        | 50.99 | 61.78 |
|       curtain       | 71.95 | 86.94 |
|        chair        | 51.03 | 63.84 |
|         car         |  80.6 | 91.45 |
|        water        | 55.43 | 67.25 |
|       painting      | 67.03 | 85.05 |
|         sofa        | 61.14 | 73.06 |
|        shelf        | 38.65 | 51.92 |
|        house        | 49.26 | 67.13 |
|         sea         | 56.96 | 81.95 |
|        mirror       | 58.98 | 65.74 |
|         rug         | 63.44 | 77.01 |
|        field        | 36.02 | 59.24 |
|       armchair      | 36.94 | 66.75 |
|         seat        | 59.91 | 71.95 |
|        fence        | 32.72 | 42.52 |
|         desk        | 45.53 | 71.66 |
|         rock        | 40.76 | 70.24 |
|       wardrobe      | 46.47 | 59.44 |
|         lamp        | 58.82 | 70.75 |
|       bathtub       |  70.8 | 78.86 |
|       railing       | 32.64 | 47.45 |
|       cushion       | 50.46 | 68.09 |
|         base        | 31.22 | 51.18 |
|         box         | 21.97 | 26.82 |
|        column       | 45.88 | 61.09 |
|      signboard      | 32.03 | 43.28 |
|   chest of drawers  | 43.28 | 61.31 |
|       counter       | 24.01 | 27.21 |
|         sand        | 29.81 | 53.08 |
|         sink        | 65.01 | 72.88 |
|      skyscraper     | 47.24 | 56.07 |
|      fireplace      | 66.05 | 90.47 |
|     refrigerator    | 76.23 | 88.21 |
|      grandstand     | 42.65 | 66.01 |
|         path        | 21.23 | 42.92 |
|        stairs       | 31.69 | 37.08 |
|        runway       | 68.44 | 94.59 |
|         case        | 53.94 |  80.1 |
|      pool table     | 88.28 | 96.81 |
|        pillow       | 53.59 | 64.06 |
|     screen door     | 50.14 | 67.99 |
|       stairway      | 28.96 | 35.11 |
|        river        |  9.98 | 25.22 |
|        bridge       |  67.6 | 87.16 |
|       bookcase      | 32.13 | 44.62 |
|        blind        |  37.5 | 41.87 |
|     coffee table    | 47.19 | 83.32 |
|        toilet       | 73.46 | 91.08 |
|        flower       | 31.67 | 50.85 |
|         book        | 43.78 | 64.03 |
|         hill        |  4.62 |  8.32 |
|        bench        | 43.99 | 49.32 |
|      countertop     | 55.05 | 71.87 |
|        stove        | 67.69 | 77.37 |
|         palm        | 43.13 | 73.59 |
|    kitchen island   | 29.74 | 82.84 |
|       computer      | 56.16 |  62.8 |
|     swivel chair    | 44.29 | 59.69 |
|         boat        | 45.53 | 67.02 |
|         bar         | 36.04 | 43.87 |
|    arcade machine   | 84.21 | 92.06 |
|        hovel        | 38.31 | 46.43 |
|         bus         | 78.32 | 94.86 |
|        towel        | 57.66 | 67.86 |
|        light        | 47.01 | 53.98 |
|        truck        | 32.52 | 54.92 |
|        tower        | 34.49 | 50.53 |
|      chandelier     | 62.71 | 78.99 |
|        awning       | 22.28 | 31.37 |
|     streetlight     |  20.0 | 25.56 |
|        booth        | 46.26 | 50.32 |
| television receiver | 65.16 | 82.02 |
|       airplane      | 56.54 | 74.35 |
|      dirt track     |  3.86 | 19.19 |
|       apparel       | 36.03 | 51.97 |
|         pole        | 18.92 | 27.16 |
|         land        |  0.0  |  0.0  |
|      bannister      | 14.72 | 18.39 |
|      escalator      | 27.87 | 32.98 |
|       ottoman       | 34.61 | 46.03 |
|        bottle       | 34.46 | 58.86 |
|        buffet       | 38.47 | 44.24 |
|        poster       | 37.28 | 51.29 |
|        stage        | 15.52 | 19.83 |
|         van         | 38.01 | 50.23 |
|         ship        | 68.11 | 87.95 |
|       fountain      | 26.39 | 26.96 |
|    conveyer belt    | 75.04 | 85.74 |
|        canopy       | 13.82 | 21.62 |
|        washer       | 74.22 | 76.23 |
|      plaything      | 23.04 | 41.38 |
|    swimming pool    | 51.85 | 75.15 |
|        stool        | 36.86 | 46.17 |
|        barrel       | 48.76 | 64.98 |
|        basket       | 36.06 |  47.3 |
|      waterfall      | 57.18 | 84.72 |
|         tent        | 74.13 | 98.22 |
|         bag         |  9.75 | 10.82 |
|       minibike      | 53.57 | 60.69 |
|        cradle       | 77.19 | 97.58 |
|         oven        | 34.16 | 43.05 |
|         ball        | 38.08 | 44.87 |
|         food        | 52.55 | 64.65 |
|         step        |  3.34 |  3.74 |
|         tank        | 49.58 | 55.03 |
|      trade name     | 12.59 | 13.25 |
|      microwave      | 73.38 | 92.98 |
|         pot         | 37.12 | 45.12 |
|        animal       |  59.1 | 67.26 |
|       bicycle       | 53.39 | 70.18 |
|         lake        | 14.29 | 14.57 |
|      dishwasher     | 51.47 | 77.13 |
|        screen       | 61.56 | 90.86 |
|       blanket       |  3.71 |  3.94 |
|      sculpture      | 50.29 | 55.36 |
|         hood        | 47.21 | 63.08 |
|        sconce       | 39.22 | 46.88 |
|         vase        | 32.18 | 51.73 |
|    traffic light    | 23.06 | 37.96 |
|         tray        |  2.95 |  3.22 |
|        ashcan       | 31.28 | 40.48 |
|         fan         |  52.5 |  74.9 |
|         pier        | 28.05 | 49.98 |
|      crt screen     |  3.32 |  9.18 |
|        plate        | 53.79 | 68.74 |
|       monitor       | 19.41 | 27.09 |
|    bulletin board   | 34.65 |  37.7 |
|        shower       |  0.06 |  0.06 |
|       radiator      | 55.36 | 61.46 |
|        glass        |  7.4  |  7.67 |
|        clock        | 25.82 | 29.42 |
|         flag        | 51.27 |  58.0 |
+---------------------+-------+-------+
2023-11-10 18:38:25,970 - mmseg - INFO - Summary:
2023-11-10 18:38:25,970 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 81.44 | 45.64 | 58.72 |
+-------+-------+-------+
2023-11-10 18:38:26,114 - mmseg - INFO - The previous best checkpoint /data/Next-ViT/segmentation/work_dirs/fpn_512_debi_base_160k/best_mIoU_iter_24000.pth was removed
2023-11-10 18:38:29,772 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_40000.pth.
2023-11-10 18:38:29,773 - mmseg - INFO - Best mIoU is 0.4564 at 40000 iter.
2023-11-10 18:38:29,798 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 18:38:29,799 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8144, mIoU: 0.4564, mAcc: 0.5872, IoU.wall: 0.7467, IoU.building: 0.8224, IoU.sky: 0.9399, IoU.floor: 0.8147, IoU.tree: 0.7288, IoU.ceiling: 0.8178, IoU.road: 0.8170, IoU.bed : 0.8776, IoU.windowpane: 0.5847, IoU.grass: 0.6683, IoU.cabinet: 0.5580, IoU.sidewalk: 0.6418, IoU.person: 0.7840, IoU.earth: 0.3215, IoU.door: 0.4323, IoU.table: 0.5220, IoU.mountain: 0.5789, IoU.plant: 0.5099, IoU.curtain: 0.7195, IoU.chair: 0.5103, IoU.car: 0.8060, IoU.water: 0.5543, IoU.painting: 0.6703, IoU.sofa: 0.6114, IoU.shelf: 0.3865, IoU.house: 0.4926, IoU.sea: 0.5696, IoU.mirror: 0.5898, IoU.rug: 0.6344, IoU.field: 0.3602, IoU.armchair: 0.3694, IoU.seat: 0.5991, IoU.fence: 0.3272, IoU.desk: 0.4553, IoU.rock: 0.4076, IoU.wardrobe: 0.4647, IoU.lamp: 0.5882, IoU.bathtub: 0.7080, IoU.railing: 0.3264, IoU.cushion: 0.5046, IoU.base: 0.3122, IoU.box: 0.2197, IoU.column: 0.4588, IoU.signboard: 0.3203, IoU.chest of drawers: 0.4328, IoU.counter: 0.2401, IoU.sand: 0.2981, IoU.sink: 0.6501, IoU.skyscraper: 0.4724, IoU.fireplace: 0.6605, IoU.refrigerator: 0.7623, IoU.grandstand: 0.4265, IoU.path: 0.2123, IoU.stairs: 0.3169, IoU.runway: 0.6844, IoU.case: 0.5394, IoU.pool table: 0.8828, IoU.pillow: 0.5359, IoU.screen door: 0.5014, IoU.stairway: 0.2896, IoU.river: 0.0998, IoU.bridge: 0.6760, IoU.bookcase: 0.3213, IoU.blind: 0.3750, IoU.coffee table: 0.4719, IoU.toilet: 0.7346, IoU.flower: 0.3167, IoU.book: 0.4378, IoU.hill: 0.0462, IoU.bench: 0.4399, IoU.countertop: 0.5505, IoU.stove: 0.6769, IoU.palm: 0.4313, IoU.kitchen island: 0.2974, IoU.computer: 0.5616, IoU.swivel chair: 0.4429, IoU.boat: 0.4553, IoU.bar: 0.3604, IoU.arcade machine: 0.8421, IoU.hovel: 0.3831, IoU.bus: 0.7832, IoU.towel: 0.5766, IoU.light: 0.4701, IoU.truck: 0.3252, IoU.tower: 0.3449, IoU.chandelier: 0.6271, IoU.awning: 0.2228, IoU.streetlight: 0.2000, IoU.booth: 0.4626, IoU.television receiver: 0.6516, IoU.airplane: 0.5654, IoU.dirt track: 0.0386, IoU.apparel: 0.3603, IoU.pole: 0.1892, IoU.land: 0.0000, IoU.bannister: 0.1472, IoU.escalator: 0.2787, IoU.ottoman: 0.3461, IoU.bottle: 0.3446, IoU.buffet: 0.3847, IoU.poster: 0.3728, IoU.stage: 0.1552, IoU.van: 0.3801, IoU.ship: 0.6811, IoU.fountain: 0.2639, IoU.conveyer belt: 0.7504, IoU.canopy: 0.1382, IoU.washer: 0.7422, IoU.plaything: 0.2304, IoU.swimming pool: 0.5185, IoU.stool: 0.3686, IoU.barrel: 0.4876, IoU.basket: 0.3606, IoU.waterfall: 0.5718, IoU.tent: 0.7413, IoU.bag: 0.0975, IoU.minibike: 0.5357, IoU.cradle: 0.7719, IoU.oven: 0.3416, IoU.ball: 0.3808, IoU.food: 0.5255, IoU.step: 0.0334, IoU.tank: 0.4958, IoU.trade name: 0.1259, IoU.microwave: 0.7338, IoU.pot: 0.3712, IoU.animal: 0.5910, IoU.bicycle: 0.5339, IoU.lake: 0.1429, IoU.dishwasher: 0.5147, IoU.screen: 0.6156, IoU.blanket: 0.0371, IoU.sculpture: 0.5029, IoU.hood: 0.4721, IoU.sconce: 0.3922, IoU.vase: 0.3218, IoU.traffic light: 0.2306, IoU.tray: 0.0295, IoU.ashcan: 0.3128, IoU.fan: 0.5250, IoU.pier: 0.2805, IoU.crt screen: 0.0332, IoU.plate: 0.5379, IoU.monitor: 0.1941, IoU.bulletin board: 0.3465, IoU.shower: 0.0006, IoU.radiator: 0.5536, IoU.glass: 0.0740, IoU.clock: 0.2582, IoU.flag: 0.5127, Acc.wall: 0.8675, Acc.building: 0.9137, Acc.sky: 0.9799, Acc.floor: 0.8873, Acc.tree: 0.8490, Acc.ceiling: 0.9139, Acc.road: 0.9003, Acc.bed : 0.9414, Acc.windowpane: 0.7655, Acc.grass: 0.8166, Acc.cabinet: 0.6762, Acc.sidewalk: 0.7709, Acc.person: 0.9223, Acc.earth: 0.4316, Acc.door: 0.6283, Acc.table: 0.6950, Acc.mountain: 0.7212, Acc.plant: 0.6178, Acc.curtain: 0.8694, Acc.chair: 0.6384, Acc.car: 0.9145, Acc.water: 0.6725, Acc.painting: 0.8505, Acc.sofa: 0.7306, Acc.shelf: 0.5192, Acc.house: 0.6713, Acc.sea: 0.8195, Acc.mirror: 0.6574, Acc.rug: 0.7701, Acc.field: 0.5924, Acc.armchair: 0.6675, Acc.seat: 0.7195, Acc.fence: 0.4252, Acc.desk: 0.7166, Acc.rock: 0.7024, Acc.wardrobe: 0.5944, Acc.lamp: 0.7075, Acc.bathtub: 0.7886, Acc.railing: 0.4745, Acc.cushion: 0.6809, Acc.base: 0.5118, Acc.box: 0.2682, Acc.column: 0.6109, Acc.signboard: 0.4328, Acc.chest of drawers: 0.6131, Acc.counter: 0.2721, Acc.sand: 0.5308, Acc.sink: 0.7288, Acc.skyscraper: 0.5607, Acc.fireplace: 0.9047, Acc.refrigerator: 0.8821, Acc.grandstand: 0.6601, Acc.path: 0.4292, Acc.stairs: 0.3708, Acc.runway: 0.9459, Acc.case: 0.8010, Acc.pool table: 0.9681, Acc.pillow: 0.6406, Acc.screen door: 0.6799, Acc.stairway: 0.3511, Acc.river: 0.2522, Acc.bridge: 0.8716, Acc.bookcase: 0.4462, Acc.blind: 0.4187, Acc.coffee table: 0.8332, Acc.toilet: 0.9108, Acc.flower: 0.5085, Acc.book: 0.6403, Acc.hill: 0.0832, Acc.bench: 0.4932, Acc.countertop: 0.7187, Acc.stove: 0.7737, Acc.palm: 0.7359, Acc.kitchen island: 0.8284, Acc.computer: 0.6280, Acc.swivel chair: 0.5969, Acc.boat: 0.6702, Acc.bar: 0.4387, Acc.arcade machine: 0.9206, Acc.hovel: 0.4643, Acc.bus: 0.9486, Acc.towel: 0.6786, Acc.light: 0.5398, Acc.truck: 0.5492, Acc.tower: 0.5053, Acc.chandelier: 0.7899, Acc.awning: 0.3137, Acc.streetlight: 0.2556, Acc.booth: 0.5032, Acc.television receiver: 0.8202, Acc.airplane: 0.7435, Acc.dirt track: 0.1919, Acc.apparel: 0.5197, Acc.pole: 0.2716, Acc.land: 0.0000, Acc.bannister: 0.1839, Acc.escalator: 0.3298, Acc.ottoman: 0.4603, Acc.bottle: 0.5886, Acc.buffet: 0.4424, Acc.poster: 0.5129, Acc.stage: 0.1983, Acc.van: 0.5023, Acc.ship: 0.8795, Acc.fountain: 0.2696, Acc.conveyer belt: 0.8574, Acc.canopy: 0.2162, Acc.washer: 0.7623, Acc.plaything: 0.4138, Acc.swimming pool: 0.7515, Acc.stool: 0.4617, Acc.barrel: 0.6498, Acc.basket: 0.4730, Acc.waterfall: 0.8472, Acc.tent: 0.9822, Acc.bag: 0.1082, Acc.minibike: 0.6069, Acc.cradle: 0.9758, Acc.oven: 0.4305, Acc.ball: 0.4487, Acc.food: 0.6465, Acc.step: 0.0374, Acc.tank: 0.5503, Acc.trade name: 0.1325, Acc.microwave: 0.9298, Acc.pot: 0.4512, Acc.animal: 0.6726, Acc.bicycle: 0.7018, Acc.lake: 0.1457, Acc.dishwasher: 0.7713, Acc.screen: 0.9086, Acc.blanket: 0.0394, Acc.sculpture: 0.5536, Acc.hood: 0.6308, Acc.sconce: 0.4688, Acc.vase: 0.5173, Acc.traffic light: 0.3796, Acc.tray: 0.0322, Acc.ashcan: 0.4048, Acc.fan: 0.7490, Acc.pier: 0.4998, Acc.crt screen: 0.0918, Acc.plate: 0.6874, Acc.monitor: 0.2709, Acc.bulletin board: 0.3770, Acc.shower: 0.0006, Acc.radiator: 0.6146, Acc.glass: 0.0767, Acc.clock: 0.2942, Acc.flag: 0.5800
2023-11-10 18:39:02,780 - mmseg - INFO - Iter [40050/160000]	lr: 8.532e-05, eta: 23:25:45, time: 3.074, data_time: 2.425, memory: 23129, decode.loss_ce: 0.3975, decode.acc_seg: 85.4689, loss: 0.3975
2023-11-10 18:39:38,014 - mmseg - INFO - Iter [40100/160000]	lr: 8.529e-05, eta: 23:25:10, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3618, decode.acc_seg: 86.0473, loss: 0.3618
2023-11-10 18:40:12,967 - mmseg - INFO - Iter [40150/160000]	lr: 8.525e-05, eta: 23:24:34, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3423, decode.acc_seg: 87.0518, loss: 0.3423
2023-11-10 18:40:46,772 - mmseg - INFO - Iter [40200/160000]	lr: 8.522e-05, eta: 23:23:55, time: 0.676, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3633, decode.acc_seg: 86.2955, loss: 0.3633
2023-11-10 18:41:19,550 - mmseg - INFO - Iter [40250/160000]	lr: 8.518e-05, eta: 23:23:13, time: 0.657, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3772, decode.acc_seg: 85.2007, loss: 0.3772
2023-11-10 18:41:51,777 - mmseg - INFO - Iter [40300/160000]	lr: 8.515e-05, eta: 23:22:29, time: 0.645, data_time: 0.009, memory: 23129, decode.loss_ce: 0.4008, decode.acc_seg: 85.1595, loss: 0.4008
2023-11-10 18:42:24,713 - mmseg - INFO - Iter [40350/160000]	lr: 8.511e-05, eta: 23:21:47, time: 0.657, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3531, decode.acc_seg: 86.2521, loss: 0.3531
2023-11-10 18:42:58,769 - mmseg - INFO - Iter [40400/160000]	lr: 8.508e-05, eta: 23:21:09, time: 0.681, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3550, decode.acc_seg: 86.3733, loss: 0.3550
2023-11-10 18:43:33,726 - mmseg - INFO - Iter [40450/160000]	lr: 8.504e-05, eta: 23:20:33, time: 0.700, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3987, decode.acc_seg: 84.9578, loss: 0.3987
2023-11-10 18:44:08,181 - mmseg - INFO - Iter [40500/160000]	lr: 8.501e-05, eta: 23:19:56, time: 0.688, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3566, decode.acc_seg: 85.7529, loss: 0.3566
2023-11-10 18:44:43,382 - mmseg - INFO - Iter [40550/160000]	lr: 8.497e-05, eta: 23:19:21, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3601, decode.acc_seg: 86.4753, loss: 0.3601
2023-11-10 18:45:18,940 - mmseg - INFO - Iter [40600/160000]	lr: 8.494e-05, eta: 23:18:47, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3798, decode.acc_seg: 85.6316, loss: 0.3798
2023-11-10 18:45:54,034 - mmseg - INFO - Iter [40650/160000]	lr: 8.490e-05, eta: 23:18:11, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3713, decode.acc_seg: 85.2944, loss: 0.3713
2023-11-10 18:46:29,383 - mmseg - INFO - Iter [40700/160000]	lr: 8.487e-05, eta: 23:17:37, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3477, decode.acc_seg: 86.4537, loss: 0.3477
2023-11-10 18:47:03,909 - mmseg - INFO - Iter [40750/160000]	lr: 8.483e-05, eta: 23:17:00, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3858, decode.acc_seg: 84.9727, loss: 0.3858
2023-11-10 18:47:37,812 - mmseg - INFO - Iter [40800/160000]	lr: 8.480e-05, eta: 23:16:21, time: 0.677, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3701, decode.acc_seg: 85.4368, loss: 0.3701
2023-11-10 18:48:12,370 - mmseg - INFO - Iter [40850/160000]	lr: 8.476e-05, eta: 23:15:44, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3474, decode.acc_seg: 85.8368, loss: 0.3474
2023-11-10 18:48:47,247 - mmseg - INFO - Iter [40900/160000]	lr: 8.473e-05, eta: 23:15:08, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3646, decode.acc_seg: 85.8288, loss: 0.3646
2023-11-10 18:49:21,903 - mmseg - INFO - Iter [40950/160000]	lr: 8.469e-05, eta: 23:14:32, time: 0.693, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3603, decode.acc_seg: 86.1788, loss: 0.3603
2023-11-10 18:49:53,895 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 18:49:53,895 - mmseg - INFO - Iter [41000/160000]	lr: 8.466e-05, eta: 23:13:48, time: 0.641, data_time: 0.010, memory: 23129, decode.loss_ce: 0.4146, decode.acc_seg: 84.8765, loss: 0.4146
2023-11-10 18:50:27,518 - mmseg - INFO - Iter [41050/160000]	lr: 8.462e-05, eta: 23:13:08, time: 0.672, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3949, decode.acc_seg: 84.9874, loss: 0.3949
2023-11-10 18:50:58,775 - mmseg - INFO - Iter [41100/160000]	lr: 8.458e-05, eta: 23:12:22, time: 0.626, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3882, decode.acc_seg: 84.8871, loss: 0.3882
2023-11-10 18:51:32,781 - mmseg - INFO - Iter [41150/160000]	lr: 8.455e-05, eta: 23:11:43, time: 0.680, data_time: 0.008, memory: 23129, decode.loss_ce: 0.3873, decode.acc_seg: 84.5621, loss: 0.3873
2023-11-10 18:52:04,671 - mmseg - INFO - Iter [41200/160000]	lr: 8.451e-05, eta: 23:10:59, time: 0.638, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3646, decode.acc_seg: 86.1000, loss: 0.3646
2023-11-10 18:52:37,394 - mmseg - INFO - Iter [41250/160000]	lr: 8.448e-05, eta: 23:10:17, time: 0.653, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3621, decode.acc_seg: 86.4533, loss: 0.3621
2023-11-10 18:53:12,875 - mmseg - INFO - Iter [41300/160000]	lr: 8.444e-05, eta: 23:09:43, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3508, decode.acc_seg: 86.1680, loss: 0.3508
2023-11-10 18:53:47,125 - mmseg - INFO - Iter [41350/160000]	lr: 8.441e-05, eta: 23:09:05, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3640, decode.acc_seg: 85.4984, loss: 0.3640
2023-11-10 18:54:18,864 - mmseg - INFO - Iter [41400/160000]	lr: 8.437e-05, eta: 23:08:20, time: 0.635, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3670, decode.acc_seg: 85.9644, loss: 0.3670
2023-11-10 18:54:50,230 - mmseg - INFO - Iter [41450/160000]	lr: 8.434e-05, eta: 23:07:35, time: 0.627, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3432, decode.acc_seg: 86.5426, loss: 0.3432
2023-11-10 18:55:25,188 - mmseg - INFO - Iter [41500/160000]	lr: 8.430e-05, eta: 23:06:59, time: 0.698, data_time: 0.008, memory: 23129, decode.loss_ce: 0.3668, decode.acc_seg: 85.7837, loss: 0.3668
2023-11-10 18:56:00,184 - mmseg - INFO - Iter [41550/160000]	lr: 8.426e-05, eta: 23:06:23, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3535, decode.acc_seg: 86.3791, loss: 0.3535
2023-11-10 18:56:35,067 - mmseg - INFO - Iter [41600/160000]	lr: 8.423e-05, eta: 23:05:48, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3675, decode.acc_seg: 86.2725, loss: 0.3675
2023-11-10 18:57:10,167 - mmseg - INFO - Iter [41650/160000]	lr: 8.419e-05, eta: 23:05:12, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3497, decode.acc_seg: 86.5137, loss: 0.3497
2023-11-10 18:57:45,254 - mmseg - INFO - Iter [41700/160000]	lr: 8.416e-05, eta: 23:04:37, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3593, decode.acc_seg: 86.3222, loss: 0.3593
2023-11-10 18:58:20,530 - mmseg - INFO - Iter [41750/160000]	lr: 8.412e-05, eta: 23:04:03, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3481, decode.acc_seg: 86.3374, loss: 0.3481
2023-11-10 18:58:53,332 - mmseg - INFO - Iter [41800/160000]	lr: 8.408e-05, eta: 23:03:21, time: 0.657, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3535, decode.acc_seg: 86.1477, loss: 0.3535
2023-11-10 18:59:27,278 - mmseg - INFO - Iter [41850/160000]	lr: 8.405e-05, eta: 23:02:43, time: 0.679, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3678, decode.acc_seg: 85.9518, loss: 0.3678
2023-11-10 18:59:58,927 - mmseg - INFO - Iter [41900/160000]	lr: 8.401e-05, eta: 23:01:58, time: 0.633, data_time: 0.008, memory: 23129, decode.loss_ce: 0.3624, decode.acc_seg: 86.3711, loss: 0.3624
2023-11-10 19:00:30,174 - mmseg - INFO - Iter [41950/160000]	lr: 8.398e-05, eta: 23:01:12, time: 0.625, data_time: 0.008, memory: 23129, decode.loss_ce: 0.3621, decode.acc_seg: 85.9972, loss: 0.3621
2023-11-10 19:01:03,994 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 19:01:03,995 - mmseg - INFO - Iter [42000/160000]	lr: 8.394e-05, eta: 23:00:33, time: 0.675, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3669, decode.acc_seg: 86.1266, loss: 0.3669
2023-11-10 19:01:38,027 - mmseg - INFO - Iter [42050/160000]	lr: 8.390e-05, eta: 22:59:55, time: 0.681, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3753, decode.acc_seg: 85.9129, loss: 0.3753
2023-11-10 19:02:12,397 - mmseg - INFO - Iter [42100/160000]	lr: 8.387e-05, eta: 22:59:18, time: 0.689, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3750, decode.acc_seg: 85.4569, loss: 0.3750
2023-11-10 19:02:43,917 - mmseg - INFO - Iter [42150/160000]	lr: 8.383e-05, eta: 22:58:33, time: 0.630, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3669, decode.acc_seg: 85.8678, loss: 0.3669
2023-11-10 19:03:18,245 - mmseg - INFO - Iter [42200/160000]	lr: 8.380e-05, eta: 22:57:56, time: 0.685, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3474, decode.acc_seg: 86.9231, loss: 0.3474
2023-11-10 19:03:51,461 - mmseg - INFO - Iter [42250/160000]	lr: 8.376e-05, eta: 22:57:15, time: 0.665, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3552, decode.acc_seg: 86.3876, loss: 0.3552
2023-11-10 19:04:23,793 - mmseg - INFO - Iter [42300/160000]	lr: 8.372e-05, eta: 22:56:32, time: 0.646, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3720, decode.acc_seg: 85.2572, loss: 0.3720
2023-11-10 19:04:57,550 - mmseg - INFO - Iter [42350/160000]	lr: 8.369e-05, eta: 22:55:54, time: 0.675, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3508, decode.acc_seg: 86.4893, loss: 0.3508
2023-11-10 19:05:34,177 - mmseg - INFO - Iter [42400/160000]	lr: 8.365e-05, eta: 22:55:23, time: 0.733, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3578, decode.acc_seg: 86.2165, loss: 0.3578
2023-11-10 19:06:10,549 - mmseg - INFO - Iter [42450/160000]	lr: 8.362e-05, eta: 22:54:51, time: 0.727, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3724, decode.acc_seg: 85.8381, loss: 0.3724
2023-11-10 19:06:46,954 - mmseg - INFO - Iter [42500/160000]	lr: 8.358e-05, eta: 22:54:20, time: 0.729, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3513, decode.acc_seg: 86.4827, loss: 0.3513
2023-11-10 19:07:24,206 - mmseg - INFO - Iter [42550/160000]	lr: 8.354e-05, eta: 22:53:51, time: 0.745, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3553, decode.acc_seg: 86.0597, loss: 0.3553
2023-11-10 19:07:59,521 - mmseg - INFO - Iter [42600/160000]	lr: 8.351e-05, eta: 22:53:16, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3519, decode.acc_seg: 86.4840, loss: 0.3519
2023-11-10 19:08:34,343 - mmseg - INFO - Iter [42650/160000]	lr: 8.347e-05, eta: 22:52:40, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3485, decode.acc_seg: 86.4517, loss: 0.3485
2023-11-10 19:09:09,201 - mmseg - INFO - Iter [42700/160000]	lr: 8.343e-05, eta: 22:52:05, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3373, decode.acc_seg: 86.7218, loss: 0.3373
2023-11-10 19:09:44,243 - mmseg - INFO - Iter [42750/160000]	lr: 8.340e-05, eta: 22:51:29, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3257, decode.acc_seg: 86.9153, loss: 0.3257
2023-11-10 19:10:19,778 - mmseg - INFO - Iter [42800/160000]	lr: 8.336e-05, eta: 22:50:56, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3468, decode.acc_seg: 86.7343, loss: 0.3468
2023-11-10 19:10:55,462 - mmseg - INFO - Iter [42850/160000]	lr: 8.332e-05, eta: 22:50:22, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3405, decode.acc_seg: 86.8329, loss: 0.3405
2023-11-10 19:11:30,185 - mmseg - INFO - Iter [42900/160000]	lr: 8.329e-05, eta: 22:49:46, time: 0.696, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3593, decode.acc_seg: 85.8715, loss: 0.3593
2023-11-10 19:12:04,268 - mmseg - INFO - Iter [42950/160000]	lr: 8.325e-05, eta: 22:49:08, time: 0.681, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3546, decode.acc_seg: 86.5296, loss: 0.3546
2023-11-10 19:12:39,074 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 19:12:39,074 - mmseg - INFO - Iter [43000/160000]	lr: 8.321e-05, eta: 22:48:32, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3556, decode.acc_seg: 86.3807, loss: 0.3556
2023-11-10 19:13:14,526 - mmseg - INFO - Iter [43050/160000]	lr: 8.318e-05, eta: 22:47:58, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3452, decode.acc_seg: 86.3426, loss: 0.3452
2023-11-10 19:13:49,556 - mmseg - INFO - Iter [43100/160000]	lr: 8.314e-05, eta: 22:47:23, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3877, decode.acc_seg: 85.4384, loss: 0.3877
2023-11-10 19:14:22,268 - mmseg - INFO - Iter [43150/160000]	lr: 8.310e-05, eta: 22:46:41, time: 0.653, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3582, decode.acc_seg: 86.3203, loss: 0.3582
2023-11-10 19:14:57,393 - mmseg - INFO - Iter [43200/160000]	lr: 8.307e-05, eta: 22:46:06, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3687, decode.acc_seg: 85.7654, loss: 0.3687
2023-11-10 19:15:32,899 - mmseg - INFO - Iter [43250/160000]	lr: 8.303e-05, eta: 22:45:32, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3663, decode.acc_seg: 86.3477, loss: 0.3663
2023-11-10 19:16:04,346 - mmseg - INFO - Iter [43300/160000]	lr: 8.299e-05, eta: 22:44:48, time: 0.630, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3647, decode.acc_seg: 85.7989, loss: 0.3647
2023-11-10 19:16:36,508 - mmseg - INFO - Iter [43350/160000]	lr: 8.296e-05, eta: 22:44:05, time: 0.643, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3567, decode.acc_seg: 86.7792, loss: 0.3567
2023-11-10 19:17:08,217 - mmseg - INFO - Iter [43400/160000]	lr: 8.292e-05, eta: 22:43:21, time: 0.634, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3591, decode.acc_seg: 86.3394, loss: 0.3591
2023-11-10 19:17:39,745 - mmseg - INFO - Iter [43450/160000]	lr: 8.288e-05, eta: 22:42:36, time: 0.631, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3398, decode.acc_seg: 86.4101, loss: 0.3398
2023-11-10 19:18:11,921 - mmseg - INFO - Iter [43500/160000]	lr: 8.284e-05, eta: 22:41:53, time: 0.643, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3439, decode.acc_seg: 86.5809, loss: 0.3439
2023-11-10 19:18:45,808 - mmseg - INFO - Iter [43550/160000]	lr: 8.281e-05, eta: 22:41:15, time: 0.678, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3480, decode.acc_seg: 86.6214, loss: 0.3480
2023-11-10 19:19:21,061 - mmseg - INFO - Iter [43600/160000]	lr: 8.277e-05, eta: 22:40:40, time: 0.704, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3686, decode.acc_seg: 85.9387, loss: 0.3686
2023-11-10 19:19:56,076 - mmseg - INFO - Iter [43650/160000]	lr: 8.273e-05, eta: 22:40:05, time: 0.702, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3580, decode.acc_seg: 85.9450, loss: 0.3580
2023-11-10 19:20:30,754 - mmseg - INFO - Iter [43700/160000]	lr: 8.270e-05, eta: 22:39:29, time: 0.693, data_time: 0.008, memory: 23129, decode.loss_ce: 0.3487, decode.acc_seg: 86.3896, loss: 0.3487
2023-11-10 19:21:06,017 - mmseg - INFO - Iter [43750/160000]	lr: 8.266e-05, eta: 22:38:54, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3679, decode.acc_seg: 85.7701, loss: 0.3679
2023-11-10 19:21:38,244 - mmseg - INFO - Iter [43800/160000]	lr: 8.262e-05, eta: 22:38:12, time: 0.645, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3574, decode.acc_seg: 85.9319, loss: 0.3574
2023-11-10 19:22:10,216 - mmseg - INFO - Iter [43850/160000]	lr: 8.258e-05, eta: 22:37:29, time: 0.639, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3553, decode.acc_seg: 86.1370, loss: 0.3553
2023-11-10 19:22:45,606 - mmseg - INFO - Iter [43900/160000]	lr: 8.255e-05, eta: 22:36:54, time: 0.707, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3854, decode.acc_seg: 85.3041, loss: 0.3854
2023-11-10 19:23:21,293 - mmseg - INFO - Iter [43950/160000]	lr: 8.251e-05, eta: 22:36:21, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3562, decode.acc_seg: 85.7586, loss: 0.3562
2023-11-10 19:23:56,966 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 19:23:56,966 - mmseg - INFO - Iter [44000/160000]	lr: 8.247e-05, eta: 22:35:47, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3817, decode.acc_seg: 85.5897, loss: 0.3817
2023-11-10 19:24:30,804 - mmseg - INFO - Iter [44050/160000]	lr: 8.244e-05, eta: 22:35:09, time: 0.678, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3441, decode.acc_seg: 86.4513, loss: 0.3441
2023-11-10 19:25:03,828 - mmseg - INFO - Iter [44100/160000]	lr: 8.240e-05, eta: 22:34:29, time: 0.659, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3472, decode.acc_seg: 86.4181, loss: 0.3472
2023-11-10 19:25:39,670 - mmseg - INFO - Iter [44150/160000]	lr: 8.236e-05, eta: 22:33:56, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3674, decode.acc_seg: 85.8329, loss: 0.3674
2023-11-10 19:26:14,958 - mmseg - INFO - Iter [44200/160000]	lr: 8.232e-05, eta: 22:33:21, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3685, decode.acc_seg: 85.4502, loss: 0.3685
2023-11-10 19:26:48,080 - mmseg - INFO - Iter [44250/160000]	lr: 8.229e-05, eta: 22:32:41, time: 0.664, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3540, decode.acc_seg: 86.2141, loss: 0.3540
2023-11-10 19:27:19,762 - mmseg - INFO - Iter [44300/160000]	lr: 8.225e-05, eta: 22:31:57, time: 0.634, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3457, decode.acc_seg: 85.8798, loss: 0.3457
2023-11-10 19:27:51,795 - mmseg - INFO - Iter [44350/160000]	lr: 8.221e-05, eta: 22:31:14, time: 0.640, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3744, decode.acc_seg: 85.4967, loss: 0.3744
2023-11-10 19:28:26,300 - mmseg - INFO - Iter [44400/160000]	lr: 8.217e-05, eta: 22:30:38, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3616, decode.acc_seg: 85.9670, loss: 0.3616
2023-11-10 19:29:01,501 - mmseg - INFO - Iter [44450/160000]	lr: 8.214e-05, eta: 22:30:03, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3566, decode.acc_seg: 86.1510, loss: 0.3566
2023-11-10 19:29:36,369 - mmseg - INFO - Iter [44500/160000]	lr: 8.210e-05, eta: 22:29:28, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3562, decode.acc_seg: 86.0844, loss: 0.3562
2023-11-10 19:30:11,538 - mmseg - INFO - Iter [44550/160000]	lr: 8.206e-05, eta: 22:28:53, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3614, decode.acc_seg: 85.4383, loss: 0.3614
2023-11-10 19:30:46,837 - mmseg - INFO - Iter [44600/160000]	lr: 8.202e-05, eta: 22:28:18, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3784, decode.acc_seg: 85.6578, loss: 0.3784
2023-11-10 19:31:21,470 - mmseg - INFO - Iter [44650/160000]	lr: 8.199e-05, eta: 22:27:42, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3763, decode.acc_seg: 85.5962, loss: 0.3763
2023-11-10 19:31:53,646 - mmseg - INFO - Iter [44700/160000]	lr: 8.195e-05, eta: 22:27:00, time: 0.644, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3501, decode.acc_seg: 86.3285, loss: 0.3501
2023-11-10 19:32:28,069 - mmseg - INFO - Iter [44750/160000]	lr: 8.191e-05, eta: 22:26:23, time: 0.689, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3470, decode.acc_seg: 86.2510, loss: 0.3470
2023-11-10 19:33:01,120 - mmseg - INFO - Iter [44800/160000]	lr: 8.187e-05, eta: 22:25:43, time: 0.660, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3607, decode.acc_seg: 85.7167, loss: 0.3607
2023-11-10 19:33:33,412 - mmseg - INFO - Iter [44850/160000]	lr: 8.183e-05, eta: 22:25:01, time: 0.647, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3593, decode.acc_seg: 85.9757, loss: 0.3593
2023-11-10 19:34:08,456 - mmseg - INFO - Iter [44900/160000]	lr: 8.180e-05, eta: 22:24:26, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3521, decode.acc_seg: 86.3600, loss: 0.3521
2023-11-10 19:34:45,084 - mmseg - INFO - Iter [44950/160000]	lr: 8.176e-05, eta: 22:23:55, time: 0.733, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3347, decode.acc_seg: 86.5099, loss: 0.3347
2023-11-10 19:35:21,679 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 19:35:21,679 - mmseg - INFO - Iter [45000/160000]	lr: 8.172e-05, eta: 22:23:24, time: 0.732, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3451, decode.acc_seg: 86.6300, loss: 0.3451
2023-11-10 19:35:57,405 - mmseg - INFO - Iter [45050/160000]	lr: 8.168e-05, eta: 22:22:50, time: 0.715, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3685, decode.acc_seg: 85.7171, loss: 0.3685
2023-11-10 19:36:32,169 - mmseg - INFO - Iter [45100/160000]	lr: 8.164e-05, eta: 22:22:15, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3617, decode.acc_seg: 85.8642, loss: 0.3617
2023-11-10 19:37:07,532 - mmseg - INFO - Iter [45150/160000]	lr: 8.161e-05, eta: 22:21:40, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3548, decode.acc_seg: 86.3357, loss: 0.3548
2023-11-10 19:37:43,133 - mmseg - INFO - Iter [45200/160000]	lr: 8.157e-05, eta: 22:21:07, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3573, decode.acc_seg: 86.1713, loss: 0.3573
2023-11-10 19:38:18,115 - mmseg - INFO - Iter [45250/160000]	lr: 8.153e-05, eta: 22:20:32, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3570, decode.acc_seg: 86.4141, loss: 0.3570
2023-11-10 19:38:53,201 - mmseg - INFO - Iter [45300/160000]	lr: 8.149e-05, eta: 22:19:57, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3694, decode.acc_seg: 85.9952, loss: 0.3694
2023-11-10 19:39:27,452 - mmseg - INFO - Iter [45350/160000]	lr: 8.145e-05, eta: 22:19:20, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3444, decode.acc_seg: 86.6410, loss: 0.3444
2023-11-10 19:40:01,473 - mmseg - INFO - Iter [45400/160000]	lr: 8.142e-05, eta: 22:18:42, time: 0.679, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3656, decode.acc_seg: 85.8362, loss: 0.3656
2023-11-10 19:40:35,687 - mmseg - INFO - Iter [45450/160000]	lr: 8.138e-05, eta: 22:18:05, time: 0.684, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3584, decode.acc_seg: 86.3422, loss: 0.3584
2023-11-10 19:41:13,152 - mmseg - INFO - Iter [45500/160000]	lr: 8.134e-05, eta: 22:17:36, time: 0.749, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3726, decode.acc_seg: 85.9524, loss: 0.3726
2023-11-10 19:41:50,442 - mmseg - INFO - Iter [45550/160000]	lr: 8.130e-05, eta: 22:17:06, time: 0.746, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3582, decode.acc_seg: 86.1972, loss: 0.3582
2023-11-10 19:42:27,808 - mmseg - INFO - Iter [45600/160000]	lr: 8.126e-05, eta: 22:16:37, time: 0.747, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3581, decode.acc_seg: 85.8849, loss: 0.3581
2023-11-10 19:43:04,269 - mmseg - INFO - Iter [45650/160000]	lr: 8.122e-05, eta: 22:16:06, time: 0.729, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3483, decode.acc_seg: 86.5393, loss: 0.3483
2023-11-10 19:43:36,090 - mmseg - INFO - Iter [45700/160000]	lr: 8.119e-05, eta: 22:15:23, time: 0.637, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3756, decode.acc_seg: 85.7039, loss: 0.3756
2023-11-10 19:44:08,988 - mmseg - INFO - Iter [45750/160000]	lr: 8.115e-05, eta: 22:14:42, time: 0.657, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3563, decode.acc_seg: 86.1968, loss: 0.3563
2023-11-10 19:44:42,905 - mmseg - INFO - Iter [45800/160000]	lr: 8.111e-05, eta: 22:14:04, time: 0.679, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3457, decode.acc_seg: 86.7329, loss: 0.3457
2023-11-10 19:45:14,776 - mmseg - INFO - Iter [45850/160000]	lr: 8.107e-05, eta: 22:13:21, time: 0.637, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3477, decode.acc_seg: 86.6005, loss: 0.3477
2023-11-10 19:45:47,962 - mmseg - INFO - Iter [45900/160000]	lr: 8.103e-05, eta: 22:12:42, time: 0.664, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3759, decode.acc_seg: 85.4297, loss: 0.3759
2023-11-10 19:46:22,250 - mmseg - INFO - Iter [45950/160000]	lr: 8.099e-05, eta: 22:12:05, time: 0.687, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3667, decode.acc_seg: 86.0299, loss: 0.3667
2023-11-10 19:46:57,878 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 19:46:57,878 - mmseg - INFO - Iter [46000/160000]	lr: 8.096e-05, eta: 22:11:31, time: 0.712, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3363, decode.acc_seg: 86.6594, loss: 0.3363
2023-11-10 19:47:33,461 - mmseg - INFO - Iter [46050/160000]	lr: 8.092e-05, eta: 22:10:57, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3557, decode.acc_seg: 86.0811, loss: 0.3557
2023-11-10 19:48:08,914 - mmseg - INFO - Iter [46100/160000]	lr: 8.088e-05, eta: 22:10:23, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3294, decode.acc_seg: 87.1122, loss: 0.3294
2023-11-10 19:48:44,165 - mmseg - INFO - Iter [46150/160000]	lr: 8.084e-05, eta: 22:09:49, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3301, decode.acc_seg: 86.8456, loss: 0.3301
2023-11-10 19:49:18,670 - mmseg - INFO - Iter [46200/160000]	lr: 8.080e-05, eta: 22:09:13, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3146, decode.acc_seg: 87.4157, loss: 0.3146
2023-11-10 19:49:53,470 - mmseg - INFO - Iter [46250/160000]	lr: 8.076e-05, eta: 22:08:37, time: 0.697, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3269, decode.acc_seg: 87.2376, loss: 0.3269
2023-11-10 19:50:25,796 - mmseg - INFO - Iter [46300/160000]	lr: 8.072e-05, eta: 22:07:55, time: 0.646, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3392, decode.acc_seg: 86.7061, loss: 0.3392
2023-11-10 19:50:58,859 - mmseg - INFO - Iter [46350/160000]	lr: 8.068e-05, eta: 22:07:15, time: 0.662, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3422, decode.acc_seg: 86.3726, loss: 0.3422
2023-11-10 19:51:31,604 - mmseg - INFO - Iter [46400/160000]	lr: 8.065e-05, eta: 22:06:35, time: 0.654, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3621, decode.acc_seg: 85.9440, loss: 0.3621
2023-11-10 19:52:05,539 - mmseg - INFO - Iter [46450/160000]	lr: 8.061e-05, eta: 22:05:57, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3462, decode.acc_seg: 86.9408, loss: 0.3462
2023-11-10 19:52:39,059 - mmseg - INFO - Iter [46500/160000]	lr: 8.057e-05, eta: 22:05:18, time: 0.671, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3273, decode.acc_seg: 87.3445, loss: 0.3273
2023-11-10 19:53:11,736 - mmseg - INFO - Iter [46550/160000]	lr: 8.053e-05, eta: 22:04:37, time: 0.653, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3422, decode.acc_seg: 86.5934, loss: 0.3422
2023-11-10 19:53:46,860 - mmseg - INFO - Iter [46600/160000]	lr: 8.049e-05, eta: 22:04:03, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3519, decode.acc_seg: 86.7353, loss: 0.3519
2023-11-10 19:54:20,370 - mmseg - INFO - Iter [46650/160000]	lr: 8.045e-05, eta: 22:03:24, time: 0.671, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3398, decode.acc_seg: 86.9470, loss: 0.3398
2023-11-10 19:54:53,250 - mmseg - INFO - Iter [46700/160000]	lr: 8.041e-05, eta: 22:02:44, time: 0.658, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3348, decode.acc_seg: 87.0834, loss: 0.3348
2023-11-10 19:55:25,980 - mmseg - INFO - Iter [46750/160000]	lr: 8.037e-05, eta: 22:02:03, time: 0.654, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3265, decode.acc_seg: 87.2640, loss: 0.3265
2023-11-10 19:55:59,193 - mmseg - INFO - Iter [46800/160000]	lr: 8.033e-05, eta: 22:01:24, time: 0.665, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3421, decode.acc_seg: 86.5346, loss: 0.3421
2023-11-10 19:56:33,997 - mmseg - INFO - Iter [46850/160000]	lr: 8.030e-05, eta: 22:00:48, time: 0.695, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3329, decode.acc_seg: 86.6737, loss: 0.3329
2023-11-10 19:57:08,691 - mmseg - INFO - Iter [46900/160000]	lr: 8.026e-05, eta: 22:00:13, time: 0.695, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3301, decode.acc_seg: 86.9322, loss: 0.3301
2023-11-10 19:57:41,944 - mmseg - INFO - Iter [46950/160000]	lr: 8.022e-05, eta: 21:59:33, time: 0.665, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3270, decode.acc_seg: 87.3260, loss: 0.3270
2023-11-10 19:58:16,734 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 19:58:16,735 - mmseg - INFO - Iter [47000/160000]	lr: 8.018e-05, eta: 21:58:58, time: 0.696, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3582, decode.acc_seg: 86.2308, loss: 0.3582
2023-11-10 19:58:52,134 - mmseg - INFO - Iter [47050/160000]	lr: 8.014e-05, eta: 21:58:23, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3228, decode.acc_seg: 87.2632, loss: 0.3228
2023-11-10 19:59:27,435 - mmseg - INFO - Iter [47100/160000]	lr: 8.010e-05, eta: 21:57:49, time: 0.706, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3331, decode.acc_seg: 86.5240, loss: 0.3331
2023-11-10 20:00:02,470 - mmseg - INFO - Iter [47150/160000]	lr: 8.006e-05, eta: 21:57:14, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3334, decode.acc_seg: 86.8852, loss: 0.3334
2023-11-10 20:00:37,702 - mmseg - INFO - Iter [47200/160000]	lr: 8.002e-05, eta: 21:56:40, time: 0.705, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3327, decode.acc_seg: 86.8851, loss: 0.3327
2023-11-10 20:01:12,452 - mmseg - INFO - Iter [47250/160000]	lr: 7.998e-05, eta: 21:56:04, time: 0.695, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3284, decode.acc_seg: 87.2051, loss: 0.3284
2023-11-10 20:01:45,785 - mmseg - INFO - Iter [47300/160000]	lr: 7.994e-05, eta: 21:55:25, time: 0.667, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3430, decode.acc_seg: 86.7801, loss: 0.3430
2023-11-10 20:02:20,746 - mmseg - INFO - Iter [47350/160000]	lr: 7.990e-05, eta: 21:54:50, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3391, decode.acc_seg: 86.9414, loss: 0.3391
2023-11-10 20:02:55,601 - mmseg - INFO - Iter [47400/160000]	lr: 7.986e-05, eta: 21:54:14, time: 0.697, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3275, decode.acc_seg: 87.1336, loss: 0.3275
2023-11-10 20:03:28,359 - mmseg - INFO - Iter [47450/160000]	lr: 7.983e-05, eta: 21:53:34, time: 0.656, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3327, decode.acc_seg: 87.0076, loss: 0.3327
2023-11-10 20:04:01,051 - mmseg - INFO - Iter [47500/160000]	lr: 7.979e-05, eta: 21:52:54, time: 0.654, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3450, decode.acc_seg: 86.7023, loss: 0.3450
2023-11-10 20:04:35,944 - mmseg - INFO - Iter [47550/160000]	lr: 7.975e-05, eta: 21:52:18, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3509, decode.acc_seg: 86.7358, loss: 0.3509
2023-11-10 20:05:09,176 - mmseg - INFO - Iter [47600/160000]	lr: 7.971e-05, eta: 21:51:39, time: 0.666, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3212, decode.acc_seg: 87.5186, loss: 0.3212
2023-11-10 20:05:43,418 - mmseg - INFO - Iter [47650/160000]	lr: 7.967e-05, eta: 21:51:02, time: 0.684, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3411, decode.acc_seg: 86.4949, loss: 0.3411
2023-11-10 20:06:19,148 - mmseg - INFO - Iter [47700/160000]	lr: 7.963e-05, eta: 21:50:29, time: 0.715, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3425, decode.acc_seg: 86.7257, loss: 0.3425
2023-11-10 20:06:54,712 - mmseg - INFO - Iter [47750/160000]	lr: 7.959e-05, eta: 21:49:55, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3402, decode.acc_seg: 86.8846, loss: 0.3402
2023-11-10 20:07:28,131 - mmseg - INFO - Iter [47800/160000]	lr: 7.955e-05, eta: 21:49:16, time: 0.668, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3437, decode.acc_seg: 86.5181, loss: 0.3437
2023-11-10 20:08:03,623 - mmseg - INFO - Iter [47850/160000]	lr: 7.951e-05, eta: 21:48:43, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3321, decode.acc_seg: 87.1587, loss: 0.3321
2023-11-10 20:08:38,893 - mmseg - INFO - Iter [47900/160000]	lr: 7.947e-05, eta: 21:48:08, time: 0.706, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3380, decode.acc_seg: 86.8908, loss: 0.3380
2023-11-10 20:09:14,179 - mmseg - INFO - Iter [47950/160000]	lr: 7.943e-05, eta: 21:47:34, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3387, decode.acc_seg: 86.9488, loss: 0.3387
2023-11-10 20:09:49,533 - mmseg - INFO - Saving checkpoint at 48000 iterations
2023-11-10 20:09:54,118 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 20:09:54,119 - mmseg - INFO - Iter [48000/160000]	lr: 7.939e-05, eta: 21:47:10, time: 0.800, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3370, decode.acc_seg: 87.0718, loss: 0.3370
2023-11-10 20:11:26,584 - mmseg - INFO - per class results:
2023-11-10 20:11:26,597 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 76.17 | 86.75 |
|       building      | 82.44 | 92.88 |
|         sky         | 94.02 | 97.27 |
|        floor        |  80.5 | 89.86 |
|         tree        |  73.8 | 86.54 |
|       ceiling       | 83.41 | 90.03 |
|         road        | 81.95 |  91.7 |
|         bed         | 88.07 | 95.37 |
|      windowpane     | 59.06 | 79.26 |
|        grass        |  66.6 | 84.59 |
|       cabinet       | 59.41 | 78.51 |
|       sidewalk      | 62.36 | 77.05 |
|        person       | 78.29 |  92.8 |
|        earth        |  31.7 | 47.58 |
|         door        | 46.98 | 62.31 |
|        table        | 54.59 | 68.09 |
|       mountain      | 53.85 | 61.85 |
|        plant        | 51.33 | 62.58 |
|       curtain       | 72.32 | 84.21 |
|        chair        | 48.59 | 58.88 |
|         car         | 81.62 | 92.54 |
|        water        | 55.53 |  70.1 |
|       painting      | 67.36 | 87.65 |
|         sofa        | 58.86 | 71.06 |
|        shelf        | 42.94 | 61.01 |
|        house        | 48.38 | 53.16 |
|         sea         | 56.04 | 81.19 |
|        mirror       | 57.45 | 69.02 |
|         rug         | 59.75 | 71.21 |
|        field        | 30.34 | 40.39 |
|       armchair      | 35.31 |  68.7 |
|         seat        | 63.12 | 83.13 |
|        fence        | 43.96 | 62.86 |
|         desk        | 48.72 | 63.09 |
|         rock        | 45.35 | 61.91 |
|       wardrobe      |  47.4 | 57.61 |
|         lamp        | 58.39 |  76.5 |
|       bathtub       | 78.23 | 83.74 |
|       railing       | 29.45 | 41.07 |
|       cushion       | 56.04 |  66.4 |
|         base        | 31.05 |  49.0 |
|         box         | 26.15 | 36.81 |
|        column       | 44.47 | 58.62 |
|      signboard      | 37.51 | 51.07 |
|   chest of drawers  | 40.57 | 50.58 |
|       counter       | 39.92 | 52.44 |
|         sand        | 34.62 | 61.11 |
|         sink        |  66.3 | 77.31 |
|      skyscraper     | 48.45 | 62.33 |
|      fireplace      | 67.18 | 81.28 |
|     refrigerator    | 71.29 | 85.65 |
|      grandstand     | 42.64 | 71.78 |
|         path        |  20.1 | 31.68 |
|        stairs       | 33.15 | 39.76 |
|        runway       | 62.88 | 80.44 |
|         case        | 55.03 | 64.48 |
|      pool table     | 92.13 |  95.2 |
|        pillow       | 57.73 | 69.09 |
|     screen door     | 68.16 | 80.73 |
|       stairway      | 40.03 | 44.33 |
|        river        |  8.25 | 15.76 |
|        bridge       | 73.22 | 83.76 |
|       bookcase      | 36.77 | 48.28 |
|        blind        | 39.51 | 45.92 |
|     coffee table    | 39.98 | 81.66 |
|        toilet       |  80.4 | 90.07 |
|        flower       |  34.3 | 44.23 |
|         book        | 44.04 | 59.96 |
|         hill        |  5.8  | 11.02 |
|        bench        | 40.09 | 49.18 |
|      countertop     | 56.96 | 70.98 |
|        stove        | 68.13 | 79.72 |
|         palm        | 47.18 | 72.25 |
|    kitchen island   | 43.44 | 68.12 |
|       computer      | 58.13 | 66.77 |
|     swivel chair    | 46.86 | 70.99 |
|         boat        | 59.25 | 85.51 |
|         bar         | 21.43 | 25.87 |
|    arcade machine   | 73.62 | 79.58 |
|        hovel        | 54.39 | 66.44 |
|         bus         | 82.28 | 94.62 |
|        towel        | 61.36 | 74.79 |
|        light        | 49.99 | 57.88 |
|        truck        | 14.65 | 27.88 |
|        tower        | 13.57 |  16.7 |
|      chandelier     | 61.22 | 83.58 |
|        awning       | 26.63 | 31.88 |
|     streetlight     | 22.39 | 31.43 |
|        booth        | 51.34 | 62.84 |
| television receiver | 66.88 | 80.48 |
|       airplane      |  56.2 | 69.89 |
|      dirt track     |  0.0  |  0.0  |
|       apparel       | 36.16 | 49.65 |
|         pole        | 21.47 | 27.89 |
|         land        |  1.41 |  2.59 |
|      bannister      |  13.9 | 18.88 |
|      escalator      | 43.91 | 65.67 |
|       ottoman       |  32.9 | 59.72 |
|        bottle       | 34.69 | 57.16 |
|        buffet       | 58.11 | 64.99 |
|        poster       | 33.25 | 47.72 |
|        stage        | 19.65 | 24.52 |
|         van         | 37.65 | 51.83 |
|         ship        |  35.7 | 38.37 |
|       fountain      | 37.77 | 46.02 |
|    conveyer belt    | 81.39 | 86.11 |
|        canopy       | 20.83 | 22.08 |
|        washer       | 74.36 | 75.81 |
|      plaything      | 20.21 | 33.28 |
|    swimming pool    | 64.84 | 75.97 |
|        stool        | 35.34 | 54.23 |
|        barrel       | 55.64 | 63.12 |
|        basket       | 37.68 |  51.0 |
|      waterfall      | 51.04 | 91.69 |
|         tent        | 95.84 | 98.43 |
|         bag         | 16.31 | 20.31 |
|       minibike      | 52.75 | 61.12 |
|        cradle       | 76.35 | 96.27 |
|         oven        | 27.83 | 51.34 |
|         ball        | 42.85 | 66.03 |
|         food        | 43.88 |  48.3 |
|         step        | 12.54 | 17.93 |
|         tank        | 35.97 | 40.61 |
|      trade name     | 21.78 | 24.21 |
|      microwave      | 59.86 | 67.75 |
|         pot         | 38.24 | 42.76 |
|        animal       |  55.0 |  62.2 |
|       bicycle       | 57.17 | 77.13 |
|         lake        | 24.41 | 25.74 |
|      dishwasher     | 54.91 | 59.74 |
|        screen       | 60.95 | 94.53 |
|       blanket       | 11.64 | 14.01 |
|      sculpture      | 48.86 | 71.97 |
|         hood        | 52.38 | 61.46 |
|        sconce       | 37.81 | 45.13 |
|         vase        | 28.64 | 43.69 |
|    traffic light    | 21.17 | 34.17 |
|         tray        |  5.81 |  8.07 |
|        ashcan       | 31.36 | 41.31 |
|         fan         | 55.28 | 78.16 |
|         pier        | 12.35 | 13.94 |
|      crt screen     |  4.58 | 10.78 |
|        plate        |  37.0 | 50.11 |
|       monitor       |  7.99 |  10.2 |
|    bulletin board   | 44.98 | 58.86 |
|        shower       |  0.0  |  0.0  |
|       radiator      | 58.96 | 69.23 |
|        glass        | 12.04 | 14.55 |
|        clock        |  28.7 | 35.75 |
|         flag        | 59.66 | 64.85 |
+---------------------+-------+-------+
2023-11-10 20:11:26,597 - mmseg - INFO - Summary:
2023-11-10 20:11:26,598 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 81.95 | 46.74 | 58.87 |
+-------+-------+-------+
2023-11-10 20:11:26,738 - mmseg - INFO - The previous best checkpoint /data/Next-ViT/segmentation/work_dirs/fpn_512_debi_base_160k/best_mIoU_iter_40000.pth was removed
2023-11-10 20:11:29,568 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_48000.pth.
2023-11-10 20:11:29,569 - mmseg - INFO - Best mIoU is 0.4674 at 48000 iter.
2023-11-10 20:11:29,611 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 20:11:29,612 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8195, mIoU: 0.4674, mAcc: 0.5887, IoU.wall: 0.7617, IoU.building: 0.8244, IoU.sky: 0.9402, IoU.floor: 0.8050, IoU.tree: 0.7380, IoU.ceiling: 0.8341, IoU.road: 0.8195, IoU.bed : 0.8807, IoU.windowpane: 0.5906, IoU.grass: 0.6660, IoU.cabinet: 0.5941, IoU.sidewalk: 0.6236, IoU.person: 0.7829, IoU.earth: 0.3170, IoU.door: 0.4698, IoU.table: 0.5459, IoU.mountain: 0.5385, IoU.plant: 0.5133, IoU.curtain: 0.7232, IoU.chair: 0.4859, IoU.car: 0.8162, IoU.water: 0.5553, IoU.painting: 0.6736, IoU.sofa: 0.5886, IoU.shelf: 0.4294, IoU.house: 0.4838, IoU.sea: 0.5604, IoU.mirror: 0.5745, IoU.rug: 0.5975, IoU.field: 0.3034, IoU.armchair: 0.3531, IoU.seat: 0.6312, IoU.fence: 0.4396, IoU.desk: 0.4872, IoU.rock: 0.4535, IoU.wardrobe: 0.4740, IoU.lamp: 0.5839, IoU.bathtub: 0.7823, IoU.railing: 0.2945, IoU.cushion: 0.5604, IoU.base: 0.3105, IoU.box: 0.2615, IoU.column: 0.4447, IoU.signboard: 0.3751, IoU.chest of drawers: 0.4057, IoU.counter: 0.3992, IoU.sand: 0.3462, IoU.sink: 0.6630, IoU.skyscraper: 0.4845, IoU.fireplace: 0.6718, IoU.refrigerator: 0.7129, IoU.grandstand: 0.4264, IoU.path: 0.2010, IoU.stairs: 0.3315, IoU.runway: 0.6288, IoU.case: 0.5503, IoU.pool table: 0.9213, IoU.pillow: 0.5773, IoU.screen door: 0.6816, IoU.stairway: 0.4003, IoU.river: 0.0825, IoU.bridge: 0.7322, IoU.bookcase: 0.3677, IoU.blind: 0.3951, IoU.coffee table: 0.3998, IoU.toilet: 0.8040, IoU.flower: 0.3430, IoU.book: 0.4404, IoU.hill: 0.0580, IoU.bench: 0.4009, IoU.countertop: 0.5696, IoU.stove: 0.6813, IoU.palm: 0.4718, IoU.kitchen island: 0.4344, IoU.computer: 0.5813, IoU.swivel chair: 0.4686, IoU.boat: 0.5925, IoU.bar: 0.2143, IoU.arcade machine: 0.7362, IoU.hovel: 0.5439, IoU.bus: 0.8228, IoU.towel: 0.6136, IoU.light: 0.4999, IoU.truck: 0.1465, IoU.tower: 0.1357, IoU.chandelier: 0.6122, IoU.awning: 0.2663, IoU.streetlight: 0.2239, IoU.booth: 0.5134, IoU.television receiver: 0.6688, IoU.airplane: 0.5620, IoU.dirt track: 0.0000, IoU.apparel: 0.3616, IoU.pole: 0.2147, IoU.land: 0.0141, IoU.bannister: 0.1390, IoU.escalator: 0.4391, IoU.ottoman: 0.3290, IoU.bottle: 0.3469, IoU.buffet: 0.5811, IoU.poster: 0.3325, IoU.stage: 0.1965, IoU.van: 0.3765, IoU.ship: 0.3570, IoU.fountain: 0.3777, IoU.conveyer belt: 0.8139, IoU.canopy: 0.2083, IoU.washer: 0.7436, IoU.plaything: 0.2021, IoU.swimming pool: 0.6484, IoU.stool: 0.3534, IoU.barrel: 0.5564, IoU.basket: 0.3768, IoU.waterfall: 0.5104, IoU.tent: 0.9584, IoU.bag: 0.1631, IoU.minibike: 0.5275, IoU.cradle: 0.7635, IoU.oven: 0.2783, IoU.ball: 0.4285, IoU.food: 0.4388, IoU.step: 0.1254, IoU.tank: 0.3597, IoU.trade name: 0.2178, IoU.microwave: 0.5986, IoU.pot: 0.3824, IoU.animal: 0.5500, IoU.bicycle: 0.5717, IoU.lake: 0.2441, IoU.dishwasher: 0.5491, IoU.screen: 0.6095, IoU.blanket: 0.1164, IoU.sculpture: 0.4886, IoU.hood: 0.5238, IoU.sconce: 0.3781, IoU.vase: 0.2864, IoU.traffic light: 0.2117, IoU.tray: 0.0581, IoU.ashcan: 0.3136, IoU.fan: 0.5528, IoU.pier: 0.1235, IoU.crt screen: 0.0458, IoU.plate: 0.3700, IoU.monitor: 0.0799, IoU.bulletin board: 0.4498, IoU.shower: 0.0000, IoU.radiator: 0.5896, IoU.glass: 0.1204, IoU.clock: 0.2870, IoU.flag: 0.5966, Acc.wall: 0.8675, Acc.building: 0.9288, Acc.sky: 0.9727, Acc.floor: 0.8986, Acc.tree: 0.8654, Acc.ceiling: 0.9003, Acc.road: 0.9170, Acc.bed : 0.9537, Acc.windowpane: 0.7926, Acc.grass: 0.8459, Acc.cabinet: 0.7851, Acc.sidewalk: 0.7705, Acc.person: 0.9280, Acc.earth: 0.4758, Acc.door: 0.6231, Acc.table: 0.6809, Acc.mountain: 0.6185, Acc.plant: 0.6258, Acc.curtain: 0.8421, Acc.chair: 0.5888, Acc.car: 0.9254, Acc.water: 0.7010, Acc.painting: 0.8765, Acc.sofa: 0.7106, Acc.shelf: 0.6101, Acc.house: 0.5316, Acc.sea: 0.8119, Acc.mirror: 0.6902, Acc.rug: 0.7121, Acc.field: 0.4039, Acc.armchair: 0.6870, Acc.seat: 0.8313, Acc.fence: 0.6286, Acc.desk: 0.6309, Acc.rock: 0.6191, Acc.wardrobe: 0.5761, Acc.lamp: 0.7650, Acc.bathtub: 0.8374, Acc.railing: 0.4107, Acc.cushion: 0.6640, Acc.base: 0.4900, Acc.box: 0.3681, Acc.column: 0.5862, Acc.signboard: 0.5107, Acc.chest of drawers: 0.5058, Acc.counter: 0.5244, Acc.sand: 0.6111, Acc.sink: 0.7731, Acc.skyscraper: 0.6233, Acc.fireplace: 0.8128, Acc.refrigerator: 0.8565, Acc.grandstand: 0.7178, Acc.path: 0.3168, Acc.stairs: 0.3976, Acc.runway: 0.8044, Acc.case: 0.6448, Acc.pool table: 0.9520, Acc.pillow: 0.6909, Acc.screen door: 0.8073, Acc.stairway: 0.4433, Acc.river: 0.1576, Acc.bridge: 0.8376, Acc.bookcase: 0.4828, Acc.blind: 0.4592, Acc.coffee table: 0.8166, Acc.toilet: 0.9007, Acc.flower: 0.4423, Acc.book: 0.5996, Acc.hill: 0.1102, Acc.bench: 0.4918, Acc.countertop: 0.7098, Acc.stove: 0.7972, Acc.palm: 0.7225, Acc.kitchen island: 0.6812, Acc.computer: 0.6677, Acc.swivel chair: 0.7099, Acc.boat: 0.8551, Acc.bar: 0.2587, Acc.arcade machine: 0.7958, Acc.hovel: 0.6644, Acc.bus: 0.9462, Acc.towel: 0.7479, Acc.light: 0.5788, Acc.truck: 0.2788, Acc.tower: 0.1670, Acc.chandelier: 0.8358, Acc.awning: 0.3188, Acc.streetlight: 0.3143, Acc.booth: 0.6284, Acc.television receiver: 0.8048, Acc.airplane: 0.6989, Acc.dirt track: 0.0000, Acc.apparel: 0.4965, Acc.pole: 0.2789, Acc.land: 0.0259, Acc.bannister: 0.1888, Acc.escalator: 0.6567, Acc.ottoman: 0.5972, Acc.bottle: 0.5716, Acc.buffet: 0.6499, Acc.poster: 0.4772, Acc.stage: 0.2452, Acc.van: 0.5183, Acc.ship: 0.3837, Acc.fountain: 0.4602, Acc.conveyer belt: 0.8611, Acc.canopy: 0.2208, Acc.washer: 0.7581, Acc.plaything: 0.3328, Acc.swimming pool: 0.7597, Acc.stool: 0.5423, Acc.barrel: 0.6312, Acc.basket: 0.5100, Acc.waterfall: 0.9169, Acc.tent: 0.9843, Acc.bag: 0.2031, Acc.minibike: 0.6112, Acc.cradle: 0.9627, Acc.oven: 0.5134, Acc.ball: 0.6603, Acc.food: 0.4830, Acc.step: 0.1793, Acc.tank: 0.4061, Acc.trade name: 0.2421, Acc.microwave: 0.6775, Acc.pot: 0.4276, Acc.animal: 0.6220, Acc.bicycle: 0.7713, Acc.lake: 0.2574, Acc.dishwasher: 0.5974, Acc.screen: 0.9453, Acc.blanket: 0.1401, Acc.sculpture: 0.7197, Acc.hood: 0.6146, Acc.sconce: 0.4513, Acc.vase: 0.4369, Acc.traffic light: 0.3417, Acc.tray: 0.0807, Acc.ashcan: 0.4131, Acc.fan: 0.7816, Acc.pier: 0.1394, Acc.crt screen: 0.1078, Acc.plate: 0.5011, Acc.monitor: 0.1020, Acc.bulletin board: 0.5886, Acc.shower: 0.0000, Acc.radiator: 0.6923, Acc.glass: 0.1455, Acc.clock: 0.3575, Acc.flag: 0.6485
2023-11-10 20:12:03,332 - mmseg - INFO - Iter [48050/160000]	lr: 7.935e-05, eta: 21:50:15, time: 2.583, data_time: 1.919, memory: 23129, decode.loss_ce: 0.3531, decode.acc_seg: 86.1978, loss: 0.3531
2023-11-10 20:12:39,679 - mmseg - INFO - Iter [48100/160000]	lr: 7.931e-05, eta: 21:49:43, time: 0.727, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3427, decode.acc_seg: 86.5382, loss: 0.3427
2023-11-10 20:13:13,286 - mmseg - INFO - Iter [48150/160000]	lr: 7.927e-05, eta: 21:49:04, time: 0.673, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3145, decode.acc_seg: 87.5243, loss: 0.3145
2023-11-10 20:13:46,517 - mmseg - INFO - Iter [48200/160000]	lr: 7.923e-05, eta: 21:48:25, time: 0.665, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3355, decode.acc_seg: 86.9916, loss: 0.3355
2023-11-10 20:14:19,684 - mmseg - INFO - Iter [48250/160000]	lr: 7.919e-05, eta: 21:47:45, time: 0.663, data_time: 0.008, memory: 23129, decode.loss_ce: 0.3349, decode.acc_seg: 86.7629, loss: 0.3349
2023-11-10 20:14:53,394 - mmseg - INFO - Iter [48300/160000]	lr: 7.915e-05, eta: 21:47:07, time: 0.674, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3429, decode.acc_seg: 86.8492, loss: 0.3429
2023-11-10 20:15:28,200 - mmseg - INFO - Iter [48350/160000]	lr: 7.911e-05, eta: 21:46:31, time: 0.695, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3369, decode.acc_seg: 86.9398, loss: 0.3369
2023-11-10 20:16:02,096 - mmseg - INFO - Iter [48400/160000]	lr: 7.907e-05, eta: 21:45:53, time: 0.678, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3259, decode.acc_seg: 86.8693, loss: 0.3259
2023-11-10 20:16:36,122 - mmseg - INFO - Iter [48450/160000]	lr: 7.903e-05, eta: 21:45:15, time: 0.680, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3464, decode.acc_seg: 86.9332, loss: 0.3464
2023-11-10 20:17:11,811 - mmseg - INFO - Iter [48500/160000]	lr: 7.899e-05, eta: 21:44:42, time: 0.715, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3253, decode.acc_seg: 87.3049, loss: 0.3253
2023-11-10 20:17:45,993 - mmseg - INFO - Iter [48550/160000]	lr: 7.895e-05, eta: 21:44:04, time: 0.684, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3450, decode.acc_seg: 86.5889, loss: 0.3450
2023-11-10 20:18:19,246 - mmseg - INFO - Iter [48600/160000]	lr: 7.891e-05, eta: 21:43:25, time: 0.665, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3464, decode.acc_seg: 86.4905, loss: 0.3464
2023-11-10 20:18:55,183 - mmseg - INFO - Iter [48650/160000]	lr: 7.887e-05, eta: 21:42:52, time: 0.718, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3634, decode.acc_seg: 85.9457, loss: 0.3634
2023-11-10 20:19:29,509 - mmseg - INFO - Iter [48700/160000]	lr: 7.883e-05, eta: 21:42:15, time: 0.687, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3410, decode.acc_seg: 86.7118, loss: 0.3410
2023-11-10 20:20:03,126 - mmseg - INFO - Iter [48750/160000]	lr: 7.879e-05, eta: 21:41:36, time: 0.673, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3580, decode.acc_seg: 86.5469, loss: 0.3580
2023-11-10 20:20:36,054 - mmseg - INFO - Iter [48800/160000]	lr: 7.875e-05, eta: 21:40:56, time: 0.659, data_time: 0.008, memory: 23129, decode.loss_ce: 0.3162, decode.acc_seg: 87.2963, loss: 0.3162
2023-11-10 20:21:10,563 - mmseg - INFO - Iter [48850/160000]	lr: 7.871e-05, eta: 21:40:20, time: 0.689, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3475, decode.acc_seg: 86.1811, loss: 0.3475
2023-11-10 20:21:46,606 - mmseg - INFO - Iter [48900/160000]	lr: 7.867e-05, eta: 21:39:47, time: 0.721, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3284, decode.acc_seg: 87.1558, loss: 0.3284
2023-11-10 20:22:21,999 - mmseg - INFO - Iter [48950/160000]	lr: 7.863e-05, eta: 21:39:12, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3310, decode.acc_seg: 86.8369, loss: 0.3310
2023-11-10 20:22:55,568 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 20:22:55,568 - mmseg - INFO - Iter [49000/160000]	lr: 7.859e-05, eta: 21:38:34, time: 0.672, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3394, decode.acc_seg: 87.2300, loss: 0.3394
2023-11-10 20:23:30,231 - mmseg - INFO - Iter [49050/160000]	lr: 7.855e-05, eta: 21:37:58, time: 0.693, data_time: 0.008, memory: 23129, decode.loss_ce: 0.3052, decode.acc_seg: 87.7848, loss: 0.3052
2023-11-10 20:24:03,074 - mmseg - INFO - Iter [49100/160000]	lr: 7.851e-05, eta: 21:37:18, time: 0.658, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3597, decode.acc_seg: 86.2965, loss: 0.3597
2023-11-10 20:24:36,864 - mmseg - INFO - Iter [49150/160000]	lr: 7.847e-05, eta: 21:36:40, time: 0.675, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3246, decode.acc_seg: 87.4403, loss: 0.3246
2023-11-10 20:25:11,117 - mmseg - INFO - Iter [49200/160000]	lr: 7.843e-05, eta: 21:36:03, time: 0.686, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3514, decode.acc_seg: 86.4156, loss: 0.3514
2023-11-10 20:25:43,774 - mmseg - INFO - Iter [49250/160000]	lr: 7.839e-05, eta: 21:35:22, time: 0.652, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3330, decode.acc_seg: 87.0750, loss: 0.3330
2023-11-10 20:26:17,648 - mmseg - INFO - Iter [49300/160000]	lr: 7.835e-05, eta: 21:34:44, time: 0.679, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3285, decode.acc_seg: 87.3328, loss: 0.3285
2023-11-10 20:26:53,218 - mmseg - INFO - Iter [49350/160000]	lr: 7.831e-05, eta: 21:34:10, time: 0.710, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3155, decode.acc_seg: 87.6394, loss: 0.3155
2023-11-10 20:27:27,433 - mmseg - INFO - Iter [49400/160000]	lr: 7.827e-05, eta: 21:33:33, time: 0.685, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3136, decode.acc_seg: 87.6265, loss: 0.3136
2023-11-10 20:28:01,718 - mmseg - INFO - Iter [49450/160000]	lr: 7.823e-05, eta: 21:32:56, time: 0.685, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3205, decode.acc_seg: 87.7437, loss: 0.3205
2023-11-10 20:28:36,049 - mmseg - INFO - Iter [49500/160000]	lr: 7.819e-05, eta: 21:32:20, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3101, decode.acc_seg: 87.6945, loss: 0.3101
2023-11-10 20:29:12,040 - mmseg - INFO - Iter [49550/160000]	lr: 7.815e-05, eta: 21:31:46, time: 0.720, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3135, decode.acc_seg: 87.2853, loss: 0.3135
2023-11-10 20:29:47,700 - mmseg - INFO - Iter [49600/160000]	lr: 7.810e-05, eta: 21:31:13, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3023, decode.acc_seg: 88.1541, loss: 0.3023
2023-11-10 20:30:20,423 - mmseg - INFO - Iter [49650/160000]	lr: 7.806e-05, eta: 21:30:32, time: 0.655, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3166, decode.acc_seg: 87.5916, loss: 0.3166
2023-11-10 20:30:53,590 - mmseg - INFO - Iter [49700/160000]	lr: 7.802e-05, eta: 21:29:53, time: 0.663, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3248, decode.acc_seg: 87.3274, loss: 0.3248
2023-11-10 20:31:30,934 - mmseg - INFO - Iter [49750/160000]	lr: 7.798e-05, eta: 21:29:23, time: 0.746, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3228, decode.acc_seg: 87.5814, loss: 0.3228
2023-11-10 20:32:06,584 - mmseg - INFO - Iter [49800/160000]	lr: 7.794e-05, eta: 21:28:49, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2988, decode.acc_seg: 88.0785, loss: 0.2988
2023-11-10 20:32:42,245 - mmseg - INFO - Iter [49850/160000]	lr: 7.790e-05, eta: 21:28:15, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3475, decode.acc_seg: 86.8198, loss: 0.3475
2023-11-10 20:33:17,600 - mmseg - INFO - Iter [49900/160000]	lr: 7.786e-05, eta: 21:27:41, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3316, decode.acc_seg: 87.2743, loss: 0.3316
2023-11-10 20:33:52,927 - mmseg - INFO - Iter [49950/160000]	lr: 7.782e-05, eta: 21:27:06, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3305, decode.acc_seg: 87.1250, loss: 0.3305
2023-11-10 20:34:27,274 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 20:34:27,275 - mmseg - INFO - Iter [50000/160000]	lr: 7.778e-05, eta: 21:26:29, time: 0.687, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3288, decode.acc_seg: 86.8403, loss: 0.3288
2023-11-10 20:35:01,999 - mmseg - INFO - Iter [50050/160000]	lr: 7.774e-05, eta: 21:25:54, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3331, decode.acc_seg: 87.1790, loss: 0.3331
2023-11-10 20:35:35,352 - mmseg - INFO - Iter [50100/160000]	lr: 7.770e-05, eta: 21:25:15, time: 0.668, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3329, decode.acc_seg: 86.9485, loss: 0.3329
2023-11-10 20:36:10,537 - mmseg - INFO - Iter [50150/160000]	lr: 7.766e-05, eta: 21:24:40, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3479, decode.acc_seg: 86.5897, loss: 0.3479
2023-11-10 20:36:46,182 - mmseg - INFO - Iter [50200/160000]	lr: 7.762e-05, eta: 21:24:06, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3278, decode.acc_seg: 86.8802, loss: 0.3278
2023-11-10 20:37:19,257 - mmseg - INFO - Iter [50250/160000]	lr: 7.757e-05, eta: 21:23:27, time: 0.662, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3152, decode.acc_seg: 87.1145, loss: 0.3152
2023-11-10 20:37:53,305 - mmseg - INFO - Iter [50300/160000]	lr: 7.753e-05, eta: 21:22:49, time: 0.680, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3286, decode.acc_seg: 86.7761, loss: 0.3286
2023-11-10 20:38:27,215 - mmseg - INFO - Iter [50350/160000]	lr: 7.749e-05, eta: 21:22:12, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3111, decode.acc_seg: 87.5146, loss: 0.3111
2023-11-10 20:39:00,337 - mmseg - INFO - Iter [50400/160000]	lr: 7.745e-05, eta: 21:21:32, time: 0.662, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3067, decode.acc_seg: 88.1345, loss: 0.3067
2023-11-10 20:39:35,951 - mmseg - INFO - Iter [50450/160000]	lr: 7.741e-05, eta: 21:20:58, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3327, decode.acc_seg: 87.1908, loss: 0.3327
2023-11-10 20:40:11,689 - mmseg - INFO - Iter [50500/160000]	lr: 7.737e-05, eta: 21:20:25, time: 0.715, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3264, decode.acc_seg: 87.3407, loss: 0.3264
2023-11-10 20:40:46,393 - mmseg - INFO - Iter [50550/160000]	lr: 7.733e-05, eta: 21:19:49, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3250, decode.acc_seg: 87.1797, loss: 0.3250
2023-11-10 20:41:21,692 - mmseg - INFO - Iter [50600/160000]	lr: 7.729e-05, eta: 21:19:14, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3543, decode.acc_seg: 86.5281, loss: 0.3543
2023-11-10 20:41:57,364 - mmseg - INFO - Iter [50650/160000]	lr: 7.725e-05, eta: 21:18:40, time: 0.713, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3337, decode.acc_seg: 87.0519, loss: 0.3337
2023-11-10 20:42:33,046 - mmseg - INFO - Iter [50700/160000]	lr: 7.721e-05, eta: 21:18:06, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3406, decode.acc_seg: 86.6097, loss: 0.3406
2023-11-10 20:43:06,778 - mmseg - INFO - Iter [50750/160000]	lr: 7.716e-05, eta: 21:17:29, time: 0.675, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3112, decode.acc_seg: 87.7553, loss: 0.3112
2023-11-10 20:43:42,416 - mmseg - INFO - Iter [50800/160000]	lr: 7.712e-05, eta: 21:16:55, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3282, decode.acc_seg: 87.1781, loss: 0.3282
2023-11-10 20:44:17,789 - mmseg - INFO - Iter [50850/160000]	lr: 7.708e-05, eta: 21:16:20, time: 0.707, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3200, decode.acc_seg: 87.0564, loss: 0.3200
2023-11-10 20:44:53,121 - mmseg - INFO - Iter [50900/160000]	lr: 7.704e-05, eta: 21:15:46, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3312, decode.acc_seg: 86.9447, loss: 0.3312
2023-11-10 20:45:24,985 - mmseg - INFO - Iter [50950/160000]	lr: 7.700e-05, eta: 21:15:04, time: 0.639, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3305, decode.acc_seg: 86.9958, loss: 0.3305
2023-11-10 20:45:58,144 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 20:45:58,145 - mmseg - INFO - Iter [51000/160000]	lr: 7.696e-05, eta: 21:14:25, time: 0.663, data_time: 0.008, memory: 23129, decode.loss_ce: 0.3171, decode.acc_seg: 87.4663, loss: 0.3171
2023-11-10 20:46:31,156 - mmseg - INFO - Iter [51050/160000]	lr: 7.692e-05, eta: 21:13:45, time: 0.660, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3205, decode.acc_seg: 88.0026, loss: 0.3205
2023-11-10 20:47:06,363 - mmseg - INFO - Iter [51100/160000]	lr: 7.688e-05, eta: 21:13:10, time: 0.704, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3271, decode.acc_seg: 87.1064, loss: 0.3271
2023-11-10 20:47:38,768 - mmseg - INFO - Iter [51150/160000]	lr: 7.683e-05, eta: 21:12:30, time: 0.648, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3376, decode.acc_seg: 86.7660, loss: 0.3376
2023-11-10 20:48:10,626 - mmseg - INFO - Iter [51200/160000]	lr: 7.679e-05, eta: 21:11:48, time: 0.637, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3333, decode.acc_seg: 86.7881, loss: 0.3333
2023-11-10 20:48:44,772 - mmseg - INFO - Iter [51250/160000]	lr: 7.675e-05, eta: 21:11:11, time: 0.682, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3468, decode.acc_seg: 87.0037, loss: 0.3468
2023-11-10 20:49:20,307 - mmseg - INFO - Iter [51300/160000]	lr: 7.671e-05, eta: 21:10:36, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3627, decode.acc_seg: 86.3208, loss: 0.3627
2023-11-10 20:49:53,336 - mmseg - INFO - Iter [51350/160000]	lr: 7.667e-05, eta: 21:09:57, time: 0.661, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3327, decode.acc_seg: 87.3478, loss: 0.3327
2023-11-10 20:50:26,854 - mmseg - INFO - Iter [51400/160000]	lr: 7.663e-05, eta: 21:09:19, time: 0.670, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3120, decode.acc_seg: 87.7946, loss: 0.3120
2023-11-10 20:51:02,206 - mmseg - INFO - Iter [51450/160000]	lr: 7.658e-05, eta: 21:08:44, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3241, decode.acc_seg: 86.9892, loss: 0.3241
2023-11-10 20:51:36,986 - mmseg - INFO - Iter [51500/160000]	lr: 7.654e-05, eta: 21:08:09, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3151, decode.acc_seg: 87.5264, loss: 0.3151
2023-11-10 20:52:12,067 - mmseg - INFO - Iter [51550/160000]	lr: 7.650e-05, eta: 21:07:34, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3245, decode.acc_seg: 87.1148, loss: 0.3245
2023-11-10 20:52:47,131 - mmseg - INFO - Iter [51600/160000]	lr: 7.646e-05, eta: 21:06:59, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3278, decode.acc_seg: 87.4098, loss: 0.3278
2023-11-10 20:53:20,832 - mmseg - INFO - Iter [51650/160000]	lr: 7.642e-05, eta: 21:06:21, time: 0.674, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3008, decode.acc_seg: 88.4386, loss: 0.3008
2023-11-10 20:53:55,900 - mmseg - INFO - Iter [51700/160000]	lr: 7.638e-05, eta: 21:05:46, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3009, decode.acc_seg: 88.2067, loss: 0.3009
2023-11-10 20:54:31,313 - mmseg - INFO - Iter [51750/160000]	lr: 7.633e-05, eta: 21:05:11, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3136, decode.acc_seg: 87.9570, loss: 0.3136
2023-11-10 20:55:03,564 - mmseg - INFO - Iter [51800/160000]	lr: 7.629e-05, eta: 21:04:30, time: 0.645, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3078, decode.acc_seg: 87.9103, loss: 0.3078
2023-11-10 20:55:40,655 - mmseg - INFO - Iter [51850/160000]	lr: 7.625e-05, eta: 21:03:59, time: 0.741, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3148, decode.acc_seg: 87.7041, loss: 0.3148
2023-11-10 20:56:17,624 - mmseg - INFO - Iter [51900/160000]	lr: 7.621e-05, eta: 21:03:28, time: 0.739, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3159, decode.acc_seg: 87.6060, loss: 0.3159
2023-11-10 20:56:54,371 - mmseg - INFO - Iter [51950/160000]	lr: 7.617e-05, eta: 21:02:57, time: 0.735, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3345, decode.acc_seg: 86.7713, loss: 0.3345
2023-11-10 20:57:32,112 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 20:57:32,112 - mmseg - INFO - Iter [52000/160000]	lr: 7.613e-05, eta: 21:02:27, time: 0.755, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3238, decode.acc_seg: 87.3125, loss: 0.3238
2023-11-10 20:58:04,663 - mmseg - INFO - Iter [52050/160000]	lr: 7.608e-05, eta: 21:01:47, time: 0.652, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3080, decode.acc_seg: 87.6802, loss: 0.3080
2023-11-10 20:58:39,868 - mmseg - INFO - Iter [52100/160000]	lr: 7.604e-05, eta: 21:01:12, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3087, decode.acc_seg: 87.9051, loss: 0.3087
2023-11-10 20:59:15,277 - mmseg - INFO - Iter [52150/160000]	lr: 7.600e-05, eta: 21:00:38, time: 0.709, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3301, decode.acc_seg: 87.4323, loss: 0.3301
2023-11-10 20:59:50,362 - mmseg - INFO - Iter [52200/160000]	lr: 7.596e-05, eta: 21:00:03, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3139, decode.acc_seg: 87.6439, loss: 0.3139
2023-11-10 21:00:24,886 - mmseg - INFO - Iter [52250/160000]	lr: 7.592e-05, eta: 20:59:27, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3166, decode.acc_seg: 87.5926, loss: 0.3166
2023-11-10 21:00:57,733 - mmseg - INFO - Iter [52300/160000]	lr: 7.587e-05, eta: 20:58:47, time: 0.657, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3086, decode.acc_seg: 87.6246, loss: 0.3086
2023-11-10 21:01:32,261 - mmseg - INFO - Iter [52350/160000]	lr: 7.583e-05, eta: 20:58:11, time: 0.689, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3469, decode.acc_seg: 86.8395, loss: 0.3469
2023-11-10 21:02:07,499 - mmseg - INFO - Iter [52400/160000]	lr: 7.579e-05, eta: 20:57:36, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3141, decode.acc_seg: 87.5772, loss: 0.3141
2023-11-10 21:02:43,253 - mmseg - INFO - Iter [52450/160000]	lr: 7.575e-05, eta: 20:57:02, time: 0.715, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3214, decode.acc_seg: 87.7734, loss: 0.3214
2023-11-10 21:03:18,388 - mmseg - INFO - Iter [52500/160000]	lr: 7.571e-05, eta: 20:56:28, time: 0.704, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3177, decode.acc_seg: 87.4438, loss: 0.3177
2023-11-10 21:03:50,045 - mmseg - INFO - Iter [52550/160000]	lr: 7.566e-05, eta: 20:55:46, time: 0.633, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3408, decode.acc_seg: 86.8878, loss: 0.3408
2023-11-10 21:04:21,850 - mmseg - INFO - Iter [52600/160000]	lr: 7.562e-05, eta: 20:55:04, time: 0.636, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3213, decode.acc_seg: 87.4747, loss: 0.3213
2023-11-10 21:04:54,692 - mmseg - INFO - Iter [52650/160000]	lr: 7.558e-05, eta: 20:54:24, time: 0.656, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3142, decode.acc_seg: 87.6741, loss: 0.3142
2023-11-10 21:05:29,987 - mmseg - INFO - Iter [52700/160000]	lr: 7.554e-05, eta: 20:53:50, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3150, decode.acc_seg: 87.1066, loss: 0.3150
2023-11-10 21:06:03,349 - mmseg - INFO - Iter [52750/160000]	lr: 7.550e-05, eta: 20:53:11, time: 0.668, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3126, decode.acc_seg: 88.0439, loss: 0.3126
2023-11-10 21:06:38,914 - mmseg - INFO - Iter [52800/160000]	lr: 7.545e-05, eta: 20:52:37, time: 0.711, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3226, decode.acc_seg: 87.3706, loss: 0.3226
2023-11-10 21:07:14,474 - mmseg - INFO - Iter [52850/160000]	lr: 7.541e-05, eta: 20:52:03, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2906, decode.acc_seg: 88.2756, loss: 0.2906
2023-11-10 21:07:50,280 - mmseg - INFO - Iter [52900/160000]	lr: 7.537e-05, eta: 20:51:30, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3283, decode.acc_seg: 87.3087, loss: 0.3283
2023-11-10 21:08:23,042 - mmseg - INFO - Iter [52950/160000]	lr: 7.533e-05, eta: 20:50:50, time: 0.656, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3127, decode.acc_seg: 87.7319, loss: 0.3127
2023-11-10 21:08:56,609 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 21:08:56,609 - mmseg - INFO - Iter [53000/160000]	lr: 7.528e-05, eta: 20:50:12, time: 0.670, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2891, decode.acc_seg: 88.6089, loss: 0.2891
2023-11-10 21:09:31,480 - mmseg - INFO - Iter [53050/160000]	lr: 7.524e-05, eta: 20:49:36, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3273, decode.acc_seg: 87.4811, loss: 0.3273
2023-11-10 21:10:06,652 - mmseg - INFO - Iter [53100/160000]	lr: 7.520e-05, eta: 20:49:02, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3126, decode.acc_seg: 87.4175, loss: 0.3126
2023-11-10 21:10:41,383 - mmseg - INFO - Iter [53150/160000]	lr: 7.516e-05, eta: 20:48:26, time: 0.695, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3457, decode.acc_seg: 87.0729, loss: 0.3457
2023-11-10 21:11:16,747 - mmseg - INFO - Iter [53200/160000]	lr: 7.511e-05, eta: 20:47:52, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2992, decode.acc_seg: 88.2238, loss: 0.2992
2023-11-10 21:11:52,607 - mmseg - INFO - Iter [53250/160000]	lr: 7.507e-05, eta: 20:47:18, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3270, decode.acc_seg: 86.8498, loss: 0.3270
2023-11-10 21:12:28,157 - mmseg - INFO - Iter [53300/160000]	lr: 7.503e-05, eta: 20:46:44, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3257, decode.acc_seg: 87.5328, loss: 0.3257
2023-11-10 21:13:01,406 - mmseg - INFO - Iter [53350/160000]	lr: 7.499e-05, eta: 20:46:05, time: 0.666, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3275, decode.acc_seg: 87.4741, loss: 0.3275
2023-11-10 21:13:33,021 - mmseg - INFO - Iter [53400/160000]	lr: 7.494e-05, eta: 20:45:24, time: 0.632, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3236, decode.acc_seg: 87.6166, loss: 0.3236
2023-11-10 21:14:04,989 - mmseg - INFO - Iter [53450/160000]	lr: 7.490e-05, eta: 20:44:42, time: 0.639, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3065, decode.acc_seg: 87.6219, loss: 0.3065
2023-11-10 21:14:36,355 - mmseg - INFO - Iter [53500/160000]	lr: 7.486e-05, eta: 20:44:00, time: 0.627, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2992, decode.acc_seg: 88.1803, loss: 0.2992
2023-11-10 21:15:08,690 - mmseg - INFO - Iter [53550/160000]	lr: 7.482e-05, eta: 20:43:20, time: 0.647, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2997, decode.acc_seg: 88.0224, loss: 0.2997
2023-11-10 21:15:42,605 - mmseg - INFO - Iter [53600/160000]	lr: 7.477e-05, eta: 20:42:42, time: 0.677, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2964, decode.acc_seg: 88.5425, loss: 0.2964
2023-11-10 21:16:17,599 - mmseg - INFO - Iter [53650/160000]	lr: 7.473e-05, eta: 20:42:07, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3167, decode.acc_seg: 87.5136, loss: 0.3167
2023-11-10 21:16:53,020 - mmseg - INFO - Iter [53700/160000]	lr: 7.469e-05, eta: 20:41:33, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2864, decode.acc_seg: 88.5190, loss: 0.2864
2023-11-10 21:17:26,951 - mmseg - INFO - Iter [53750/160000]	lr: 7.465e-05, eta: 20:40:56, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3029, decode.acc_seg: 87.8260, loss: 0.3029
2023-11-10 21:17:59,436 - mmseg - INFO - Iter [53800/160000]	lr: 7.460e-05, eta: 20:40:15, time: 0.649, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3053, decode.acc_seg: 88.2102, loss: 0.3053
2023-11-10 21:18:34,766 - mmseg - INFO - Iter [53850/160000]	lr: 7.456e-05, eta: 20:39:41, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2948, decode.acc_seg: 88.0362, loss: 0.2948
2023-11-10 21:19:09,810 - mmseg - INFO - Iter [53900/160000]	lr: 7.452e-05, eta: 20:39:06, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3095, decode.acc_seg: 87.9256, loss: 0.3095
2023-11-10 21:19:44,186 - mmseg - INFO - Iter [53950/160000]	lr: 7.447e-05, eta: 20:38:30, time: 0.687, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3158, decode.acc_seg: 87.2758, loss: 0.3158
2023-11-10 21:20:19,651 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 21:20:19,652 - mmseg - INFO - Iter [54000/160000]	lr: 7.443e-05, eta: 20:37:55, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3259, decode.acc_seg: 87.4286, loss: 0.3259
2023-11-10 21:20:52,992 - mmseg - INFO - Iter [54050/160000]	lr: 7.439e-05, eta: 20:37:17, time: 0.667, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3270, decode.acc_seg: 87.0692, loss: 0.3270
2023-11-10 21:21:28,629 - mmseg - INFO - Iter [54100/160000]	lr: 7.435e-05, eta: 20:36:43, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3182, decode.acc_seg: 87.5636, loss: 0.3182
2023-11-10 21:22:02,536 - mmseg - INFO - Iter [54150/160000]	lr: 7.430e-05, eta: 20:36:06, time: 0.679, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3332, decode.acc_seg: 87.0069, loss: 0.3332
2023-11-10 21:22:36,710 - mmseg - INFO - Iter [54200/160000]	lr: 7.426e-05, eta: 20:35:29, time: 0.685, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3239, decode.acc_seg: 87.4619, loss: 0.3239
2023-11-10 21:23:11,646 - mmseg - INFO - Iter [54250/160000]	lr: 7.422e-05, eta: 20:34:54, time: 0.698, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3091, decode.acc_seg: 87.4283, loss: 0.3091
2023-11-10 21:23:46,609 - mmseg - INFO - Iter [54300/160000]	lr: 7.417e-05, eta: 20:34:19, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3238, decode.acc_seg: 87.3203, loss: 0.3238
2023-11-10 21:24:21,535 - mmseg - INFO - Iter [54350/160000]	lr: 7.413e-05, eta: 20:33:44, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3157, decode.acc_seg: 87.2367, loss: 0.3157
2023-11-10 21:24:56,581 - mmseg - INFO - Iter [54400/160000]	lr: 7.409e-05, eta: 20:33:09, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3206, decode.acc_seg: 87.4481, loss: 0.3206
2023-11-10 21:25:30,017 - mmseg - INFO - Iter [54450/160000]	lr: 7.405e-05, eta: 20:32:31, time: 0.670, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3158, decode.acc_seg: 87.5587, loss: 0.3158
2023-11-10 21:26:05,353 - mmseg - INFO - Iter [54500/160000]	lr: 7.400e-05, eta: 20:31:56, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3148, decode.acc_seg: 87.6787, loss: 0.3148
2023-11-10 21:26:41,126 - mmseg - INFO - Iter [54550/160000]	lr: 7.396e-05, eta: 20:31:22, time: 0.715, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3175, decode.acc_seg: 87.3461, loss: 0.3175
2023-11-10 21:27:16,242 - mmseg - INFO - Iter [54600/160000]	lr: 7.392e-05, eta: 20:30:48, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3070, decode.acc_seg: 87.7995, loss: 0.3070
2023-11-10 21:27:48,607 - mmseg - INFO - Iter [54650/160000]	lr: 7.387e-05, eta: 20:30:08, time: 0.648, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3113, decode.acc_seg: 87.7325, loss: 0.3113
2023-11-10 21:28:20,400 - mmseg - INFO - Iter [54700/160000]	lr: 7.383e-05, eta: 20:29:26, time: 0.636, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3172, decode.acc_seg: 87.7875, loss: 0.3172
2023-11-10 21:28:53,419 - mmseg - INFO - Iter [54750/160000]	lr: 7.379e-05, eta: 20:28:47, time: 0.659, data_time: 0.008, memory: 23129, decode.loss_ce: 0.3253, decode.acc_seg: 87.3387, loss: 0.3253
2023-11-10 21:29:28,239 - mmseg - INFO - Iter [54800/160000]	lr: 7.374e-05, eta: 20:28:12, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3035, decode.acc_seg: 88.2394, loss: 0.3035
2023-11-10 21:30:03,589 - mmseg - INFO - Iter [54850/160000]	lr: 7.370e-05, eta: 20:27:37, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3333, decode.acc_seg: 87.0954, loss: 0.3333
2023-11-10 21:30:38,744 - mmseg - INFO - Iter [54900/160000]	lr: 7.366e-05, eta: 20:27:03, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3022, decode.acc_seg: 88.3204, loss: 0.3022
2023-11-10 21:31:14,363 - mmseg - INFO - Iter [54950/160000]	lr: 7.361e-05, eta: 20:26:29, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3194, decode.acc_seg: 87.3073, loss: 0.3194
2023-11-10 21:31:48,235 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 21:31:48,235 - mmseg - INFO - Iter [55000/160000]	lr: 7.357e-05, eta: 20:25:52, time: 0.678, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3200, decode.acc_seg: 87.6771, loss: 0.3200
2023-11-10 21:32:23,589 - mmseg - INFO - Iter [55050/160000]	lr: 7.353e-05, eta: 20:25:17, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2947, decode.acc_seg: 88.4702, loss: 0.2947
2023-11-10 21:32:59,043 - mmseg - INFO - Iter [55100/160000]	lr: 7.348e-05, eta: 20:24:43, time: 0.709, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2956, decode.acc_seg: 88.7195, loss: 0.2956
2023-11-10 21:33:32,803 - mmseg - INFO - Iter [55150/160000]	lr: 7.344e-05, eta: 20:24:06, time: 0.676, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2965, decode.acc_seg: 88.0671, loss: 0.2965
2023-11-10 21:34:04,588 - mmseg - INFO - Iter [55200/160000]	lr: 7.340e-05, eta: 20:23:24, time: 0.634, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3066, decode.acc_seg: 87.7634, loss: 0.3066
2023-11-10 21:34:36,410 - mmseg - INFO - Iter [55250/160000]	lr: 7.335e-05, eta: 20:22:43, time: 0.638, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3054, decode.acc_seg: 87.6836, loss: 0.3054
2023-11-10 21:35:09,823 - mmseg - INFO - Iter [55300/160000]	lr: 7.331e-05, eta: 20:22:05, time: 0.669, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3225, decode.acc_seg: 87.2965, loss: 0.3225
2023-11-10 21:35:43,862 - mmseg - INFO - Iter [55350/160000]	lr: 7.327e-05, eta: 20:21:28, time: 0.680, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2947, decode.acc_seg: 88.1815, loss: 0.2947
2023-11-10 21:36:17,669 - mmseg - INFO - Iter [55400/160000]	lr: 7.322e-05, eta: 20:20:51, time: 0.677, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3044, decode.acc_seg: 87.9954, loss: 0.3044
2023-11-10 21:36:52,115 - mmseg - INFO - Iter [55450/160000]	lr: 7.318e-05, eta: 20:20:15, time: 0.688, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2918, decode.acc_seg: 88.3106, loss: 0.2918
2023-11-10 21:37:24,052 - mmseg - INFO - Iter [55500/160000]	lr: 7.314e-05, eta: 20:19:34, time: 0.640, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3072, decode.acc_seg: 87.7746, loss: 0.3072
2023-11-10 21:37:55,922 - mmseg - INFO - Iter [55550/160000]	lr: 7.309e-05, eta: 20:18:53, time: 0.638, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3095, decode.acc_seg: 88.0515, loss: 0.3095
2023-11-10 21:38:27,200 - mmseg - INFO - Iter [55600/160000]	lr: 7.305e-05, eta: 20:18:11, time: 0.625, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2995, decode.acc_seg: 87.7789, loss: 0.2995
2023-11-10 21:39:01,356 - mmseg - INFO - Iter [55650/160000]	lr: 7.301e-05, eta: 20:17:35, time: 0.682, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2880, decode.acc_seg: 88.3113, loss: 0.2880
2023-11-10 21:39:35,158 - mmseg - INFO - Iter [55700/160000]	lr: 7.296e-05, eta: 20:16:57, time: 0.677, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3208, decode.acc_seg: 87.6775, loss: 0.3208
2023-11-10 21:40:06,818 - mmseg - INFO - Iter [55750/160000]	lr: 7.292e-05, eta: 20:16:16, time: 0.633, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3017, decode.acc_seg: 87.9651, loss: 0.3017
2023-11-10 21:40:40,592 - mmseg - INFO - Iter [55800/160000]	lr: 7.288e-05, eta: 20:15:39, time: 0.674, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3014, decode.acc_seg: 88.3527, loss: 0.3014
2023-11-10 21:41:15,639 - mmseg - INFO - Iter [55850/160000]	lr: 7.283e-05, eta: 20:15:04, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3070, decode.acc_seg: 87.9755, loss: 0.3070
2023-11-10 21:41:48,493 - mmseg - INFO - Iter [55900/160000]	lr: 7.279e-05, eta: 20:14:25, time: 0.658, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2949, decode.acc_seg: 88.1235, loss: 0.2949
2023-11-10 21:42:22,284 - mmseg - INFO - Iter [55950/160000]	lr: 7.274e-05, eta: 20:13:48, time: 0.675, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2886, decode.acc_seg: 88.6580, loss: 0.2886
2023-11-10 21:42:57,243 - mmseg - INFO - Saving checkpoint at 56000 iterations
2023-11-10 21:43:01,931 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 21:43:01,931 - mmseg - INFO - Iter [56000/160000]	lr: 7.270e-05, eta: 20:13:21, time: 0.794, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2946, decode.acc_seg: 88.0739, loss: 0.2946
2023-11-10 21:44:32,066 - mmseg - INFO - per class results:
2023-11-10 21:44:32,080 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 76.14 | 88.01 |
|       building      | 81.95 | 92.33 |
|         sky         | 94.38 | 97.21 |
|        floor        | 80.65 | 89.62 |
|         tree        | 74.01 | 87.36 |
|       ceiling       |  83.1 | 90.78 |
|         road        | 82.35 |  88.1 |
|         bed         | 86.96 | 93.39 |
|      windowpane     | 60.47 | 75.27 |
|        grass        | 67.88 | 81.01 |
|       cabinet       | 56.71 | 67.04 |
|       sidewalk      | 60.68 | 85.01 |
|        person       |  78.0 | 93.19 |
|        earth        | 37.25 | 52.19 |
|         door        | 48.45 | 58.93 |
|        table        | 54.02 | 71.16 |
|       mountain      | 61.09 | 75.22 |
|        plant        | 50.54 | 58.87 |
|       curtain       | 73.55 | 85.15 |
|        chair        |  49.7 | 61.93 |
|         car         |  81.7 | 91.84 |
|        water        | 49.26 | 58.81 |
|       painting      | 65.78 | 83.57 |
|         sofa        | 59.44 | 80.04 |
|        shelf        | 37.57 | 47.97 |
|        house        | 35.21 | 44.63 |
|         sea         | 49.19 | 78.95 |
|        mirror       | 59.05 | 64.53 |
|         rug         | 63.92 | 77.88 |
|        field        | 36.09 | 53.38 |
|       armchair      | 33.91 | 52.31 |
|         seat        | 60.37 | 73.06 |
|        fence        | 47.03 | 65.31 |
|         desk        | 45.98 | 59.35 |
|         rock        | 40.14 | 56.43 |
|       wardrobe      | 40.88 | 75.76 |
|         lamp        | 61.03 | 71.47 |
|       bathtub       | 75.55 |  82.4 |
|       railing       | 31.37 |  44.5 |
|       cushion       | 54.15 | 69.37 |
|         base        |  33.4 | 48.93 |
|         box         | 24.48 | 30.99 |
|        column       | 43.45 | 50.64 |
|      signboard      | 35.69 | 51.66 |
|   chest of drawers  | 47.72 |  65.3 |
|       counter       | 31.91 | 36.72 |
|         sand        | 31.69 | 62.89 |
|         sink        | 66.51 | 76.89 |
|      skyscraper     | 50.93 | 67.08 |
|      fireplace      |  70.1 | 89.43 |
|     refrigerator    | 74.06 | 89.68 |
|      grandstand     | 41.05 |  64.5 |
|         path        | 24.34 | 37.76 |
|        stairs       | 30.32 | 39.62 |
|        runway       | 68.91 | 89.82 |
|         case        | 53.12 |  70.2 |
|      pool table     | 92.24 | 96.01 |
|        pillow       | 58.33 | 75.56 |
|     screen door     | 60.11 | 83.57 |
|       stairway      | 36.21 | 53.56 |
|        river        | 15.33 | 28.07 |
|        bridge       | 57.01 | 86.91 |
|       bookcase      | 34.29 | 60.46 |
|        blind        | 35.71 | 40.12 |
|     coffee table    | 49.35 | 75.63 |
|        toilet       |  84.1 | 91.69 |
|        flower       | 30.54 | 45.94 |
|         book        | 40.61 |  55.2 |
|         hill        |  4.23 |  7.98 |
|        bench        |  40.4 | 49.84 |
|      countertop     | 62.57 | 82.17 |
|        stove        | 65.03 | 85.08 |
|         palm        | 43.53 | 76.64 |
|    kitchen island   | 33.53 | 82.49 |
|       computer      | 64.61 | 79.33 |
|     swivel chair    | 42.22 | 76.52 |
|         boat        | 43.54 | 49.14 |
|         bar         | 38.23 | 56.47 |
|    arcade machine   | 63.33 |  69.5 |
|        hovel        | 35.46 | 39.17 |
|         bus         | 81.99 | 91.72 |
|        towel        |  61.6 | 76.97 |
|        light        | 49.74 | 55.93 |
|        truck        | 13.73 |  15.2 |
|        tower        | 45.03 | 58.02 |
|      chandelier     | 65.54 | 76.78 |
|        awning       | 27.33 | 31.65 |
|     streetlight     | 22.86 | 32.23 |
|        booth        | 38.46 | 65.67 |
| television receiver | 67.11 |  78.2 |
|       airplane      | 50.74 |  64.9 |
|      dirt track     |  1.51 |  2.07 |
|       apparel       | 34.14 | 48.11 |
|         pole        | 18.04 | 22.83 |
|         land        |  0.0  |  0.0  |
|      bannister      |  14.1 | 19.46 |
|      escalator      | 33.41 |  39.7 |
|       ottoman       | 37.02 |  68.7 |
|        bottle       | 34.03 | 57.54 |
|        buffet       |  56.1 | 59.95 |
|        poster       | 26.95 | 48.23 |
|        stage        |  17.2 | 33.78 |
|         van         | 36.77 | 60.84 |
|         ship        |  5.03 |  7.29 |
|       fountain      | 32.67 | 34.11 |
|    conveyer belt    | 63.91 | 89.97 |
|        canopy       | 26.39 | 36.48 |
|        washer       | 71.65 |  75.0 |
|      plaything      | 19.74 | 35.59 |
|    swimming pool    | 47.44 | 82.42 |
|        stool        | 30.61 | 41.65 |
|        barrel       | 48.33 | 64.94 |
|        basket       | 34.92 | 40.19 |
|      waterfall      |  76.1 |  89.7 |
|         tent        | 83.28 | 98.09 |
|         bag         |  9.89 |  11.4 |
|       minibike      |  54.3 | 67.03 |
|        cradle       | 82.69 | 97.12 |
|         oven        | 23.45 | 42.19 |
|         ball        | 45.75 | 60.04 |
|         food        | 52.85 | 59.33 |
|         step        |  8.45 | 10.98 |
|         tank        | 46.08 | 47.55 |
|      trade name     | 14.17 | 14.57 |
|      microwave      |  62.4 | 71.36 |
|         pot         | 35.43 | 38.51 |
|        animal       | 52.99 | 57.39 |
|       bicycle       | 57.56 | 76.67 |
|         lake        | 29.72 | 31.29 |
|      dishwasher     | 45.48 | 57.61 |
|        screen       | 44.02 | 63.49 |
|       blanket       |  13.8 | 16.04 |
|      sculpture      | 51.55 | 69.72 |
|         hood        | 50.15 |  62.0 |
|        sconce       | 36.51 | 41.23 |
|         vase        | 35.58 | 47.09 |
|    traffic light    | 24.27 |  40.8 |
|         tray        |  2.8  |  4.84 |
|        ashcan       | 38.65 | 48.03 |
|         fan         | 53.16 | 70.33 |
|         pier        |  35.0 | 37.85 |
|      crt screen     |  6.0  | 29.12 |
|        plate        | 36.01 | 43.95 |
|       monitor       |  6.02 |  8.12 |
|    bulletin board   | 43.98 | 59.03 |
|        shower       |  2.55 |  6.85 |
|       radiator      | 63.42 | 70.97 |
|        glass        |  7.37 |  7.8  |
|        clock        |  24.9 |  28.8 |
|         flag        | 40.96 | 42.73 |
+---------------------+-------+-------+
2023-11-10 21:44:32,080 - mmseg - INFO - Summary:
2023-11-10 21:44:32,080 - mmseg - INFO - 
+-------+------+-------+
|  aAcc | mIoU |  mAcc |
+-------+------+-------+
| 81.81 | 45.7 | 58.59 |
+-------+------+-------+
2023-11-10 21:44:32,101 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 21:44:32,101 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8181, mIoU: 0.4570, mAcc: 0.5859, IoU.wall: 0.7614, IoU.building: 0.8195, IoU.sky: 0.9438, IoU.floor: 0.8065, IoU.tree: 0.7401, IoU.ceiling: 0.8310, IoU.road: 0.8235, IoU.bed : 0.8696, IoU.windowpane: 0.6047, IoU.grass: 0.6788, IoU.cabinet: 0.5671, IoU.sidewalk: 0.6068, IoU.person: 0.7800, IoU.earth: 0.3725, IoU.door: 0.4845, IoU.table: 0.5402, IoU.mountain: 0.6109, IoU.plant: 0.5054, IoU.curtain: 0.7355, IoU.chair: 0.4970, IoU.car: 0.8170, IoU.water: 0.4926, IoU.painting: 0.6578, IoU.sofa: 0.5944, IoU.shelf: 0.3757, IoU.house: 0.3521, IoU.sea: 0.4919, IoU.mirror: 0.5905, IoU.rug: 0.6392, IoU.field: 0.3609, IoU.armchair: 0.3391, IoU.seat: 0.6037, IoU.fence: 0.4703, IoU.desk: 0.4598, IoU.rock: 0.4014, IoU.wardrobe: 0.4088, IoU.lamp: 0.6103, IoU.bathtub: 0.7555, IoU.railing: 0.3137, IoU.cushion: 0.5415, IoU.base: 0.3340, IoU.box: 0.2448, IoU.column: 0.4345, IoU.signboard: 0.3569, IoU.chest of drawers: 0.4772, IoU.counter: 0.3191, IoU.sand: 0.3169, IoU.sink: 0.6651, IoU.skyscraper: 0.5093, IoU.fireplace: 0.7010, IoU.refrigerator: 0.7406, IoU.grandstand: 0.4105, IoU.path: 0.2434, IoU.stairs: 0.3032, IoU.runway: 0.6891, IoU.case: 0.5312, IoU.pool table: 0.9224, IoU.pillow: 0.5833, IoU.screen door: 0.6011, IoU.stairway: 0.3621, IoU.river: 0.1533, IoU.bridge: 0.5701, IoU.bookcase: 0.3429, IoU.blind: 0.3571, IoU.coffee table: 0.4935, IoU.toilet: 0.8410, IoU.flower: 0.3054, IoU.book: 0.4061, IoU.hill: 0.0423, IoU.bench: 0.4040, IoU.countertop: 0.6257, IoU.stove: 0.6503, IoU.palm: 0.4353, IoU.kitchen island: 0.3353, IoU.computer: 0.6461, IoU.swivel chair: 0.4222, IoU.boat: 0.4354, IoU.bar: 0.3823, IoU.arcade machine: 0.6333, IoU.hovel: 0.3546, IoU.bus: 0.8199, IoU.towel: 0.6160, IoU.light: 0.4974, IoU.truck: 0.1373, IoU.tower: 0.4503, IoU.chandelier: 0.6554, IoU.awning: 0.2733, IoU.streetlight: 0.2286, IoU.booth: 0.3846, IoU.television receiver: 0.6711, IoU.airplane: 0.5074, IoU.dirt track: 0.0151, IoU.apparel: 0.3414, IoU.pole: 0.1804, IoU.land: 0.0000, IoU.bannister: 0.1410, IoU.escalator: 0.3341, IoU.ottoman: 0.3702, IoU.bottle: 0.3403, IoU.buffet: 0.5610, IoU.poster: 0.2695, IoU.stage: 0.1720, IoU.van: 0.3677, IoU.ship: 0.0503, IoU.fountain: 0.3267, IoU.conveyer belt: 0.6391, IoU.canopy: 0.2639, IoU.washer: 0.7165, IoU.plaything: 0.1974, IoU.swimming pool: 0.4744, IoU.stool: 0.3061, IoU.barrel: 0.4833, IoU.basket: 0.3492, IoU.waterfall: 0.7610, IoU.tent: 0.8328, IoU.bag: 0.0989, IoU.minibike: 0.5430, IoU.cradle: 0.8269, IoU.oven: 0.2345, IoU.ball: 0.4575, IoU.food: 0.5285, IoU.step: 0.0845, IoU.tank: 0.4608, IoU.trade name: 0.1417, IoU.microwave: 0.6240, IoU.pot: 0.3543, IoU.animal: 0.5299, IoU.bicycle: 0.5756, IoU.lake: 0.2972, IoU.dishwasher: 0.4548, IoU.screen: 0.4402, IoU.blanket: 0.1380, IoU.sculpture: 0.5155, IoU.hood: 0.5015, IoU.sconce: 0.3651, IoU.vase: 0.3558, IoU.traffic light: 0.2427, IoU.tray: 0.0280, IoU.ashcan: 0.3865, IoU.fan: 0.5316, IoU.pier: 0.3500, IoU.crt screen: 0.0600, IoU.plate: 0.3601, IoU.monitor: 0.0602, IoU.bulletin board: 0.4398, IoU.shower: 0.0255, IoU.radiator: 0.6342, IoU.glass: 0.0737, IoU.clock: 0.2490, IoU.flag: 0.4096, Acc.wall: 0.8801, Acc.building: 0.9233, Acc.sky: 0.9721, Acc.floor: 0.8962, Acc.tree: 0.8736, Acc.ceiling: 0.9078, Acc.road: 0.8810, Acc.bed : 0.9339, Acc.windowpane: 0.7527, Acc.grass: 0.8101, Acc.cabinet: 0.6704, Acc.sidewalk: 0.8501, Acc.person: 0.9319, Acc.earth: 0.5219, Acc.door: 0.5893, Acc.table: 0.7116, Acc.mountain: 0.7522, Acc.plant: 0.5887, Acc.curtain: 0.8515, Acc.chair: 0.6193, Acc.car: 0.9184, Acc.water: 0.5881, Acc.painting: 0.8357, Acc.sofa: 0.8004, Acc.shelf: 0.4797, Acc.house: 0.4463, Acc.sea: 0.7895, Acc.mirror: 0.6453, Acc.rug: 0.7788, Acc.field: 0.5338, Acc.armchair: 0.5231, Acc.seat: 0.7306, Acc.fence: 0.6531, Acc.desk: 0.5935, Acc.rock: 0.5643, Acc.wardrobe: 0.7576, Acc.lamp: 0.7147, Acc.bathtub: 0.8240, Acc.railing: 0.4450, Acc.cushion: 0.6937, Acc.base: 0.4893, Acc.box: 0.3099, Acc.column: 0.5064, Acc.signboard: 0.5166, Acc.chest of drawers: 0.6530, Acc.counter: 0.3672, Acc.sand: 0.6289, Acc.sink: 0.7689, Acc.skyscraper: 0.6708, Acc.fireplace: 0.8943, Acc.refrigerator: 0.8968, Acc.grandstand: 0.6450, Acc.path: 0.3776, Acc.stairs: 0.3962, Acc.runway: 0.8982, Acc.case: 0.7020, Acc.pool table: 0.9601, Acc.pillow: 0.7556, Acc.screen door: 0.8357, Acc.stairway: 0.5356, Acc.river: 0.2807, Acc.bridge: 0.8691, Acc.bookcase: 0.6046, Acc.blind: 0.4012, Acc.coffee table: 0.7563, Acc.toilet: 0.9169, Acc.flower: 0.4594, Acc.book: 0.5520, Acc.hill: 0.0798, Acc.bench: 0.4984, Acc.countertop: 0.8217, Acc.stove: 0.8508, Acc.palm: 0.7664, Acc.kitchen island: 0.8249, Acc.computer: 0.7933, Acc.swivel chair: 0.7652, Acc.boat: 0.4914, Acc.bar: 0.5647, Acc.arcade machine: 0.6950, Acc.hovel: 0.3917, Acc.bus: 0.9172, Acc.towel: 0.7697, Acc.light: 0.5593, Acc.truck: 0.1520, Acc.tower: 0.5802, Acc.chandelier: 0.7678, Acc.awning: 0.3165, Acc.streetlight: 0.3223, Acc.booth: 0.6567, Acc.television receiver: 0.7820, Acc.airplane: 0.6490, Acc.dirt track: 0.0207, Acc.apparel: 0.4811, Acc.pole: 0.2283, Acc.land: 0.0000, Acc.bannister: 0.1946, Acc.escalator: 0.3970, Acc.ottoman: 0.6870, Acc.bottle: 0.5754, Acc.buffet: 0.5995, Acc.poster: 0.4823, Acc.stage: 0.3378, Acc.van: 0.6084, Acc.ship: 0.0729, Acc.fountain: 0.3411, Acc.conveyer belt: 0.8997, Acc.canopy: 0.3648, Acc.washer: 0.7500, Acc.plaything: 0.3559, Acc.swimming pool: 0.8242, Acc.stool: 0.4165, Acc.barrel: 0.6494, Acc.basket: 0.4019, Acc.waterfall: 0.8970, Acc.tent: 0.9809, Acc.bag: 0.1140, Acc.minibike: 0.6703, Acc.cradle: 0.9712, Acc.oven: 0.4219, Acc.ball: 0.6004, Acc.food: 0.5933, Acc.step: 0.1098, Acc.tank: 0.4755, Acc.trade name: 0.1457, Acc.microwave: 0.7136, Acc.pot: 0.3851, Acc.animal: 0.5739, Acc.bicycle: 0.7667, Acc.lake: 0.3129, Acc.dishwasher: 0.5761, Acc.screen: 0.6349, Acc.blanket: 0.1604, Acc.sculpture: 0.6972, Acc.hood: 0.6200, Acc.sconce: 0.4123, Acc.vase: 0.4709, Acc.traffic light: 0.4080, Acc.tray: 0.0484, Acc.ashcan: 0.4803, Acc.fan: 0.7033, Acc.pier: 0.3785, Acc.crt screen: 0.2912, Acc.plate: 0.4395, Acc.monitor: 0.0812, Acc.bulletin board: 0.5903, Acc.shower: 0.0685, Acc.radiator: 0.7097, Acc.glass: 0.0780, Acc.clock: 0.2880, Acc.flag: 0.4273
2023-11-10 21:45:04,084 - mmseg - INFO - Iter [56050/160000]	lr: 7.266e-05, eta: 20:15:28, time: 2.442, data_time: 1.812, memory: 23129, decode.loss_ce: 0.2809, decode.acc_seg: 88.1861, loss: 0.2809
2023-11-10 21:45:35,675 - mmseg - INFO - Iter [56100/160000]	lr: 7.261e-05, eta: 20:14:46, time: 0.633, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3166, decode.acc_seg: 87.3496, loss: 0.3166
2023-11-10 21:46:10,082 - mmseg - INFO - Iter [56150/160000]	lr: 7.257e-05, eta: 20:14:10, time: 0.687, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2853, decode.acc_seg: 88.3797, loss: 0.2853
2023-11-10 21:46:45,429 - mmseg - INFO - Iter [56200/160000]	lr: 7.253e-05, eta: 20:13:35, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2797, decode.acc_seg: 88.7945, loss: 0.2797
2023-11-10 21:47:16,998 - mmseg - INFO - Iter [56250/160000]	lr: 7.248e-05, eta: 20:12:54, time: 0.633, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3273, decode.acc_seg: 87.2718, loss: 0.3273
2023-11-10 21:47:49,062 - mmseg - INFO - Iter [56300/160000]	lr: 7.244e-05, eta: 20:12:13, time: 0.641, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3161, decode.acc_seg: 87.7585, loss: 0.3161
2023-11-10 21:48:21,747 - mmseg - INFO - Iter [56350/160000]	lr: 7.239e-05, eta: 20:11:34, time: 0.654, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3371, decode.acc_seg: 86.6110, loss: 0.3371
2023-11-10 21:48:54,268 - mmseg - INFO - Iter [56400/160000]	lr: 7.235e-05, eta: 20:10:54, time: 0.649, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3197, decode.acc_seg: 87.5927, loss: 0.3197
2023-11-10 21:49:28,969 - mmseg - INFO - Iter [56450/160000]	lr: 7.231e-05, eta: 20:10:18, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3062, decode.acc_seg: 87.9199, loss: 0.3062
2023-11-10 21:50:03,742 - mmseg - INFO - Iter [56500/160000]	lr: 7.226e-05, eta: 20:09:43, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3209, decode.acc_seg: 87.5356, loss: 0.3209
2023-11-10 21:50:37,754 - mmseg - INFO - Iter [56550/160000]	lr: 7.222e-05, eta: 20:09:06, time: 0.680, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2937, decode.acc_seg: 88.4844, loss: 0.2937
2023-11-10 21:51:12,842 - mmseg - INFO - Iter [56600/160000]	lr: 7.217e-05, eta: 20:08:31, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3147, decode.acc_seg: 87.7519, loss: 0.3147
2023-11-10 21:51:48,226 - mmseg - INFO - Iter [56650/160000]	lr: 7.213e-05, eta: 20:07:56, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3087, decode.acc_seg: 87.7183, loss: 0.3087
2023-11-10 21:52:24,030 - mmseg - INFO - Iter [56700/160000]	lr: 7.209e-05, eta: 20:07:23, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3100, decode.acc_seg: 87.7482, loss: 0.3100
2023-11-10 21:52:59,849 - mmseg - INFO - Iter [56750/160000]	lr: 7.204e-05, eta: 20:06:49, time: 0.716, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2829, decode.acc_seg: 88.5308, loss: 0.2829
2023-11-10 21:53:35,099 - mmseg - INFO - Iter [56800/160000]	lr: 7.200e-05, eta: 20:06:14, time: 0.706, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3093, decode.acc_seg: 87.9191, loss: 0.3093
2023-11-10 21:54:10,258 - mmseg - INFO - Iter [56850/160000]	lr: 7.195e-05, eta: 20:05:39, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3081, decode.acc_seg: 87.7523, loss: 0.3081
2023-11-10 21:54:45,582 - mmseg - INFO - Iter [56900/160000]	lr: 7.191e-05, eta: 20:05:05, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3144, decode.acc_seg: 87.0824, loss: 0.3144
2023-11-10 21:55:21,129 - mmseg - INFO - Iter [56950/160000]	lr: 7.187e-05, eta: 20:04:30, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3054, decode.acc_seg: 87.8012, loss: 0.3054
2023-11-10 21:55:56,458 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 21:55:56,458 - mmseg - INFO - Iter [57000/160000]	lr: 7.182e-05, eta: 20:03:56, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3037, decode.acc_seg: 87.7604, loss: 0.3037
2023-11-10 21:56:31,056 - mmseg - INFO - Iter [57050/160000]	lr: 7.178e-05, eta: 20:03:20, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2971, decode.acc_seg: 88.1317, loss: 0.2971
2023-11-10 21:57:05,429 - mmseg - INFO - Iter [57100/160000]	lr: 7.173e-05, eta: 20:02:44, time: 0.687, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2980, decode.acc_seg: 88.3317, loss: 0.2980
2023-11-10 21:57:40,887 - mmseg - INFO - Iter [57150/160000]	lr: 7.169e-05, eta: 20:02:09, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3203, decode.acc_seg: 87.4708, loss: 0.3203
2023-11-10 21:58:16,821 - mmseg - INFO - Iter [57200/160000]	lr: 7.164e-05, eta: 20:01:36, time: 0.719, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2984, decode.acc_seg: 87.9299, loss: 0.2984
2023-11-10 21:58:52,402 - mmseg - INFO - Iter [57250/160000]	lr: 7.160e-05, eta: 20:01:02, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3065, decode.acc_seg: 87.8470, loss: 0.3065
2023-11-10 21:59:27,548 - mmseg - INFO - Iter [57300/160000]	lr: 7.156e-05, eta: 20:00:27, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3072, decode.acc_seg: 88.0741, loss: 0.3072
2023-11-10 22:00:00,702 - mmseg - INFO - Iter [57350/160000]	lr: 7.151e-05, eta: 19:59:48, time: 0.664, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3285, decode.acc_seg: 87.4634, loss: 0.3285
2023-11-10 22:00:35,072 - mmseg - INFO - Iter [57400/160000]	lr: 7.147e-05, eta: 19:59:12, time: 0.687, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2948, decode.acc_seg: 88.4404, loss: 0.2948
2023-11-10 22:01:09,724 - mmseg - INFO - Iter [57450/160000]	lr: 7.142e-05, eta: 19:58:36, time: 0.692, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2902, decode.acc_seg: 88.3367, loss: 0.2902
2023-11-10 22:01:45,319 - mmseg - INFO - Iter [57500/160000]	lr: 7.138e-05, eta: 19:58:02, time: 0.712, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3092, decode.acc_seg: 87.8084, loss: 0.3092
2023-11-10 22:02:21,157 - mmseg - INFO - Iter [57550/160000]	lr: 7.133e-05, eta: 19:57:28, time: 0.717, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2978, decode.acc_seg: 88.3324, loss: 0.2978
2023-11-10 22:02:56,520 - mmseg - INFO - Iter [57600/160000]	lr: 7.129e-05, eta: 19:56:54, time: 0.707, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3056, decode.acc_seg: 87.7241, loss: 0.3056
2023-11-10 22:03:32,279 - mmseg - INFO - Iter [57650/160000]	lr: 7.125e-05, eta: 19:56:20, time: 0.715, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3039, decode.acc_seg: 88.0127, loss: 0.3039
2023-11-10 22:04:07,410 - mmseg - INFO - Iter [57700/160000]	lr: 7.120e-05, eta: 19:55:45, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3074, decode.acc_seg: 87.9855, loss: 0.3074
2023-11-10 22:04:42,485 - mmseg - INFO - Iter [57750/160000]	lr: 7.116e-05, eta: 19:55:10, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2852, decode.acc_seg: 88.6042, loss: 0.2852
2023-11-10 22:05:17,692 - mmseg - INFO - Iter [57800/160000]	lr: 7.111e-05, eta: 19:54:35, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2922, decode.acc_seg: 88.3938, loss: 0.2922
2023-11-10 22:05:52,775 - mmseg - INFO - Iter [57850/160000]	lr: 7.107e-05, eta: 19:54:00, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3070, decode.acc_seg: 87.9512, loss: 0.3070
2023-11-10 22:06:28,031 - mmseg - INFO - Iter [57900/160000]	lr: 7.102e-05, eta: 19:53:25, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2985, decode.acc_seg: 88.1047, loss: 0.2985
2023-11-10 22:07:02,362 - mmseg - INFO - Iter [57950/160000]	lr: 7.098e-05, eta: 19:52:49, time: 0.688, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3013, decode.acc_seg: 88.0505, loss: 0.3013
2023-11-10 22:07:37,070 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 22:07:37,070 - mmseg - INFO - Iter [58000/160000]	lr: 7.093e-05, eta: 19:52:13, time: 0.693, data_time: 0.008, memory: 23129, decode.loss_ce: 0.3154, decode.acc_seg: 88.1657, loss: 0.3154
2023-11-10 22:08:09,613 - mmseg - INFO - Iter [58050/160000]	lr: 7.089e-05, eta: 19:51:34, time: 0.652, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2854, decode.acc_seg: 88.5679, loss: 0.2854
2023-11-10 22:08:41,326 - mmseg - INFO - Iter [58100/160000]	lr: 7.084e-05, eta: 19:50:53, time: 0.634, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3124, decode.acc_seg: 87.5909, loss: 0.3124
2023-11-10 22:09:15,882 - mmseg - INFO - Iter [58150/160000]	lr: 7.080e-05, eta: 19:50:17, time: 0.690, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2978, decode.acc_seg: 88.4776, loss: 0.2978
2023-11-10 22:09:50,792 - mmseg - INFO - Iter [58200/160000]	lr: 7.076e-05, eta: 19:49:42, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3045, decode.acc_seg: 87.9893, loss: 0.3045
2023-11-10 22:10:25,371 - mmseg - INFO - Iter [58250/160000]	lr: 7.071e-05, eta: 19:49:06, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2867, decode.acc_seg: 88.5720, loss: 0.2867
2023-11-10 22:10:58,356 - mmseg - INFO - Iter [58300/160000]	lr: 7.067e-05, eta: 19:48:27, time: 0.660, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2767, decode.acc_seg: 89.1808, loss: 0.2767
2023-11-10 22:11:33,503 - mmseg - INFO - Iter [58350/160000]	lr: 7.062e-05, eta: 19:47:52, time: 0.703, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2935, decode.acc_seg: 88.3406, loss: 0.2935
2023-11-10 22:12:06,800 - mmseg - INFO - Iter [58400/160000]	lr: 7.058e-05, eta: 19:47:14, time: 0.666, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2928, decode.acc_seg: 88.4738, loss: 0.2928
2023-11-10 22:12:41,905 - mmseg - INFO - Iter [58450/160000]	lr: 7.053e-05, eta: 19:46:39, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2812, decode.acc_seg: 88.6857, loss: 0.2812
2023-11-10 22:13:17,466 - mmseg - INFO - Iter [58500/160000]	lr: 7.049e-05, eta: 19:46:05, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2927, decode.acc_seg: 88.4048, loss: 0.2927
2023-11-10 22:13:49,335 - mmseg - INFO - Iter [58550/160000]	lr: 7.044e-05, eta: 19:45:24, time: 0.639, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2799, decode.acc_seg: 88.8170, loss: 0.2799
2023-11-10 22:14:21,386 - mmseg - INFO - Iter [58600/160000]	lr: 7.040e-05, eta: 19:44:44, time: 0.640, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2895, decode.acc_seg: 88.5386, loss: 0.2895
2023-11-10 22:14:56,649 - mmseg - INFO - Iter [58650/160000]	lr: 7.035e-05, eta: 19:44:09, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2984, decode.acc_seg: 88.6052, loss: 0.2984
2023-11-10 22:15:31,959 - mmseg - INFO - Iter [58700/160000]	lr: 7.031e-05, eta: 19:43:35, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2918, decode.acc_seg: 88.5465, loss: 0.2918
2023-11-10 22:16:07,308 - mmseg - INFO - Iter [58750/160000]	lr: 7.026e-05, eta: 19:43:00, time: 0.707, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3043, decode.acc_seg: 87.8547, loss: 0.3043
2023-11-10 22:16:42,647 - mmseg - INFO - Iter [58800/160000]	lr: 7.022e-05, eta: 19:42:26, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2856, decode.acc_seg: 88.2841, loss: 0.2856
2023-11-10 22:17:18,576 - mmseg - INFO - Iter [58850/160000]	lr: 7.017e-05, eta: 19:41:52, time: 0.719, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3106, decode.acc_seg: 87.6938, loss: 0.3106
2023-11-10 22:17:53,746 - mmseg - INFO - Iter [58900/160000]	lr: 7.013e-05, eta: 19:41:17, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2893, decode.acc_seg: 88.5202, loss: 0.2893
2023-11-10 22:18:28,602 - mmseg - INFO - Iter [58950/160000]	lr: 7.008e-05, eta: 19:40:42, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3010, decode.acc_seg: 87.9705, loss: 0.3010
2023-11-10 22:19:02,415 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 22:19:02,415 - mmseg - INFO - Iter [59000/160000]	lr: 7.004e-05, eta: 19:40:05, time: 0.675, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3129, decode.acc_seg: 87.6888, loss: 0.3129
2023-11-10 22:19:35,289 - mmseg - INFO - Iter [59050/160000]	lr: 6.999e-05, eta: 19:39:26, time: 0.658, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2867, decode.acc_seg: 88.3226, loss: 0.2867
2023-11-10 22:20:10,740 - mmseg - INFO - Iter [59100/160000]	lr: 6.995e-05, eta: 19:38:51, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3062, decode.acc_seg: 87.8001, loss: 0.3062
2023-11-10 22:20:46,427 - mmseg - INFO - Iter [59150/160000]	lr: 6.990e-05, eta: 19:38:18, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3217, decode.acc_seg: 87.4606, loss: 0.3217
2023-11-10 22:21:22,248 - mmseg - INFO - Iter [59200/160000]	lr: 6.986e-05, eta: 19:37:44, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2917, decode.acc_seg: 88.4237, loss: 0.2917
2023-11-10 22:21:57,606 - mmseg - INFO - Iter [59250/160000]	lr: 6.981e-05, eta: 19:37:09, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3014, decode.acc_seg: 87.7900, loss: 0.3014
2023-11-10 22:22:30,594 - mmseg - INFO - Iter [59300/160000]	lr: 6.977e-05, eta: 19:36:31, time: 0.661, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3158, decode.acc_seg: 87.8122, loss: 0.3158
2023-11-10 22:23:02,373 - mmseg - INFO - Iter [59350/160000]	lr: 6.972e-05, eta: 19:35:50, time: 0.636, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3005, decode.acc_seg: 88.0134, loss: 0.3005
2023-11-10 22:23:34,404 - mmseg - INFO - Iter [59400/160000]	lr: 6.968e-05, eta: 19:35:10, time: 0.641, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3104, decode.acc_seg: 87.8723, loss: 0.3104
2023-11-10 22:24:08,495 - mmseg - INFO - Iter [59450/160000]	lr: 6.963e-05, eta: 19:34:33, time: 0.681, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3076, decode.acc_seg: 87.9437, loss: 0.3076
2023-11-10 22:24:43,531 - mmseg - INFO - Iter [59500/160000]	lr: 6.959e-05, eta: 19:33:58, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3070, decode.acc_seg: 88.2383, loss: 0.3070
2023-11-10 22:25:18,530 - mmseg - INFO - Iter [59550/160000]	lr: 6.954e-05, eta: 19:33:23, time: 0.700, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2801, decode.acc_seg: 88.6036, loss: 0.2801
2023-11-10 22:25:53,407 - mmseg - INFO - Iter [59600/160000]	lr: 6.950e-05, eta: 19:32:48, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3015, decode.acc_seg: 88.3328, loss: 0.3015
2023-11-10 22:26:28,030 - mmseg - INFO - Iter [59650/160000]	lr: 6.945e-05, eta: 19:32:12, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2988, decode.acc_seg: 88.3026, loss: 0.2988
2023-11-10 22:26:59,193 - mmseg - INFO - Iter [59700/160000]	lr: 6.941e-05, eta: 19:31:31, time: 0.623, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2968, decode.acc_seg: 88.2307, loss: 0.2968
2023-11-10 22:27:33,597 - mmseg - INFO - Iter [59750/160000]	lr: 6.936e-05, eta: 19:30:54, time: 0.687, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2932, decode.acc_seg: 88.1747, loss: 0.2932
2023-11-10 22:28:09,144 - mmseg - INFO - Iter [59800/160000]	lr: 6.932e-05, eta: 19:30:20, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3011, decode.acc_seg: 88.0137, loss: 0.3011
2023-11-10 22:28:44,900 - mmseg - INFO - Iter [59850/160000]	lr: 6.927e-05, eta: 19:29:46, time: 0.715, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3014, decode.acc_seg: 88.3003, loss: 0.3014
2023-11-10 22:29:20,433 - mmseg - INFO - Iter [59900/160000]	lr: 6.923e-05, eta: 19:29:12, time: 0.711, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2956, decode.acc_seg: 88.4534, loss: 0.2956
2023-11-10 22:29:56,038 - mmseg - INFO - Iter [59950/160000]	lr: 6.918e-05, eta: 19:28:38, time: 0.712, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2861, decode.acc_seg: 88.8235, loss: 0.2861
2023-11-10 22:30:31,448 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 22:30:31,448 - mmseg - INFO - Iter [60000/160000]	lr: 6.914e-05, eta: 19:28:04, time: 0.708, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2802, decode.acc_seg: 88.8881, loss: 0.2802
2023-11-10 22:31:05,935 - mmseg - INFO - Iter [60050/160000]	lr: 6.909e-05, eta: 19:27:28, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2853, decode.acc_seg: 88.7458, loss: 0.2853
2023-11-10 22:31:40,608 - mmseg - INFO - Iter [60100/160000]	lr: 6.904e-05, eta: 19:26:52, time: 0.694, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3131, decode.acc_seg: 87.9378, loss: 0.3131
2023-11-10 22:32:13,578 - mmseg - INFO - Iter [60150/160000]	lr: 6.900e-05, eta: 19:26:14, time: 0.659, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2857, decode.acc_seg: 88.3328, loss: 0.2857
2023-11-10 22:32:46,231 - mmseg - INFO - Iter [60200/160000]	lr: 6.895e-05, eta: 19:25:34, time: 0.652, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2894, decode.acc_seg: 88.5497, loss: 0.2894
2023-11-10 22:33:21,406 - mmseg - INFO - Iter [60250/160000]	lr: 6.891e-05, eta: 19:25:00, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3076, decode.acc_seg: 87.9977, loss: 0.3076
2023-11-10 22:33:56,625 - mmseg - INFO - Iter [60300/160000]	lr: 6.886e-05, eta: 19:24:25, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3177, decode.acc_seg: 87.4443, loss: 0.3177
2023-11-10 22:34:29,928 - mmseg - INFO - Iter [60350/160000]	lr: 6.882e-05, eta: 19:23:47, time: 0.667, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2950, decode.acc_seg: 88.4478, loss: 0.2950
2023-11-10 22:35:03,689 - mmseg - INFO - Iter [60400/160000]	lr: 6.877e-05, eta: 19:23:10, time: 0.674, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3022, decode.acc_seg: 88.2720, loss: 0.3022
2023-11-10 22:35:37,611 - mmseg - INFO - Iter [60450/160000]	lr: 6.873e-05, eta: 19:22:33, time: 0.679, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3211, decode.acc_seg: 87.5391, loss: 0.3211
2023-11-10 22:36:11,089 - mmseg - INFO - Iter [60500/160000]	lr: 6.868e-05, eta: 19:21:55, time: 0.669, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3211, decode.acc_seg: 87.3930, loss: 0.3211
2023-11-10 22:36:45,673 - mmseg - INFO - Iter [60550/160000]	lr: 6.864e-05, eta: 19:21:20, time: 0.691, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2909, decode.acc_seg: 88.6687, loss: 0.2909
2023-11-10 22:37:21,103 - mmseg - INFO - Iter [60600/160000]	lr: 6.859e-05, eta: 19:20:45, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2969, decode.acc_seg: 88.3931, loss: 0.2969
2023-11-10 22:37:57,086 - mmseg - INFO - Iter [60650/160000]	lr: 6.854e-05, eta: 19:20:12, time: 0.720, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2905, decode.acc_seg: 88.6091, loss: 0.2905
2023-11-10 22:38:32,660 - mmseg - INFO - Iter [60700/160000]	lr: 6.850e-05, eta: 19:19:38, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2984, decode.acc_seg: 88.2893, loss: 0.2984
2023-11-10 22:39:08,061 - mmseg - INFO - Iter [60750/160000]	lr: 6.845e-05, eta: 19:19:03, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2940, decode.acc_seg: 88.2857, loss: 0.2940
2023-11-10 22:39:39,711 - mmseg - INFO - Iter [60800/160000]	lr: 6.841e-05, eta: 19:18:23, time: 0.633, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2760, decode.acc_seg: 88.6604, loss: 0.2760
2023-11-10 22:40:14,791 - mmseg - INFO - Iter [60850/160000]	lr: 6.836e-05, eta: 19:17:48, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2974, decode.acc_seg: 88.0636, loss: 0.2974
2023-11-10 22:40:49,666 - mmseg - INFO - Iter [60900/160000]	lr: 6.832e-05, eta: 19:17:12, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3017, decode.acc_seg: 88.0609, loss: 0.3017
2023-11-10 22:41:23,821 - mmseg - INFO - Iter [60950/160000]	lr: 6.827e-05, eta: 19:16:36, time: 0.684, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2908, decode.acc_seg: 88.5908, loss: 0.2908
2023-11-10 22:41:56,700 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 22:41:56,700 - mmseg - INFO - Iter [61000/160000]	lr: 6.822e-05, eta: 19:15:57, time: 0.658, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3024, decode.acc_seg: 88.2931, loss: 0.3024
2023-11-10 22:42:31,549 - mmseg - INFO - Iter [61050/160000]	lr: 6.818e-05, eta: 19:15:22, time: 0.697, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2783, decode.acc_seg: 88.8353, loss: 0.2783
2023-11-10 22:43:06,080 - mmseg - INFO - Iter [61100/160000]	lr: 6.813e-05, eta: 19:14:46, time: 0.689, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2716, decode.acc_seg: 89.0186, loss: 0.2716
2023-11-10 22:43:41,504 - mmseg - INFO - Iter [61150/160000]	lr: 6.809e-05, eta: 19:14:12, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2883, decode.acc_seg: 88.8142, loss: 0.2883
2023-11-10 22:44:15,399 - mmseg - INFO - Iter [61200/160000]	lr: 6.804e-05, eta: 19:13:35, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2774, decode.acc_seg: 89.0791, loss: 0.2774
2023-11-10 22:44:48,826 - mmseg - INFO - Iter [61250/160000]	lr: 6.800e-05, eta: 19:12:57, time: 0.667, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2892, decode.acc_seg: 88.5952, loss: 0.2892
2023-11-10 22:45:23,851 - mmseg - INFO - Iter [61300/160000]	lr: 6.795e-05, eta: 19:12:22, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2732, decode.acc_seg: 89.0051, loss: 0.2732
2023-11-10 22:45:58,665 - mmseg - INFO - Iter [61350/160000]	lr: 6.790e-05, eta: 19:11:47, time: 0.697, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2985, decode.acc_seg: 88.4973, loss: 0.2985
2023-11-10 22:46:33,454 - mmseg - INFO - Iter [61400/160000]	lr: 6.786e-05, eta: 19:11:12, time: 0.695, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2823, decode.acc_seg: 88.8875, loss: 0.2823
2023-11-10 22:47:07,369 - mmseg - INFO - Iter [61450/160000]	lr: 6.781e-05, eta: 19:10:35, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2996, decode.acc_seg: 87.9381, loss: 0.2996
2023-11-10 22:47:39,638 - mmseg - INFO - Iter [61500/160000]	lr: 6.777e-05, eta: 19:09:55, time: 0.646, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2707, decode.acc_seg: 89.1269, loss: 0.2707
2023-11-10 22:48:14,585 - mmseg - INFO - Iter [61550/160000]	lr: 6.772e-05, eta: 19:09:20, time: 0.698, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3017, decode.acc_seg: 87.7869, loss: 0.3017
2023-11-10 22:48:50,013 - mmseg - INFO - Iter [61600/160000]	lr: 6.767e-05, eta: 19:08:46, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2940, decode.acc_seg: 88.0828, loss: 0.2940
2023-11-10 22:49:25,382 - mmseg - INFO - Iter [61650/160000]	lr: 6.763e-05, eta: 19:08:11, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2870, decode.acc_seg: 88.5325, loss: 0.2870
2023-11-10 22:49:59,311 - mmseg - INFO - Iter [61700/160000]	lr: 6.758e-05, eta: 19:07:35, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2834, decode.acc_seg: 88.6845, loss: 0.2834
2023-11-10 22:50:35,418 - mmseg - INFO - Iter [61750/160000]	lr: 6.754e-05, eta: 19:07:01, time: 0.722, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2960, decode.acc_seg: 87.9432, loss: 0.2960
2023-11-10 22:51:10,645 - mmseg - INFO - Iter [61800/160000]	lr: 6.749e-05, eta: 19:06:27, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2911, decode.acc_seg: 88.3002, loss: 0.2911
2023-11-10 22:51:44,563 - mmseg - INFO - Iter [61850/160000]	lr: 6.744e-05, eta: 19:05:50, time: 0.679, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2974, decode.acc_seg: 88.1714, loss: 0.2974
2023-11-10 22:52:16,441 - mmseg - INFO - Iter [61900/160000]	lr: 6.740e-05, eta: 19:05:10, time: 0.637, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2856, decode.acc_seg: 88.4367, loss: 0.2856
2023-11-10 22:52:48,292 - mmseg - INFO - Iter [61950/160000]	lr: 6.735e-05, eta: 19:04:30, time: 0.638, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2833, decode.acc_seg: 88.8362, loss: 0.2833
2023-11-10 22:53:19,600 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 22:53:19,600 - mmseg - INFO - Iter [62000/160000]	lr: 6.731e-05, eta: 19:03:49, time: 0.626, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2883, decode.acc_seg: 88.4024, loss: 0.2883
2023-11-10 22:53:52,859 - mmseg - INFO - Iter [62050/160000]	lr: 6.726e-05, eta: 19:03:11, time: 0.664, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2876, decode.acc_seg: 88.5764, loss: 0.2876
2023-11-10 22:54:27,903 - mmseg - INFO - Iter [62100/160000]	lr: 6.721e-05, eta: 19:02:36, time: 0.702, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2968, decode.acc_seg: 88.6350, loss: 0.2968
2023-11-10 22:55:03,160 - mmseg - INFO - Iter [62150/160000]	lr: 6.717e-05, eta: 19:02:01, time: 0.704, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2866, decode.acc_seg: 88.4170, loss: 0.2866
2023-11-10 22:55:38,048 - mmseg - INFO - Iter [62200/160000]	lr: 6.712e-05, eta: 19:01:26, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2930, decode.acc_seg: 88.5001, loss: 0.2930
2023-11-10 22:56:13,024 - mmseg - INFO - Iter [62250/160000]	lr: 6.708e-05, eta: 19:00:51, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2880, decode.acc_seg: 88.6081, loss: 0.2880
2023-11-10 22:56:48,473 - mmseg - INFO - Iter [62300/160000]	lr: 6.703e-05, eta: 19:00:17, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2909, decode.acc_seg: 88.8451, loss: 0.2909
2023-11-10 22:57:23,550 - mmseg - INFO - Iter [62350/160000]	lr: 6.698e-05, eta: 18:59:42, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2831, decode.acc_seg: 89.0973, loss: 0.2831
2023-11-10 22:57:59,233 - mmseg - INFO - Iter [62400/160000]	lr: 6.694e-05, eta: 18:59:08, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2897, decode.acc_seg: 88.4402, loss: 0.2897
2023-11-10 22:58:33,659 - mmseg - INFO - Iter [62450/160000]	lr: 6.689e-05, eta: 18:58:32, time: 0.689, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2897, decode.acc_seg: 88.6206, loss: 0.2897
2023-11-10 22:59:07,923 - mmseg - INFO - Iter [62500/160000]	lr: 6.685e-05, eta: 18:57:56, time: 0.685, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2727, decode.acc_seg: 88.9792, loss: 0.2727
2023-11-10 22:59:42,919 - mmseg - INFO - Iter [62550/160000]	lr: 6.680e-05, eta: 18:57:21, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2894, decode.acc_seg: 88.9737, loss: 0.2894
2023-11-10 23:00:17,878 - mmseg - INFO - Iter [62600/160000]	lr: 6.675e-05, eta: 18:56:46, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2890, decode.acc_seg: 88.5058, loss: 0.2890
2023-11-10 23:00:50,737 - mmseg - INFO - Iter [62650/160000]	lr: 6.671e-05, eta: 18:56:07, time: 0.658, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2815, decode.acc_seg: 88.9709, loss: 0.2815
2023-11-10 23:01:23,403 - mmseg - INFO - Iter [62700/160000]	lr: 6.666e-05, eta: 18:55:29, time: 0.653, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3019, decode.acc_seg: 88.2979, loss: 0.3019
2023-11-10 23:01:56,636 - mmseg - INFO - Iter [62750/160000]	lr: 6.661e-05, eta: 18:54:51, time: 0.665, data_time: 0.008, memory: 23129, decode.loss_ce: 0.3021, decode.acc_seg: 87.9483, loss: 0.3021
2023-11-10 23:02:29,486 - mmseg - INFO - Iter [62800/160000]	lr: 6.657e-05, eta: 18:54:13, time: 0.657, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2782, decode.acc_seg: 88.9204, loss: 0.2782
2023-11-10 23:03:02,869 - mmseg - INFO - Iter [62850/160000]	lr: 6.652e-05, eta: 18:53:35, time: 0.667, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2931, decode.acc_seg: 88.5966, loss: 0.2931
2023-11-10 23:03:34,696 - mmseg - INFO - Iter [62900/160000]	lr: 6.648e-05, eta: 18:52:55, time: 0.638, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2797, decode.acc_seg: 88.9905, loss: 0.2797
2023-11-10 23:04:09,480 - mmseg - INFO - Iter [62950/160000]	lr: 6.643e-05, eta: 18:52:20, time: 0.695, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2815, decode.acc_seg: 88.6409, loss: 0.2815
2023-11-10 23:04:44,426 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 23:04:44,427 - mmseg - INFO - Iter [63000/160000]	lr: 6.638e-05, eta: 18:51:45, time: 0.699, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2843, decode.acc_seg: 88.2063, loss: 0.2843
2023-11-10 23:05:18,588 - mmseg - INFO - Iter [63050/160000]	lr: 6.634e-05, eta: 18:51:08, time: 0.684, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2865, decode.acc_seg: 88.7391, loss: 0.2865
2023-11-10 23:05:51,689 - mmseg - INFO - Iter [63100/160000]	lr: 6.629e-05, eta: 18:50:31, time: 0.662, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3054, decode.acc_seg: 88.1066, loss: 0.3054
2023-11-10 23:06:23,085 - mmseg - INFO - Iter [63150/160000]	lr: 6.624e-05, eta: 18:49:50, time: 0.628, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2835, decode.acc_seg: 88.7621, loss: 0.2835
2023-11-10 23:06:57,760 - mmseg - INFO - Iter [63200/160000]	lr: 6.620e-05, eta: 18:49:14, time: 0.692, data_time: 0.052, memory: 23129, decode.loss_ce: 0.2705, decode.acc_seg: 88.9843, loss: 0.2705
2023-11-10 23:07:32,824 - mmseg - INFO - Iter [63250/160000]	lr: 6.615e-05, eta: 18:48:40, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2907, decode.acc_seg: 88.5372, loss: 0.2907
2023-11-10 23:08:08,461 - mmseg - INFO - Iter [63300/160000]	lr: 6.610e-05, eta: 18:48:06, time: 0.711, data_time: 0.009, memory: 23129, decode.loss_ce: 0.3067, decode.acc_seg: 87.9543, loss: 0.3067
2023-11-10 23:08:43,849 - mmseg - INFO - Iter [63350/160000]	lr: 6.606e-05, eta: 18:47:31, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2944, decode.acc_seg: 88.1704, loss: 0.2944
2023-11-10 23:09:19,171 - mmseg - INFO - Iter [63400/160000]	lr: 6.601e-05, eta: 18:46:57, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2840, decode.acc_seg: 88.6189, loss: 0.2840
2023-11-10 23:09:52,008 - mmseg - INFO - Iter [63450/160000]	lr: 6.596e-05, eta: 18:46:18, time: 0.656, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2826, decode.acc_seg: 88.6671, loss: 0.2826
2023-11-10 23:10:27,184 - mmseg - INFO - Iter [63500/160000]	lr: 6.592e-05, eta: 18:45:44, time: 0.704, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2803, decode.acc_seg: 88.9860, loss: 0.2803
2023-11-10 23:11:02,318 - mmseg - INFO - Iter [63550/160000]	lr: 6.587e-05, eta: 18:45:09, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2884, decode.acc_seg: 88.8586, loss: 0.2884
2023-11-10 23:11:36,978 - mmseg - INFO - Iter [63600/160000]	lr: 6.582e-05, eta: 18:44:33, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2933, decode.acc_seg: 88.7465, loss: 0.2933
2023-11-10 23:12:12,384 - mmseg - INFO - Iter [63650/160000]	lr: 6.578e-05, eta: 18:43:59, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2797, decode.acc_seg: 88.5623, loss: 0.2797
2023-11-10 23:12:43,941 - mmseg - INFO - Iter [63700/160000]	lr: 6.573e-05, eta: 18:43:19, time: 0.631, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2681, decode.acc_seg: 89.1126, loss: 0.2681
2023-11-10 23:13:19,066 - mmseg - INFO - Iter [63750/160000]	lr: 6.569e-05, eta: 18:42:44, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2663, decode.acc_seg: 89.0502, loss: 0.2663
2023-11-10 23:13:51,477 - mmseg - INFO - Iter [63800/160000]	lr: 6.564e-05, eta: 18:42:05, time: 0.649, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2898, decode.acc_seg: 88.6778, loss: 0.2898
2023-11-10 23:14:25,143 - mmseg - INFO - Iter [63850/160000]	lr: 6.559e-05, eta: 18:41:28, time: 0.672, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2688, decode.acc_seg: 89.1247, loss: 0.2688
2023-11-10 23:15:00,900 - mmseg - INFO - Iter [63900/160000]	lr: 6.555e-05, eta: 18:40:54, time: 0.715, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2633, decode.acc_seg: 89.4532, loss: 0.2633
2023-11-10 23:15:32,432 - mmseg - INFO - Iter [63950/160000]	lr: 6.550e-05, eta: 18:40:14, time: 0.632, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2816, decode.acc_seg: 88.6285, loss: 0.2816
2023-11-10 23:16:07,299 - mmseg - INFO - Saving checkpoint at 64000 iterations
2023-11-10 23:16:11,889 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 23:16:11,890 - mmseg - INFO - Iter [64000/160000]	lr: 6.545e-05, eta: 18:39:46, time: 0.789, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2835, decode.acc_seg: 88.7747, loss: 0.2835
2023-11-10 23:17:42,937 - mmseg - INFO - per class results:
2023-11-10 23:17:42,950 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 76.08 | 85.78 |
|       building      |  82.4 | 91.52 |
|         sky         | 94.01 | 97.19 |
|        floor        | 80.46 | 90.45 |
|         tree        | 73.62 | 85.83 |
|       ceiling       | 82.45 | 93.42 |
|         road        | 81.13 | 89.78 |
|         bed         | 87.83 | 95.55 |
|      windowpane     |  60.4 | 74.99 |
|        grass        | 62.78 | 85.07 |
|       cabinet       | 57.92 | 76.47 |
|       sidewalk      | 63.54 | 79.58 |
|        person       | 80.04 | 92.09 |
|        earth        | 32.55 | 45.84 |
|         door        | 45.25 | 62.25 |
|        table        | 57.32 | 70.85 |
|       mountain      | 56.95 | 72.93 |
|        plant        | 50.46 | 60.38 |
|       curtain       | 73.37 | 88.46 |
|        chair        | 52.84 | 65.41 |
|         car         | 81.62 | 93.03 |
|        water        | 54.53 | 67.41 |
|       painting      | 64.56 | 89.88 |
|         sofa        | 60.68 | 79.37 |
|        shelf        | 39.64 | 53.99 |
|        house        | 50.88 | 72.15 |
|         sea         | 51.86 |  79.0 |
|        mirror       | 65.62 |  72.2 |
|         rug         | 61.71 | 71.65 |
|        field        | 34.34 | 43.76 |
|       armchair      | 38.87 | 58.49 |
|         seat        | 58.38 | 81.31 |
|        fence        | 44.77 | 57.87 |
|         desk        | 45.56 | 69.89 |
|         rock        | 39.23 | 49.28 |
|       wardrobe      | 52.39 | 63.59 |
|         lamp        | 61.57 | 76.14 |
|       bathtub       | 76.06 | 82.86 |
|       railing       | 33.43 | 49.17 |
|       cushion       | 54.79 | 68.71 |
|         base        |  26.5 | 40.28 |
|         box         | 23.87 | 32.91 |
|        column       | 46.32 | 58.64 |
|      signboard      | 36.06 |  47.0 |
|   chest of drawers  | 39.39 | 63.63 |
|       counter       | 34.85 | 46.87 |
|         sand        | 35.56 |  60.9 |
|         sink        | 64.43 | 78.16 |
|      skyscraper     | 50.81 | 63.46 |
|      fireplace      | 68.88 | 89.49 |
|     refrigerator    | 71.96 | 80.38 |
|      grandstand     | 47.03 | 71.31 |
|         path        | 19.37 | 26.37 |
|        stairs       | 29.52 | 36.44 |
|        runway       | 65.03 | 82.53 |
|         case        | 36.51 | 40.18 |
|      pool table     | 90.96 | 96.97 |
|        pillow       | 55.21 | 66.85 |
|     screen door     | 64.41 | 67.51 |
|       stairway      | 31.94 | 42.75 |
|        river        |  10.6 | 19.03 |
|        bridge       | 71.61 | 82.86 |
|       bookcase      | 31.64 | 53.53 |
|        blind        | 41.55 | 45.88 |
|     coffee table    | 50.17 | 79.91 |
|        toilet       |  84.7 | 90.62 |
|        flower       |  34.8 | 52.81 |
|         book        | 41.89 |  63.2 |
|         hill        | 11.18 | 22.92 |
|        bench        | 39.59 | 51.79 |
|      countertop     |  54.2 | 77.42 |
|        stove        | 66.66 | 74.14 |
|         palm        | 45.64 | 69.99 |
|    kitchen island   | 32.88 | 69.48 |
|       computer      | 59.29 |  69.7 |
|     swivel chair    | 48.29 | 61.13 |
|         boat        | 57.35 |  85.1 |
|         bar         | 45.69 | 61.19 |
|    arcade machine   | 72.77 | 80.97 |
|        hovel        | 36.44 | 39.66 |
|         bus         | 84.69 | 95.44 |
|        towel        | 59.69 | 84.76 |
|        light        |  48.3 | 56.15 |
|        truck        | 23.93 | 29.93 |
|        tower        | 33.42 | 41.79 |
|      chandelier     |  65.5 | 79.35 |
|        awning       | 27.07 | 33.84 |
|     streetlight     | 23.25 | 29.19 |
|        booth        | 59.04 | 85.07 |
| television receiver |  66.5 | 81.67 |
|       airplane      | 58.42 |  71.1 |
|      dirt track     |  0.0  |  0.0  |
|       apparel       | 35.84 | 47.51 |
|         pole        | 22.39 | 33.77 |
|         land        |  0.0  |  0.0  |
|      bannister      | 12.74 |  19.9 |
|      escalator      | 25.74 | 31.03 |
|       ottoman       | 42.12 | 57.05 |
|        bottle       | 35.64 | 63.09 |
|        buffet       |  41.4 | 48.49 |
|        poster       | 31.01 | 43.86 |
|        stage        | 20.65 | 26.92 |
|         van         | 41.26 | 58.98 |
|         ship        | 11.96 | 13.71 |
|       fountain      | 50.59 | 53.28 |
|    conveyer belt    | 75.73 | 91.65 |
|        canopy       | 17.33 | 31.39 |
|        washer       | 73.16 | 74.46 |
|      plaything      | 20.65 | 33.43 |
|    swimming pool    | 57.66 | 69.69 |
|        stool        | 34.18 | 48.68 |
|        barrel       | 52.25 | 65.59 |
|        basket       | 28.76 |  34.1 |
|      waterfall      | 76.58 | 90.93 |
|         tent        | 76.15 | 97.83 |
|         bag         | 10.29 | 11.98 |
|       minibike      | 63.63 |  87.7 |
|        cradle       | 80.26 | 97.18 |
|         oven        | 31.39 | 60.41 |
|         ball        | 50.11 | 62.99 |
|         food        | 26.26 | 30.17 |
|         step        | 11.51 | 15.43 |
|         tank        | 47.09 | 54.09 |
|      trade name     | 27.13 | 30.36 |
|      microwave      |  65.8 | 73.89 |
|         pot         | 35.27 | 39.65 |
|        animal       | 50.93 |  57.6 |
|       bicycle       | 56.06 | 81.01 |
|         lake        |  54.3 | 57.08 |
|      dishwasher     | 57.85 | 71.27 |
|        screen       | 65.74 | 93.96 |
|       blanket       | 10.18 | 11.26 |
|      sculpture      | 50.13 | 74.94 |
|         hood        | 49.92 |  72.1 |
|        sconce       | 34.03 | 38.64 |
|         vase        | 35.56 | 50.81 |
|    traffic light    | 25.34 | 47.79 |
|         tray        |  6.94 | 14.52 |
|        ashcan       | 41.92 | 54.42 |
|         fan         | 55.13 | 72.02 |
|         pier        | 29.69 | 37.02 |
|      crt screen     |  0.51 |  1.16 |
|        plate        | 38.29 | 51.45 |
|       monitor       | 53.59 | 70.61 |
|    bulletin board   | 39.59 | 42.01 |
|        shower       |  1.18 |  1.56 |
|       radiator      | 50.77 | 54.99 |
|        glass        |  13.5 | 15.03 |
|        clock        | 32.12 | 40.49 |
|         flag        | 42.96 | 49.25 |
+---------------------+-------+-------+
2023-11-10 23:17:42,951 - mmseg - INFO - Summary:
2023-11-10 23:17:42,951 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 81.96 | 47.17 | 59.84 |
+-------+-------+-------+
2023-11-10 23:17:43,090 - mmseg - INFO - The previous best checkpoint /data/Next-ViT/segmentation/work_dirs/fpn_512_debi_base_160k/best_mIoU_iter_48000.pth was removed
2023-11-10 23:17:45,944 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_64000.pth.
2023-11-10 23:17:45,944 - mmseg - INFO - Best mIoU is 0.4717 at 64000 iter.
2023-11-10 23:17:45,990 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 23:17:45,991 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8196, mIoU: 0.4717, mAcc: 0.5984, IoU.wall: 0.7608, IoU.building: 0.8240, IoU.sky: 0.9401, IoU.floor: 0.8046, IoU.tree: 0.7362, IoU.ceiling: 0.8245, IoU.road: 0.8113, IoU.bed : 0.8783, IoU.windowpane: 0.6040, IoU.grass: 0.6278, IoU.cabinet: 0.5792, IoU.sidewalk: 0.6354, IoU.person: 0.8004, IoU.earth: 0.3255, IoU.door: 0.4525, IoU.table: 0.5732, IoU.mountain: 0.5695, IoU.plant: 0.5046, IoU.curtain: 0.7337, IoU.chair: 0.5284, IoU.car: 0.8162, IoU.water: 0.5453, IoU.painting: 0.6456, IoU.sofa: 0.6068, IoU.shelf: 0.3964, IoU.house: 0.5088, IoU.sea: 0.5186, IoU.mirror: 0.6562, IoU.rug: 0.6171, IoU.field: 0.3434, IoU.armchair: 0.3887, IoU.seat: 0.5838, IoU.fence: 0.4477, IoU.desk: 0.4556, IoU.rock: 0.3923, IoU.wardrobe: 0.5239, IoU.lamp: 0.6157, IoU.bathtub: 0.7606, IoU.railing: 0.3343, IoU.cushion: 0.5479, IoU.base: 0.2650, IoU.box: 0.2387, IoU.column: 0.4632, IoU.signboard: 0.3606, IoU.chest of drawers: 0.3939, IoU.counter: 0.3485, IoU.sand: 0.3556, IoU.sink: 0.6443, IoU.skyscraper: 0.5081, IoU.fireplace: 0.6888, IoU.refrigerator: 0.7196, IoU.grandstand: 0.4703, IoU.path: 0.1937, IoU.stairs: 0.2952, IoU.runway: 0.6503, IoU.case: 0.3651, IoU.pool table: 0.9096, IoU.pillow: 0.5521, IoU.screen door: 0.6441, IoU.stairway: 0.3194, IoU.river: 0.1060, IoU.bridge: 0.7161, IoU.bookcase: 0.3164, IoU.blind: 0.4155, IoU.coffee table: 0.5017, IoU.toilet: 0.8470, IoU.flower: 0.3480, IoU.book: 0.4189, IoU.hill: 0.1118, IoU.bench: 0.3959, IoU.countertop: 0.5420, IoU.stove: 0.6666, IoU.palm: 0.4564, IoU.kitchen island: 0.3288, IoU.computer: 0.5929, IoU.swivel chair: 0.4829, IoU.boat: 0.5735, IoU.bar: 0.4569, IoU.arcade machine: 0.7277, IoU.hovel: 0.3644, IoU.bus: 0.8469, IoU.towel: 0.5969, IoU.light: 0.4830, IoU.truck: 0.2393, IoU.tower: 0.3342, IoU.chandelier: 0.6550, IoU.awning: 0.2707, IoU.streetlight: 0.2325, IoU.booth: 0.5904, IoU.television receiver: 0.6650, IoU.airplane: 0.5842, IoU.dirt track: 0.0000, IoU.apparel: 0.3584, IoU.pole: 0.2239, IoU.land: 0.0000, IoU.bannister: 0.1274, IoU.escalator: 0.2574, IoU.ottoman: 0.4212, IoU.bottle: 0.3564, IoU.buffet: 0.4140, IoU.poster: 0.3101, IoU.stage: 0.2065, IoU.van: 0.4126, IoU.ship: 0.1196, IoU.fountain: 0.5059, IoU.conveyer belt: 0.7573, IoU.canopy: 0.1733, IoU.washer: 0.7316, IoU.plaything: 0.2065, IoU.swimming pool: 0.5766, IoU.stool: 0.3418, IoU.barrel: 0.5225, IoU.basket: 0.2876, IoU.waterfall: 0.7658, IoU.tent: 0.7615, IoU.bag: 0.1029, IoU.minibike: 0.6363, IoU.cradle: 0.8026, IoU.oven: 0.3139, IoU.ball: 0.5011, IoU.food: 0.2626, IoU.step: 0.1151, IoU.tank: 0.4709, IoU.trade name: 0.2713, IoU.microwave: 0.6580, IoU.pot: 0.3527, IoU.animal: 0.5093, IoU.bicycle: 0.5606, IoU.lake: 0.5430, IoU.dishwasher: 0.5785, IoU.screen: 0.6574, IoU.blanket: 0.1018, IoU.sculpture: 0.5013, IoU.hood: 0.4992, IoU.sconce: 0.3403, IoU.vase: 0.3556, IoU.traffic light: 0.2534, IoU.tray: 0.0694, IoU.ashcan: 0.4192, IoU.fan: 0.5513, IoU.pier: 0.2969, IoU.crt screen: 0.0051, IoU.plate: 0.3829, IoU.monitor: 0.5359, IoU.bulletin board: 0.3959, IoU.shower: 0.0118, IoU.radiator: 0.5077, IoU.glass: 0.1350, IoU.clock: 0.3212, IoU.flag: 0.4296, Acc.wall: 0.8578, Acc.building: 0.9152, Acc.sky: 0.9719, Acc.floor: 0.9045, Acc.tree: 0.8583, Acc.ceiling: 0.9342, Acc.road: 0.8978, Acc.bed : 0.9555, Acc.windowpane: 0.7499, Acc.grass: 0.8507, Acc.cabinet: 0.7647, Acc.sidewalk: 0.7958, Acc.person: 0.9209, Acc.earth: 0.4584, Acc.door: 0.6225, Acc.table: 0.7085, Acc.mountain: 0.7293, Acc.plant: 0.6038, Acc.curtain: 0.8846, Acc.chair: 0.6541, Acc.car: 0.9303, Acc.water: 0.6741, Acc.painting: 0.8988, Acc.sofa: 0.7937, Acc.shelf: 0.5399, Acc.house: 0.7215, Acc.sea: 0.7900, Acc.mirror: 0.7220, Acc.rug: 0.7165, Acc.field: 0.4376, Acc.armchair: 0.5849, Acc.seat: 0.8131, Acc.fence: 0.5787, Acc.desk: 0.6989, Acc.rock: 0.4928, Acc.wardrobe: 0.6359, Acc.lamp: 0.7614, Acc.bathtub: 0.8286, Acc.railing: 0.4917, Acc.cushion: 0.6871, Acc.base: 0.4028, Acc.box: 0.3291, Acc.column: 0.5864, Acc.signboard: 0.4700, Acc.chest of drawers: 0.6363, Acc.counter: 0.4687, Acc.sand: 0.6090, Acc.sink: 0.7816, Acc.skyscraper: 0.6346, Acc.fireplace: 0.8949, Acc.refrigerator: 0.8038, Acc.grandstand: 0.7131, Acc.path: 0.2637, Acc.stairs: 0.3644, Acc.runway: 0.8253, Acc.case: 0.4018, Acc.pool table: 0.9697, Acc.pillow: 0.6685, Acc.screen door: 0.6751, Acc.stairway: 0.4275, Acc.river: 0.1903, Acc.bridge: 0.8286, Acc.bookcase: 0.5353, Acc.blind: 0.4588, Acc.coffee table: 0.7991, Acc.toilet: 0.9062, Acc.flower: 0.5281, Acc.book: 0.6320, Acc.hill: 0.2292, Acc.bench: 0.5179, Acc.countertop: 0.7742, Acc.stove: 0.7414, Acc.palm: 0.6999, Acc.kitchen island: 0.6948, Acc.computer: 0.6970, Acc.swivel chair: 0.6113, Acc.boat: 0.8510, Acc.bar: 0.6119, Acc.arcade machine: 0.8097, Acc.hovel: 0.3966, Acc.bus: 0.9544, Acc.towel: 0.8476, Acc.light: 0.5615, Acc.truck: 0.2993, Acc.tower: 0.4179, Acc.chandelier: 0.7935, Acc.awning: 0.3384, Acc.streetlight: 0.2919, Acc.booth: 0.8507, Acc.television receiver: 0.8167, Acc.airplane: 0.7110, Acc.dirt track: 0.0000, Acc.apparel: 0.4751, Acc.pole: 0.3377, Acc.land: 0.0000, Acc.bannister: 0.1990, Acc.escalator: 0.3103, Acc.ottoman: 0.5705, Acc.bottle: 0.6309, Acc.buffet: 0.4849, Acc.poster: 0.4386, Acc.stage: 0.2692, Acc.van: 0.5898, Acc.ship: 0.1371, Acc.fountain: 0.5328, Acc.conveyer belt: 0.9165, Acc.canopy: 0.3139, Acc.washer: 0.7446, Acc.plaything: 0.3343, Acc.swimming pool: 0.6969, Acc.stool: 0.4868, Acc.barrel: 0.6559, Acc.basket: 0.3410, Acc.waterfall: 0.9093, Acc.tent: 0.9783, Acc.bag: 0.1198, Acc.minibike: 0.8770, Acc.cradle: 0.9718, Acc.oven: 0.6041, Acc.ball: 0.6299, Acc.food: 0.3017, Acc.step: 0.1543, Acc.tank: 0.5409, Acc.trade name: 0.3036, Acc.microwave: 0.7389, Acc.pot: 0.3965, Acc.animal: 0.5760, Acc.bicycle: 0.8101, Acc.lake: 0.5708, Acc.dishwasher: 0.7127, Acc.screen: 0.9396, Acc.blanket: 0.1126, Acc.sculpture: 0.7494, Acc.hood: 0.7210, Acc.sconce: 0.3864, Acc.vase: 0.5081, Acc.traffic light: 0.4779, Acc.tray: 0.1452, Acc.ashcan: 0.5442, Acc.fan: 0.7202, Acc.pier: 0.3702, Acc.crt screen: 0.0116, Acc.plate: 0.5145, Acc.monitor: 0.7061, Acc.bulletin board: 0.4201, Acc.shower: 0.0156, Acc.radiator: 0.5499, Acc.glass: 0.1503, Acc.clock: 0.4049, Acc.flag: 0.4925
2023-11-10 23:18:19,672 - mmseg - INFO - Iter [64050/160000]	lr: 6.541e-05, eta: 18:41:30, time: 2.555, data_time: 1.890, memory: 23129, decode.loss_ce: 0.2769, decode.acc_seg: 88.9622, loss: 0.2769
2023-11-10 23:18:53,816 - mmseg - INFO - Iter [64100/160000]	lr: 6.536e-05, eta: 18:40:53, time: 0.682, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2926, decode.acc_seg: 88.5673, loss: 0.2926
2023-11-10 23:19:26,372 - mmseg - INFO - Iter [64150/160000]	lr: 6.531e-05, eta: 18:40:15, time: 0.652, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2829, decode.acc_seg: 89.1119, loss: 0.2829
2023-11-10 23:20:01,620 - mmseg - INFO - Iter [64200/160000]	lr: 6.526e-05, eta: 18:39:40, time: 0.704, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2819, decode.acc_seg: 88.8972, loss: 0.2819
2023-11-10 23:20:36,590 - mmseg - INFO - Iter [64250/160000]	lr: 6.522e-05, eta: 18:39:05, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2871, decode.acc_seg: 88.4891, loss: 0.2871
2023-11-10 23:21:11,268 - mmseg - INFO - Iter [64300/160000]	lr: 6.517e-05, eta: 18:38:29, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2779, decode.acc_seg: 88.8487, loss: 0.2779
2023-11-10 23:21:47,228 - mmseg - INFO - Iter [64350/160000]	lr: 6.512e-05, eta: 18:37:55, time: 0.718, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2867, decode.acc_seg: 88.4701, loss: 0.2867
2023-11-10 23:22:23,342 - mmseg - INFO - Iter [64400/160000]	lr: 6.508e-05, eta: 18:37:22, time: 0.723, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2928, decode.acc_seg: 88.2993, loss: 0.2928
2023-11-10 23:23:00,051 - mmseg - INFO - Iter [64450/160000]	lr: 6.503e-05, eta: 18:36:49, time: 0.734, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2951, decode.acc_seg: 88.4151, loss: 0.2951
2023-11-10 23:23:34,415 - mmseg - INFO - Iter [64500/160000]	lr: 6.498e-05, eta: 18:36:13, time: 0.688, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3007, decode.acc_seg: 88.2029, loss: 0.3007
2023-11-10 23:24:08,913 - mmseg - INFO - Iter [64550/160000]	lr: 6.494e-05, eta: 18:35:37, time: 0.689, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2930, decode.acc_seg: 88.5460, loss: 0.2930
2023-11-10 23:24:44,339 - mmseg - INFO - Iter [64600/160000]	lr: 6.489e-05, eta: 18:35:02, time: 0.709, data_time: 0.011, memory: 23129, decode.loss_ce: 0.3020, decode.acc_seg: 88.3489, loss: 0.3020
2023-11-10 23:25:19,568 - mmseg - INFO - Iter [64650/160000]	lr: 6.484e-05, eta: 18:34:28, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2718, decode.acc_seg: 89.1427, loss: 0.2718
2023-11-10 23:25:55,001 - mmseg - INFO - Iter [64700/160000]	lr: 6.480e-05, eta: 18:33:53, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2949, decode.acc_seg: 88.2376, loss: 0.2949
2023-11-10 23:26:28,130 - mmseg - INFO - Iter [64750/160000]	lr: 6.475e-05, eta: 18:33:15, time: 0.664, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2938, decode.acc_seg: 88.4580, loss: 0.2938
2023-11-10 23:27:02,473 - mmseg - INFO - Iter [64800/160000]	lr: 6.470e-05, eta: 18:32:39, time: 0.686, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2776, decode.acc_seg: 89.2576, loss: 0.2776
2023-11-10 23:27:37,497 - mmseg - INFO - Iter [64850/160000]	lr: 6.466e-05, eta: 18:32:04, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2794, decode.acc_seg: 89.1785, loss: 0.2794
2023-11-10 23:28:12,871 - mmseg - INFO - Iter [64900/160000]	lr: 6.461e-05, eta: 18:31:29, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2689, decode.acc_seg: 89.2789, loss: 0.2689
2023-11-10 23:28:47,462 - mmseg - INFO - Iter [64950/160000]	lr: 6.456e-05, eta: 18:30:54, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2947, decode.acc_seg: 88.5470, loss: 0.2947
2023-11-10 23:29:20,842 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 23:29:20,843 - mmseg - INFO - Iter [65000/160000]	lr: 6.452e-05, eta: 18:30:16, time: 0.669, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2730, decode.acc_seg: 89.3550, loss: 0.2730
2023-11-10 23:29:55,357 - mmseg - INFO - Iter [65050/160000]	lr: 6.447e-05, eta: 18:29:40, time: 0.689, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2740, decode.acc_seg: 88.8331, loss: 0.2740
2023-11-10 23:30:29,208 - mmseg - INFO - Iter [65100/160000]	lr: 6.442e-05, eta: 18:29:03, time: 0.678, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2839, decode.acc_seg: 88.8508, loss: 0.2839
2023-11-10 23:31:02,458 - mmseg - INFO - Iter [65150/160000]	lr: 6.437e-05, eta: 18:28:26, time: 0.664, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2945, decode.acc_seg: 88.2341, loss: 0.2945
2023-11-10 23:31:37,088 - mmseg - INFO - Iter [65200/160000]	lr: 6.433e-05, eta: 18:27:50, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2866, decode.acc_seg: 88.6209, loss: 0.2866
2023-11-10 23:32:10,861 - mmseg - INFO - Iter [65250/160000]	lr: 6.428e-05, eta: 18:27:13, time: 0.675, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2817, decode.acc_seg: 88.5732, loss: 0.2817
2023-11-10 23:32:45,211 - mmseg - INFO - Iter [65300/160000]	lr: 6.423e-05, eta: 18:26:37, time: 0.687, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2792, decode.acc_seg: 88.6208, loss: 0.2792
2023-11-10 23:33:19,248 - mmseg - INFO - Iter [65350/160000]	lr: 6.419e-05, eta: 18:26:01, time: 0.682, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2692, decode.acc_seg: 89.1796, loss: 0.2692
2023-11-10 23:33:54,589 - mmseg - INFO - Iter [65400/160000]	lr: 6.414e-05, eta: 18:25:26, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2726, decode.acc_seg: 88.8805, loss: 0.2726
2023-11-10 23:34:29,989 - mmseg - INFO - Iter [65450/160000]	lr: 6.409e-05, eta: 18:24:51, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2929, decode.acc_seg: 88.5460, loss: 0.2929
2023-11-10 23:35:03,151 - mmseg - INFO - Iter [65500/160000]	lr: 6.404e-05, eta: 18:24:13, time: 0.663, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2626, decode.acc_seg: 89.4225, loss: 0.2626
2023-11-10 23:35:34,957 - mmseg - INFO - Iter [65550/160000]	lr: 6.400e-05, eta: 18:23:34, time: 0.638, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2772, decode.acc_seg: 88.8166, loss: 0.2772
2023-11-10 23:36:09,449 - mmseg - INFO - Iter [65600/160000]	lr: 6.395e-05, eta: 18:22:58, time: 0.689, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2970, decode.acc_seg: 88.6405, loss: 0.2970
2023-11-10 23:36:44,899 - mmseg - INFO - Iter [65650/160000]	lr: 6.390e-05, eta: 18:22:23, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2889, decode.acc_seg: 88.6597, loss: 0.2889
2023-11-10 23:37:18,406 - mmseg - INFO - Iter [65700/160000]	lr: 6.386e-05, eta: 18:21:46, time: 0.671, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2935, decode.acc_seg: 88.3859, loss: 0.2935
2023-11-10 23:37:51,528 - mmseg - INFO - Iter [65750/160000]	lr: 6.381e-05, eta: 18:21:08, time: 0.662, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2813, decode.acc_seg: 89.0376, loss: 0.2813
2023-11-10 23:38:23,209 - mmseg - INFO - Iter [65800/160000]	lr: 6.376e-05, eta: 18:20:29, time: 0.634, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2815, decode.acc_seg: 88.7378, loss: 0.2815
2023-11-10 23:38:54,883 - mmseg - INFO - Iter [65850/160000]	lr: 6.371e-05, eta: 18:19:49, time: 0.633, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2917, decode.acc_seg: 88.5515, loss: 0.2917
2023-11-10 23:39:29,105 - mmseg - INFO - Iter [65900/160000]	lr: 6.367e-05, eta: 18:19:12, time: 0.683, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2833, decode.acc_seg: 89.1074, loss: 0.2833
2023-11-10 23:40:03,525 - mmseg - INFO - Iter [65950/160000]	lr: 6.362e-05, eta: 18:18:37, time: 0.690, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2734, decode.acc_seg: 89.0234, loss: 0.2734
2023-11-10 23:40:39,516 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 23:40:39,517 - mmseg - INFO - Iter [66000/160000]	lr: 6.357e-05, eta: 18:18:03, time: 0.720, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2781, decode.acc_seg: 88.9282, loss: 0.2781
2023-11-10 23:41:12,239 - mmseg - INFO - Iter [66050/160000]	lr: 6.353e-05, eta: 18:17:24, time: 0.654, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2750, decode.acc_seg: 89.1125, loss: 0.2750
2023-11-10 23:41:45,782 - mmseg - INFO - Iter [66100/160000]	lr: 6.348e-05, eta: 18:16:47, time: 0.671, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2672, decode.acc_seg: 89.2827, loss: 0.2672
2023-11-10 23:42:19,110 - mmseg - INFO - Iter [66150/160000]	lr: 6.343e-05, eta: 18:16:10, time: 0.666, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2947, decode.acc_seg: 88.2553, loss: 0.2947
2023-11-10 23:42:54,884 - mmseg - INFO - Iter [66200/160000]	lr: 6.338e-05, eta: 18:15:36, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2688, decode.acc_seg: 89.3701, loss: 0.2688
2023-11-10 23:43:27,064 - mmseg - INFO - Iter [66250/160000]	lr: 6.334e-05, eta: 18:14:57, time: 0.644, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2561, decode.acc_seg: 89.8798, loss: 0.2561
2023-11-10 23:43:58,421 - mmseg - INFO - Iter [66300/160000]	lr: 6.329e-05, eta: 18:14:16, time: 0.627, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2701, decode.acc_seg: 89.2166, loss: 0.2701
2023-11-10 23:44:31,588 - mmseg - INFO - Iter [66350/160000]	lr: 6.324e-05, eta: 18:13:39, time: 0.663, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2651, decode.acc_seg: 89.4731, loss: 0.2651
2023-11-10 23:45:06,711 - mmseg - INFO - Iter [66400/160000]	lr: 6.319e-05, eta: 18:13:04, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2674, decode.acc_seg: 89.2023, loss: 0.2674
2023-11-10 23:45:41,628 - mmseg - INFO - Iter [66450/160000]	lr: 6.315e-05, eta: 18:12:29, time: 0.699, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2647, decode.acc_seg: 89.3636, loss: 0.2647
2023-11-10 23:46:15,791 - mmseg - INFO - Iter [66500/160000]	lr: 6.310e-05, eta: 18:11:52, time: 0.682, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2731, decode.acc_seg: 88.8418, loss: 0.2731
2023-11-10 23:46:50,991 - mmseg - INFO - Iter [66550/160000]	lr: 6.305e-05, eta: 18:11:18, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2562, decode.acc_seg: 89.4627, loss: 0.2562
2023-11-10 23:47:26,307 - mmseg - INFO - Iter [66600/160000]	lr: 6.301e-05, eta: 18:10:43, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2690, decode.acc_seg: 89.0684, loss: 0.2690
2023-11-10 23:47:58,182 - mmseg - INFO - Iter [66650/160000]	lr: 6.296e-05, eta: 18:10:04, time: 0.637, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2775, decode.acc_seg: 89.2685, loss: 0.2775
2023-11-10 23:48:30,570 - mmseg - INFO - Iter [66700/160000]	lr: 6.291e-05, eta: 18:09:25, time: 0.647, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2784, decode.acc_seg: 88.9657, loss: 0.2784
2023-11-10 23:49:04,499 - mmseg - INFO - Iter [66750/160000]	lr: 6.286e-05, eta: 18:08:48, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2597, decode.acc_seg: 89.5688, loss: 0.2597
2023-11-10 23:49:37,528 - mmseg - INFO - Iter [66800/160000]	lr: 6.282e-05, eta: 18:08:10, time: 0.659, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2700, decode.acc_seg: 89.2607, loss: 0.2700
2023-11-10 23:50:13,114 - mmseg - INFO - Iter [66850/160000]	lr: 6.277e-05, eta: 18:07:36, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2709, decode.acc_seg: 89.3352, loss: 0.2709
2023-11-10 23:50:48,142 - mmseg - INFO - Iter [66900/160000]	lr: 6.272e-05, eta: 18:07:01, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2694, decode.acc_seg: 89.3232, loss: 0.2694
2023-11-10 23:51:19,767 - mmseg - INFO - Iter [66950/160000]	lr: 6.267e-05, eta: 18:06:21, time: 0.632, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2835, decode.acc_seg: 88.7331, loss: 0.2835
2023-11-10 23:51:53,924 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-10 23:51:53,925 - mmseg - INFO - Iter [67000/160000]	lr: 6.263e-05, eta: 18:05:45, time: 0.682, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2706, decode.acc_seg: 89.1958, loss: 0.2706
2023-11-10 23:52:25,863 - mmseg - INFO - Iter [67050/160000]	lr: 6.258e-05, eta: 18:05:06, time: 0.640, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2669, decode.acc_seg: 89.3295, loss: 0.2669
2023-11-10 23:53:01,214 - mmseg - INFO - Iter [67100/160000]	lr: 6.253e-05, eta: 18:04:31, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2622, decode.acc_seg: 89.2141, loss: 0.2622
2023-11-10 23:53:35,663 - mmseg - INFO - Iter [67150/160000]	lr: 6.248e-05, eta: 18:03:55, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2831, decode.acc_seg: 88.4391, loss: 0.2831
2023-11-10 23:54:08,347 - mmseg - INFO - Iter [67200/160000]	lr: 6.244e-05, eta: 18:03:17, time: 0.654, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2632, decode.acc_seg: 89.7098, loss: 0.2632
2023-11-10 23:54:40,512 - mmseg - INFO - Iter [67250/160000]	lr: 6.239e-05, eta: 18:02:38, time: 0.642, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2606, decode.acc_seg: 89.3906, loss: 0.2606
2023-11-10 23:55:15,136 - mmseg - INFO - Iter [67300/160000]	lr: 6.234e-05, eta: 18:02:03, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2682, decode.acc_seg: 89.5376, loss: 0.2682
2023-11-10 23:55:50,111 - mmseg - INFO - Iter [67350/160000]	lr: 6.229e-05, eta: 18:01:28, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2658, decode.acc_seg: 89.5405, loss: 0.2658
2023-11-10 23:56:24,972 - mmseg - INFO - Iter [67400/160000]	lr: 6.225e-05, eta: 18:00:52, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2734, decode.acc_seg: 88.8958, loss: 0.2734
2023-11-10 23:57:00,352 - mmseg - INFO - Iter [67450/160000]	lr: 6.220e-05, eta: 18:00:18, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2626, decode.acc_seg: 89.4266, loss: 0.2626
2023-11-10 23:57:33,909 - mmseg - INFO - Iter [67500/160000]	lr: 6.215e-05, eta: 17:59:41, time: 0.670, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2518, decode.acc_seg: 89.9585, loss: 0.2518
2023-11-10 23:58:09,393 - mmseg - INFO - Iter [67550/160000]	lr: 6.210e-05, eta: 17:59:06, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2621, decode.acc_seg: 89.5014, loss: 0.2621
2023-11-10 23:58:44,849 - mmseg - INFO - Iter [67600/160000]	lr: 6.205e-05, eta: 17:58:32, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2776, decode.acc_seg: 88.9869, loss: 0.2776
2023-11-10 23:59:16,595 - mmseg - INFO - Iter [67650/160000]	lr: 6.201e-05, eta: 17:57:53, time: 0.636, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2560, decode.acc_seg: 89.8151, loss: 0.2560
2023-11-10 23:59:50,816 - mmseg - INFO - Iter [67700/160000]	lr: 6.196e-05, eta: 17:57:16, time: 0.685, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2713, decode.acc_seg: 89.2592, loss: 0.2713
2023-11-11 00:00:23,271 - mmseg - INFO - Iter [67750/160000]	lr: 6.191e-05, eta: 17:56:38, time: 0.648, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2557, decode.acc_seg: 89.9650, loss: 0.2557
2023-11-11 00:00:58,422 - mmseg - INFO - Iter [67800/160000]	lr: 6.186e-05, eta: 17:56:03, time: 0.703, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2700, decode.acc_seg: 89.0216, loss: 0.2700
2023-11-11 00:01:33,285 - mmseg - INFO - Iter [67850/160000]	lr: 6.182e-05, eta: 17:55:28, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2526, decode.acc_seg: 89.5624, loss: 0.2526
2023-11-11 00:02:05,884 - mmseg - INFO - Iter [67900/160000]	lr: 6.177e-05, eta: 17:54:50, time: 0.653, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2691, decode.acc_seg: 89.0692, loss: 0.2691
2023-11-11 00:02:41,564 - mmseg - INFO - Iter [67950/160000]	lr: 6.172e-05, eta: 17:54:15, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.3017, decode.acc_seg: 88.0508, loss: 0.3017
2023-11-11 00:03:16,874 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 00:03:16,875 - mmseg - INFO - Iter [68000/160000]	lr: 6.167e-05, eta: 17:53:41, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2856, decode.acc_seg: 88.5215, loss: 0.2856
2023-11-11 00:03:50,017 - mmseg - INFO - Iter [68050/160000]	lr: 6.163e-05, eta: 17:53:03, time: 0.664, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2717, decode.acc_seg: 89.2172, loss: 0.2717
2023-11-11 00:04:21,494 - mmseg - INFO - Iter [68100/160000]	lr: 6.158e-05, eta: 17:52:24, time: 0.629, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2546, decode.acc_seg: 89.9398, loss: 0.2546
2023-11-11 00:04:54,412 - mmseg - INFO - Iter [68150/160000]	lr: 6.153e-05, eta: 17:51:46, time: 0.658, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2742, decode.acc_seg: 89.1125, loss: 0.2742
2023-11-11 00:05:29,630 - mmseg - INFO - Iter [68200/160000]	lr: 6.148e-05, eta: 17:51:11, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2878, decode.acc_seg: 88.6227, loss: 0.2878
2023-11-11 00:06:04,888 - mmseg - INFO - Iter [68250/160000]	lr: 6.143e-05, eta: 17:50:36, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2771, decode.acc_seg: 88.9892, loss: 0.2771
2023-11-11 00:06:39,555 - mmseg - INFO - Iter [68300/160000]	lr: 6.139e-05, eta: 17:50:01, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2662, decode.acc_seg: 89.2659, loss: 0.2662
2023-11-11 00:07:15,075 - mmseg - INFO - Iter [68350/160000]	lr: 6.134e-05, eta: 17:49:27, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2533, decode.acc_seg: 89.6477, loss: 0.2533
2023-11-11 00:07:49,446 - mmseg - INFO - Iter [68400/160000]	lr: 6.129e-05, eta: 17:48:51, time: 0.687, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2678, decode.acc_seg: 89.1970, loss: 0.2678
2023-11-11 00:08:26,296 - mmseg - INFO - Iter [68450/160000]	lr: 6.124e-05, eta: 17:48:18, time: 0.737, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2756, decode.acc_seg: 89.0223, loss: 0.2756
2023-11-11 00:09:02,401 - mmseg - INFO - Iter [68500/160000]	lr: 6.120e-05, eta: 17:47:45, time: 0.722, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2625, decode.acc_seg: 89.6945, loss: 0.2625
2023-11-11 00:09:37,739 - mmseg - INFO - Iter [68550/160000]	lr: 6.115e-05, eta: 17:47:10, time: 0.707, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2670, decode.acc_seg: 89.2986, loss: 0.2670
2023-11-11 00:10:12,757 - mmseg - INFO - Iter [68600/160000]	lr: 6.110e-05, eta: 17:46:35, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2574, decode.acc_seg: 89.7317, loss: 0.2574
2023-11-11 00:10:47,784 - mmseg - INFO - Iter [68650/160000]	lr: 6.105e-05, eta: 17:46:00, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2622, decode.acc_seg: 89.3556, loss: 0.2622
2023-11-11 00:11:20,338 - mmseg - INFO - Iter [68700/160000]	lr: 6.100e-05, eta: 17:45:22, time: 0.652, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2626, decode.acc_seg: 89.3581, loss: 0.2626
2023-11-11 00:11:51,633 - mmseg - INFO - Iter [68750/160000]	lr: 6.096e-05, eta: 17:44:42, time: 0.626, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2627, decode.acc_seg: 89.3418, loss: 0.2627
2023-11-11 00:12:25,832 - mmseg - INFO - Iter [68800/160000]	lr: 6.091e-05, eta: 17:44:06, time: 0.683, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2793, decode.acc_seg: 88.7694, loss: 0.2793
2023-11-11 00:13:00,493 - mmseg - INFO - Iter [68850/160000]	lr: 6.086e-05, eta: 17:43:30, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2768, decode.acc_seg: 88.9963, loss: 0.2768
2023-11-11 00:13:35,653 - mmseg - INFO - Iter [68900/160000]	lr: 6.081e-05, eta: 17:42:56, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2573, decode.acc_seg: 89.5906, loss: 0.2573
2023-11-11 00:14:06,993 - mmseg - INFO - Iter [68950/160000]	lr: 6.076e-05, eta: 17:42:16, time: 0.627, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2551, decode.acc_seg: 89.7492, loss: 0.2551
2023-11-11 00:14:38,312 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 00:14:38,312 - mmseg - INFO - Iter [69000/160000]	lr: 6.072e-05, eta: 17:41:36, time: 0.627, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2791, decode.acc_seg: 89.3071, loss: 0.2791
2023-11-11 00:15:12,990 - mmseg - INFO - Iter [69050/160000]	lr: 6.067e-05, eta: 17:41:00, time: 0.693, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2792, decode.acc_seg: 89.2602, loss: 0.2792
2023-11-11 00:15:47,986 - mmseg - INFO - Iter [69100/160000]	lr: 6.062e-05, eta: 17:40:25, time: 0.700, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2720, decode.acc_seg: 89.1372, loss: 0.2720
2023-11-11 00:16:23,062 - mmseg - INFO - Iter [69150/160000]	lr: 6.057e-05, eta: 17:39:51, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2873, decode.acc_seg: 88.6311, loss: 0.2873
2023-11-11 00:16:55,340 - mmseg - INFO - Iter [69200/160000]	lr: 6.052e-05, eta: 17:39:12, time: 0.646, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2649, decode.acc_seg: 89.2833, loss: 0.2649
2023-11-11 00:17:30,082 - mmseg - INFO - Iter [69250/160000]	lr: 6.048e-05, eta: 17:38:37, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2658, decode.acc_seg: 89.4398, loss: 0.2658
2023-11-11 00:18:05,086 - mmseg - INFO - Iter [69300/160000]	lr: 6.043e-05, eta: 17:38:02, time: 0.699, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2743, decode.acc_seg: 89.2620, loss: 0.2743
2023-11-11 00:18:40,677 - mmseg - INFO - Iter [69350/160000]	lr: 6.038e-05, eta: 17:37:27, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2863, decode.acc_seg: 88.6422, loss: 0.2863
2023-11-11 00:19:15,938 - mmseg - INFO - Iter [69400/160000]	lr: 6.033e-05, eta: 17:36:53, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2803, decode.acc_seg: 88.5499, loss: 0.2803
2023-11-11 00:19:50,955 - mmseg - INFO - Iter [69450/160000]	lr: 6.028e-05, eta: 17:36:18, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2740, decode.acc_seg: 88.9628, loss: 0.2740
2023-11-11 00:20:23,499 - mmseg - INFO - Iter [69500/160000]	lr: 6.024e-05, eta: 17:35:40, time: 0.652, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2663, decode.acc_seg: 89.2242, loss: 0.2663
2023-11-11 00:20:54,792 - mmseg - INFO - Iter [69550/160000]	lr: 6.019e-05, eta: 17:35:00, time: 0.626, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2709, decode.acc_seg: 89.4316, loss: 0.2709
2023-11-11 00:21:28,291 - mmseg - INFO - Iter [69600/160000]	lr: 6.014e-05, eta: 17:34:23, time: 0.669, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2691, decode.acc_seg: 89.2889, loss: 0.2691
2023-11-11 00:22:01,389 - mmseg - INFO - Iter [69650/160000]	lr: 6.009e-05, eta: 17:33:45, time: 0.662, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2517, decode.acc_seg: 89.9453, loss: 0.2517
2023-11-11 00:22:36,990 - mmseg - INFO - Iter [69700/160000]	lr: 6.004e-05, eta: 17:33:11, time: 0.712, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2682, decode.acc_seg: 88.9050, loss: 0.2682
2023-11-11 00:23:12,556 - mmseg - INFO - Iter [69750/160000]	lr: 6.000e-05, eta: 17:32:37, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2707, decode.acc_seg: 89.1810, loss: 0.2707
2023-11-11 00:23:48,461 - mmseg - INFO - Iter [69800/160000]	lr: 5.995e-05, eta: 17:32:03, time: 0.718, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2949, decode.acc_seg: 88.3873, loss: 0.2949
2023-11-11 00:24:23,994 - mmseg - INFO - Iter [69850/160000]	lr: 5.990e-05, eta: 17:31:29, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2659, decode.acc_seg: 89.3785, loss: 0.2659
2023-11-11 00:24:59,307 - mmseg - INFO - Iter [69900/160000]	lr: 5.985e-05, eta: 17:30:54, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2543, decode.acc_seg: 89.9369, loss: 0.2543
2023-11-11 00:25:32,755 - mmseg - INFO - Iter [69950/160000]	lr: 5.980e-05, eta: 17:30:17, time: 0.670, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2621, decode.acc_seg: 89.5169, loss: 0.2621
2023-11-11 00:26:06,465 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 00:26:06,465 - mmseg - INFO - Iter [70000/160000]	lr: 5.976e-05, eta: 17:29:41, time: 0.674, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2576, decode.acc_seg: 89.6245, loss: 0.2576
2023-11-11 00:26:40,168 - mmseg - INFO - Iter [70050/160000]	lr: 5.971e-05, eta: 17:29:04, time: 0.673, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2561, decode.acc_seg: 90.1228, loss: 0.2561
2023-11-11 00:27:16,000 - mmseg - INFO - Iter [70100/160000]	lr: 5.966e-05, eta: 17:28:30, time: 0.718, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2637, decode.acc_seg: 89.2917, loss: 0.2637
2023-11-11 00:27:47,702 - mmseg - INFO - Iter [70150/160000]	lr: 5.961e-05, eta: 17:27:51, time: 0.634, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2646, decode.acc_seg: 89.0320, loss: 0.2646
2023-11-11 00:28:22,770 - mmseg - INFO - Iter [70200/160000]	lr: 5.956e-05, eta: 17:27:16, time: 0.700, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2732, decode.acc_seg: 88.8876, loss: 0.2732
2023-11-11 00:28:58,240 - mmseg - INFO - Iter [70250/160000]	lr: 5.951e-05, eta: 17:26:42, time: 0.710, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2556, decode.acc_seg: 89.5524, loss: 0.2556
2023-11-11 00:29:33,122 - mmseg - INFO - Iter [70300/160000]	lr: 5.947e-05, eta: 17:26:07, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2617, decode.acc_seg: 89.2532, loss: 0.2617
2023-11-11 00:30:08,393 - mmseg - INFO - Iter [70350/160000]	lr: 5.942e-05, eta: 17:25:32, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2759, decode.acc_seg: 89.0537, loss: 0.2759
2023-11-11 00:30:42,388 - mmseg - INFO - Iter [70400/160000]	lr: 5.937e-05, eta: 17:24:56, time: 0.681, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2716, decode.acc_seg: 89.1609, loss: 0.2716
2023-11-11 00:31:15,098 - mmseg - INFO - Iter [70450/160000]	lr: 5.932e-05, eta: 17:24:18, time: 0.653, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2759, decode.acc_seg: 89.3313, loss: 0.2759
2023-11-11 00:31:49,804 - mmseg - INFO - Iter [70500/160000]	lr: 5.927e-05, eta: 17:23:42, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2711, decode.acc_seg: 89.3784, loss: 0.2711
2023-11-11 00:32:24,594 - mmseg - INFO - Iter [70550/160000]	lr: 5.923e-05, eta: 17:23:07, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2557, decode.acc_seg: 89.8007, loss: 0.2557
2023-11-11 00:33:00,419 - mmseg - INFO - Iter [70600/160000]	lr: 5.918e-05, eta: 17:22:33, time: 0.716, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2633, decode.acc_seg: 89.4020, loss: 0.2633
2023-11-11 00:33:35,261 - mmseg - INFO - Iter [70650/160000]	lr: 5.913e-05, eta: 17:21:58, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2632, decode.acc_seg: 89.4638, loss: 0.2632
2023-11-11 00:34:10,453 - mmseg - INFO - Iter [70700/160000]	lr: 5.908e-05, eta: 17:21:23, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2587, decode.acc_seg: 89.6942, loss: 0.2587
2023-11-11 00:34:45,602 - mmseg - INFO - Iter [70750/160000]	lr: 5.903e-05, eta: 17:20:49, time: 0.703, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2729, decode.acc_seg: 89.3330, loss: 0.2729
2023-11-11 00:35:19,005 - mmseg - INFO - Iter [70800/160000]	lr: 5.898e-05, eta: 17:20:12, time: 0.669, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2683, decode.acc_seg: 88.8508, loss: 0.2683
2023-11-11 00:35:52,106 - mmseg - INFO - Iter [70850/160000]	lr: 5.894e-05, eta: 17:19:34, time: 0.662, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2756, decode.acc_seg: 89.0738, loss: 0.2756
2023-11-11 00:36:26,149 - mmseg - INFO - Iter [70900/160000]	lr: 5.889e-05, eta: 17:18:58, time: 0.680, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2680, decode.acc_seg: 89.2997, loss: 0.2680
2023-11-11 00:37:02,294 - mmseg - INFO - Iter [70950/160000]	lr: 5.884e-05, eta: 17:18:25, time: 0.723, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2625, decode.acc_seg: 89.5350, loss: 0.2625
2023-11-11 00:37:39,033 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 00:37:39,033 - mmseg - INFO - Iter [71000/160000]	lr: 5.879e-05, eta: 17:17:52, time: 0.735, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2744, decode.acc_seg: 89.1922, loss: 0.2744
2023-11-11 00:38:14,369 - mmseg - INFO - Iter [71050/160000]	lr: 5.874e-05, eta: 17:17:17, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2849, decode.acc_seg: 88.5841, loss: 0.2849
2023-11-11 00:38:47,875 - mmseg - INFO - Iter [71100/160000]	lr: 5.869e-05, eta: 17:16:40, time: 0.669, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2856, decode.acc_seg: 88.8991, loss: 0.2856
2023-11-11 00:39:23,444 - mmseg - INFO - Iter [71150/160000]	lr: 5.865e-05, eta: 17:16:06, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2670, decode.acc_seg: 89.1025, loss: 0.2670
2023-11-11 00:39:58,610 - mmseg - INFO - Iter [71200/160000]	lr: 5.860e-05, eta: 17:15:31, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2784, decode.acc_seg: 88.7329, loss: 0.2784
2023-11-11 00:40:33,925 - mmseg - INFO - Iter [71250/160000]	lr: 5.855e-05, eta: 17:14:57, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2707, decode.acc_seg: 88.9924, loss: 0.2707
2023-11-11 00:41:09,130 - mmseg - INFO - Iter [71300/160000]	lr: 5.850e-05, eta: 17:14:22, time: 0.704, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2555, decode.acc_seg: 89.7215, loss: 0.2555
2023-11-11 00:41:44,737 - mmseg - INFO - Iter [71350/160000]	lr: 5.845e-05, eta: 17:13:48, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2540, decode.acc_seg: 89.7159, loss: 0.2540
2023-11-11 00:42:16,675 - mmseg - INFO - Iter [71400/160000]	lr: 5.840e-05, eta: 17:13:09, time: 0.639, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2542, decode.acc_seg: 89.7749, loss: 0.2542
2023-11-11 00:42:52,477 - mmseg - INFO - Iter [71450/160000]	lr: 5.836e-05, eta: 17:12:35, time: 0.716, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2508, decode.acc_seg: 89.6042, loss: 0.2508
2023-11-11 00:43:27,802 - mmseg - INFO - Iter [71500/160000]	lr: 5.831e-05, eta: 17:12:01, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2542, decode.acc_seg: 89.8120, loss: 0.2542
2023-11-11 00:44:02,994 - mmseg - INFO - Iter [71550/160000]	lr: 5.826e-05, eta: 17:11:26, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2427, decode.acc_seg: 90.1127, loss: 0.2427
2023-11-11 00:44:38,358 - mmseg - INFO - Iter [71600/160000]	lr: 5.821e-05, eta: 17:10:51, time: 0.707, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2610, decode.acc_seg: 89.4931, loss: 0.2610
2023-11-11 00:45:13,206 - mmseg - INFO - Iter [71650/160000]	lr: 5.816e-05, eta: 17:10:16, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2470, decode.acc_seg: 89.9233, loss: 0.2470
2023-11-11 00:45:46,597 - mmseg - INFO - Iter [71700/160000]	lr: 5.811e-05, eta: 17:09:39, time: 0.668, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2529, decode.acc_seg: 89.4866, loss: 0.2529
2023-11-11 00:46:21,001 - mmseg - INFO - Iter [71750/160000]	lr: 5.807e-05, eta: 17:09:04, time: 0.689, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2548, decode.acc_seg: 89.9437, loss: 0.2548
2023-11-11 00:46:53,747 - mmseg - INFO - Iter [71800/160000]	lr: 5.802e-05, eta: 17:08:26, time: 0.655, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2331, decode.acc_seg: 90.4709, loss: 0.2331
2023-11-11 00:47:26,526 - mmseg - INFO - Iter [71850/160000]	lr: 5.797e-05, eta: 17:07:48, time: 0.656, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2721, decode.acc_seg: 89.5931, loss: 0.2721
2023-11-11 00:47:58,821 - mmseg - INFO - Iter [71900/160000]	lr: 5.792e-05, eta: 17:07:10, time: 0.645, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2694, decode.acc_seg: 89.4087, loss: 0.2694
2023-11-11 00:48:34,025 - mmseg - INFO - Iter [71950/160000]	lr: 5.787e-05, eta: 17:06:35, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2665, decode.acc_seg: 89.3937, loss: 0.2665
2023-11-11 00:49:06,276 - mmseg - INFO - Saving checkpoint at 72000 iterations
2023-11-11 00:49:11,087 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 00:49:11,087 - mmseg - INFO - Iter [72000/160000]	lr: 5.782e-05, eta: 17:06:03, time: 0.742, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2524, decode.acc_seg: 89.9498, loss: 0.2524
2023-11-11 00:51:14,690 - mmseg - INFO - per class results:
2023-11-11 00:51:14,703 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 76.45 | 86.89 |
|       building      | 81.71 | 90.52 |
|         sky         | 94.44 | 97.68 |
|        floor        | 80.63 | 89.56 |
|         tree        | 73.82 |  87.2 |
|       ceiling       | 82.29 | 92.02 |
|         road        | 80.92 | 89.44 |
|         bed         | 88.57 | 94.99 |
|      windowpane     | 57.45 | 68.24 |
|        grass        | 71.39 | 88.89 |
|       cabinet       | 59.71 | 72.42 |
|       sidewalk      | 62.63 | 79.57 |
|        person       | 80.32 | 92.58 |
|        earth        | 31.24 | 40.08 |
|         door        | 48.55 | 70.68 |
|        table        | 55.98 | 74.59 |
|       mountain      | 54.72 | 75.86 |
|        plant        | 51.41 | 62.04 |
|       curtain       | 72.47 | 85.79 |
|        chair        |  52.3 | 63.82 |
|         car         | 82.01 | 92.55 |
|        water        | 54.02 | 67.18 |
|       painting      | 67.42 | 85.81 |
|         sofa        | 62.89 | 85.11 |
|        shelf        | 35.59 | 51.75 |
|        house        | 47.71 | 74.97 |
|         sea         | 55.36 | 84.21 |
|        mirror       | 65.48 | 75.03 |
|         rug         | 61.91 | 75.06 |
|        field        | 27.43 |  36.4 |
|       armchair      |  39.2 | 56.02 |
|         seat        |  52.1 | 68.65 |
|        fence        |  44.6 | 59.77 |
|         desk        |  44.2 | 71.35 |
|         rock        |  38.1 | 57.08 |
|       wardrobe      | 46.28 | 61.54 |
|         lamp        | 61.29 | 76.35 |
|       bathtub       | 75.09 | 84.44 |
|       railing       | 32.16 | 48.98 |
|       cushion       |  57.8 | 73.85 |
|         base        | 30.94 | 39.97 |
|         box         | 25.19 | 33.37 |
|        column       | 43.46 | 52.11 |
|      signboard      | 37.12 | 47.86 |
|   chest of drawers  | 40.12 | 51.68 |
|       counter       |  34.4 | 46.23 |
|         sand        | 49.66 |  71.7 |
|         sink        | 68.75 | 78.82 |
|      skyscraper     | 47.16 | 55.55 |
|      fireplace      | 70.75 |  89.9 |
|     refrigerator    | 72.24 | 83.59 |
|      grandstand     |  39.9 | 81.17 |
|         path        | 20.91 |  36.8 |
|        stairs       | 30.94 |  39.0 |
|        runway       | 69.36 | 88.64 |
|         case        | 42.39 | 53.62 |
|      pool table     | 90.68 | 97.36 |
|        pillow       | 60.09 | 73.91 |
|     screen door     | 62.13 | 72.26 |
|       stairway      | 26.03 | 34.98 |
|        river        | 12.49 | 22.46 |
|        bridge       |  70.6 | 78.02 |
|       bookcase      | 31.19 | 56.81 |
|        blind        | 48.02 | 59.98 |
|     coffee table    | 54.81 |  80.0 |
|        toilet       | 84.85 | 91.08 |
|        flower       | 37.92 | 53.26 |
|         book        | 42.32 | 65.05 |
|         hill        | 10.56 | 19.68 |
|        bench        | 40.88 | 52.93 |
|      countertop     | 60.26 | 80.59 |
|        stove        | 66.34 | 81.05 |
|         palm        | 49.19 | 65.46 |
|    kitchen island   | 36.69 | 73.26 |
|       computer      | 63.05 | 80.73 |
|     swivel chair    | 40.97 | 73.28 |
|         boat        | 72.24 | 89.53 |
|         bar         | 30.52 | 34.51 |
|    arcade machine   | 65.66 | 71.04 |
|        hovel        | 50.88 |  64.0 |
|         bus         | 84.47 | 95.41 |
|        towel        | 63.67 | 75.47 |
|        light        | 53.02 |  63.1 |
|        truck        |  32.9 | 48.26 |
|        tower        | 22.37 | 37.26 |
|      chandelier     | 63.11 | 79.67 |
|        awning       | 25.96 |  33.5 |
|     streetlight     | 24.79 | 32.28 |
|        booth        | 62.93 |  68.4 |
| television receiver | 65.72 | 78.28 |
|       airplane      | 58.15 | 79.69 |
|      dirt track     |  2.58 | 14.63 |
|       apparel       | 36.94 | 47.73 |
|         pole        | 23.46 | 31.18 |
|         land        |  1.8  |  3.85 |
|      bannister      | 12.42 | 15.75 |
|      escalator      | 35.47 | 42.46 |
|       ottoman       | 47.65 | 59.74 |
|        bottle       | 28.51 | 58.23 |
|        buffet       | 46.78 | 51.61 |
|        poster       | 30.07 | 39.58 |
|        stage        | 13.35 | 17.32 |
|         van         | 37.18 | 49.86 |
|         ship        |  0.03 |  0.03 |
|       fountain      | 21.72 |  21.9 |
|    conveyer belt    | 66.62 | 86.09 |
|        canopy       | 29.76 | 47.76 |
|        washer       | 72.29 | 73.62 |
|      plaything      | 19.49 | 32.76 |
|    swimming pool    | 63.49 |  89.6 |
|        stool        | 37.25 | 55.29 |
|        barrel       | 11.07 | 69.79 |
|        basket       | 38.36 | 50.91 |
|      waterfall      | 69.33 | 89.57 |
|         tent        |  95.0 | 97.52 |
|         bag         | 14.09 |  16.8 |
|       minibike      | 63.68 | 85.14 |
|        cradle       |  74.8 | 96.03 |
|         oven        | 43.42 |  52.3 |
|         ball        |  40.6 | 66.82 |
|         food        | 46.87 | 53.67 |
|         step        | 12.47 | 16.81 |
|         tank        | 32.92 | 37.67 |
|      trade name     | 26.05 | 29.37 |
|      microwave      | 81.55 | 91.06 |
|         pot         | 39.78 |  46.7 |
|        animal       | 57.82 | 60.99 |
|       bicycle       | 56.66 | 77.15 |
|         lake        | 48.87 | 50.91 |
|      dishwasher     | 42.85 | 62.44 |
|        screen       | 70.27 | 93.37 |
|       blanket       |  13.7 |  15.2 |
|      sculpture      | 49.57 | 59.87 |
|         hood        | 51.67 | 72.98 |
|        sconce       | 43.02 | 52.66 |
|         vase        | 32.22 | 45.62 |
|    traffic light    | 28.95 | 51.32 |
|         tray        |  9.68 | 18.13 |
|        ashcan       | 37.08 | 49.33 |
|         fan         | 57.62 | 76.19 |
|         pier        | 13.16 | 13.78 |
|      crt screen     |  1.1  |  3.34 |
|        plate        | 48.56 | 64.64 |
|       monitor       |  2.3  |  2.51 |
|    bulletin board   | 47.92 | 61.33 |
|        shower       |  1.55 |  1.61 |
|       radiator      | 55.05 |  61.5 |
|        glass        |  9.48 |  9.98 |
|        clock        | 29.21 | 34.98 |
|         flag        | 44.28 | 47.06 |
+---------------------+-------+-------+
2023-11-11 00:51:14,703 - mmseg - INFO - Summary:
2023-11-11 00:51:14,704 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 82.08 | 47.11 | 60.18 |
+-------+-------+-------+
2023-11-11 00:51:14,727 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 00:51:14,728 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8208, mIoU: 0.4711, mAcc: 0.6018, IoU.wall: 0.7645, IoU.building: 0.8171, IoU.sky: 0.9444, IoU.floor: 0.8063, IoU.tree: 0.7382, IoU.ceiling: 0.8229, IoU.road: 0.8092, IoU.bed : 0.8857, IoU.windowpane: 0.5745, IoU.grass: 0.7139, IoU.cabinet: 0.5971, IoU.sidewalk: 0.6263, IoU.person: 0.8032, IoU.earth: 0.3124, IoU.door: 0.4855, IoU.table: 0.5598, IoU.mountain: 0.5472, IoU.plant: 0.5141, IoU.curtain: 0.7247, IoU.chair: 0.5230, IoU.car: 0.8201, IoU.water: 0.5402, IoU.painting: 0.6742, IoU.sofa: 0.6289, IoU.shelf: 0.3559, IoU.house: 0.4771, IoU.sea: 0.5536, IoU.mirror: 0.6548, IoU.rug: 0.6191, IoU.field: 0.2743, IoU.armchair: 0.3920, IoU.seat: 0.5210, IoU.fence: 0.4460, IoU.desk: 0.4420, IoU.rock: 0.3810, IoU.wardrobe: 0.4628, IoU.lamp: 0.6129, IoU.bathtub: 0.7509, IoU.railing: 0.3216, IoU.cushion: 0.5780, IoU.base: 0.3094, IoU.box: 0.2519, IoU.column: 0.4346, IoU.signboard: 0.3712, IoU.chest of drawers: 0.4012, IoU.counter: 0.3440, IoU.sand: 0.4966, IoU.sink: 0.6875, IoU.skyscraper: 0.4716, IoU.fireplace: 0.7075, IoU.refrigerator: 0.7224, IoU.grandstand: 0.3990, IoU.path: 0.2091, IoU.stairs: 0.3094, IoU.runway: 0.6936, IoU.case: 0.4239, IoU.pool table: 0.9068, IoU.pillow: 0.6009, IoU.screen door: 0.6213, IoU.stairway: 0.2603, IoU.river: 0.1249, IoU.bridge: 0.7060, IoU.bookcase: 0.3119, IoU.blind: 0.4802, IoU.coffee table: 0.5481, IoU.toilet: 0.8485, IoU.flower: 0.3792, IoU.book: 0.4232, IoU.hill: 0.1056, IoU.bench: 0.4088, IoU.countertop: 0.6026, IoU.stove: 0.6634, IoU.palm: 0.4919, IoU.kitchen island: 0.3669, IoU.computer: 0.6305, IoU.swivel chair: 0.4097, IoU.boat: 0.7224, IoU.bar: 0.3052, IoU.arcade machine: 0.6566, IoU.hovel: 0.5088, IoU.bus: 0.8447, IoU.towel: 0.6367, IoU.light: 0.5302, IoU.truck: 0.3290, IoU.tower: 0.2237, IoU.chandelier: 0.6311, IoU.awning: 0.2596, IoU.streetlight: 0.2479, IoU.booth: 0.6293, IoU.television receiver: 0.6572, IoU.airplane: 0.5815, IoU.dirt track: 0.0258, IoU.apparel: 0.3694, IoU.pole: 0.2346, IoU.land: 0.0180, IoU.bannister: 0.1242, IoU.escalator: 0.3547, IoU.ottoman: 0.4765, IoU.bottle: 0.2851, IoU.buffet: 0.4678, IoU.poster: 0.3007, IoU.stage: 0.1335, IoU.van: 0.3718, IoU.ship: 0.0003, IoU.fountain: 0.2172, IoU.conveyer belt: 0.6662, IoU.canopy: 0.2976, IoU.washer: 0.7229, IoU.plaything: 0.1949, IoU.swimming pool: 0.6349, IoU.stool: 0.3725, IoU.barrel: 0.1107, IoU.basket: 0.3836, IoU.waterfall: 0.6933, IoU.tent: 0.9500, IoU.bag: 0.1409, IoU.minibike: 0.6368, IoU.cradle: 0.7480, IoU.oven: 0.4342, IoU.ball: 0.4060, IoU.food: 0.4687, IoU.step: 0.1247, IoU.tank: 0.3292, IoU.trade name: 0.2605, IoU.microwave: 0.8155, IoU.pot: 0.3978, IoU.animal: 0.5782, IoU.bicycle: 0.5666, IoU.lake: 0.4887, IoU.dishwasher: 0.4285, IoU.screen: 0.7027, IoU.blanket: 0.1370, IoU.sculpture: 0.4957, IoU.hood: 0.5167, IoU.sconce: 0.4302, IoU.vase: 0.3222, IoU.traffic light: 0.2895, IoU.tray: 0.0968, IoU.ashcan: 0.3708, IoU.fan: 0.5762, IoU.pier: 0.1316, IoU.crt screen: 0.0110, IoU.plate: 0.4856, IoU.monitor: 0.0230, IoU.bulletin board: 0.4792, IoU.shower: 0.0155, IoU.radiator: 0.5505, IoU.glass: 0.0948, IoU.clock: 0.2921, IoU.flag: 0.4428, Acc.wall: 0.8689, Acc.building: 0.9052, Acc.sky: 0.9768, Acc.floor: 0.8956, Acc.tree: 0.8720, Acc.ceiling: 0.9202, Acc.road: 0.8944, Acc.bed : 0.9499, Acc.windowpane: 0.6824, Acc.grass: 0.8889, Acc.cabinet: 0.7242, Acc.sidewalk: 0.7957, Acc.person: 0.9258, Acc.earth: 0.4008, Acc.door: 0.7068, Acc.table: 0.7459, Acc.mountain: 0.7586, Acc.plant: 0.6204, Acc.curtain: 0.8579, Acc.chair: 0.6382, Acc.car: 0.9255, Acc.water: 0.6718, Acc.painting: 0.8581, Acc.sofa: 0.8511, Acc.shelf: 0.5175, Acc.house: 0.7497, Acc.sea: 0.8421, Acc.mirror: 0.7503, Acc.rug: 0.7506, Acc.field: 0.3640, Acc.armchair: 0.5602, Acc.seat: 0.6865, Acc.fence: 0.5977, Acc.desk: 0.7135, Acc.rock: 0.5708, Acc.wardrobe: 0.6154, Acc.lamp: 0.7635, Acc.bathtub: 0.8444, Acc.railing: 0.4898, Acc.cushion: 0.7385, Acc.base: 0.3997, Acc.box: 0.3337, Acc.column: 0.5211, Acc.signboard: 0.4786, Acc.chest of drawers: 0.5168, Acc.counter: 0.4623, Acc.sand: 0.7170, Acc.sink: 0.7882, Acc.skyscraper: 0.5555, Acc.fireplace: 0.8990, Acc.refrigerator: 0.8359, Acc.grandstand: 0.8117, Acc.path: 0.3680, Acc.stairs: 0.3900, Acc.runway: 0.8864, Acc.case: 0.5362, Acc.pool table: 0.9736, Acc.pillow: 0.7391, Acc.screen door: 0.7226, Acc.stairway: 0.3498, Acc.river: 0.2246, Acc.bridge: 0.7802, Acc.bookcase: 0.5681, Acc.blind: 0.5998, Acc.coffee table: 0.8000, Acc.toilet: 0.9108, Acc.flower: 0.5326, Acc.book: 0.6505, Acc.hill: 0.1968, Acc.bench: 0.5293, Acc.countertop: 0.8059, Acc.stove: 0.8105, Acc.palm: 0.6546, Acc.kitchen island: 0.7326, Acc.computer: 0.8073, Acc.swivel chair: 0.7328, Acc.boat: 0.8953, Acc.bar: 0.3451, Acc.arcade machine: 0.7104, Acc.hovel: 0.6400, Acc.bus: 0.9541, Acc.towel: 0.7547, Acc.light: 0.6310, Acc.truck: 0.4826, Acc.tower: 0.3726, Acc.chandelier: 0.7967, Acc.awning: 0.3350, Acc.streetlight: 0.3228, Acc.booth: 0.6840, Acc.television receiver: 0.7828, Acc.airplane: 0.7969, Acc.dirt track: 0.1463, Acc.apparel: 0.4773, Acc.pole: 0.3118, Acc.land: 0.0385, Acc.bannister: 0.1575, Acc.escalator: 0.4246, Acc.ottoman: 0.5974, Acc.bottle: 0.5823, Acc.buffet: 0.5161, Acc.poster: 0.3958, Acc.stage: 0.1732, Acc.van: 0.4986, Acc.ship: 0.0003, Acc.fountain: 0.2190, Acc.conveyer belt: 0.8609, Acc.canopy: 0.4776, Acc.washer: 0.7362, Acc.plaything: 0.3276, Acc.swimming pool: 0.8960, Acc.stool: 0.5529, Acc.barrel: 0.6979, Acc.basket: 0.5091, Acc.waterfall: 0.8957, Acc.tent: 0.9752, Acc.bag: 0.1680, Acc.minibike: 0.8514, Acc.cradle: 0.9603, Acc.oven: 0.5230, Acc.ball: 0.6682, Acc.food: 0.5367, Acc.step: 0.1681, Acc.tank: 0.3767, Acc.trade name: 0.2937, Acc.microwave: 0.9106, Acc.pot: 0.4670, Acc.animal: 0.6099, Acc.bicycle: 0.7715, Acc.lake: 0.5091, Acc.dishwasher: 0.6244, Acc.screen: 0.9337, Acc.blanket: 0.1520, Acc.sculpture: 0.5987, Acc.hood: 0.7298, Acc.sconce: 0.5266, Acc.vase: 0.4562, Acc.traffic light: 0.5132, Acc.tray: 0.1813, Acc.ashcan: 0.4933, Acc.fan: 0.7619, Acc.pier: 0.1378, Acc.crt screen: 0.0334, Acc.plate: 0.6464, Acc.monitor: 0.0251, Acc.bulletin board: 0.6133, Acc.shower: 0.0161, Acc.radiator: 0.6150, Acc.glass: 0.0998, Acc.clock: 0.3498, Acc.flag: 0.4706
2023-11-11 00:51:50,683 - mmseg - INFO - Iter [72050/160000]	lr: 5.777e-05, eta: 17:08:00, time: 3.191, data_time: 2.481, memory: 23129, decode.loss_ce: 0.2576, decode.acc_seg: 89.6300, loss: 0.2576
2023-11-11 00:52:26,023 - mmseg - INFO - Iter [72100/160000]	lr: 5.773e-05, eta: 17:07:25, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2539, decode.acc_seg: 89.7184, loss: 0.2539
2023-11-11 00:53:01,590 - mmseg - INFO - Iter [72150/160000]	lr: 5.768e-05, eta: 17:06:51, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2496, decode.acc_seg: 89.7220, loss: 0.2496
2023-11-11 00:53:36,705 - mmseg - INFO - Iter [72200/160000]	lr: 5.763e-05, eta: 17:06:16, time: 0.702, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2439, decode.acc_seg: 90.1004, loss: 0.2439
2023-11-11 00:54:12,042 - mmseg - INFO - Iter [72250/160000]	lr: 5.758e-05, eta: 17:05:41, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2457, decode.acc_seg: 90.2076, loss: 0.2457
2023-11-11 00:54:47,212 - mmseg - INFO - Iter [72300/160000]	lr: 5.753e-05, eta: 17:05:06, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2566, decode.acc_seg: 89.2814, loss: 0.2566
2023-11-11 00:55:20,226 - mmseg - INFO - Iter [72350/160000]	lr: 5.748e-05, eta: 17:04:29, time: 0.661, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2607, decode.acc_seg: 89.3536, loss: 0.2607
2023-11-11 00:55:55,479 - mmseg - INFO - Iter [72400/160000]	lr: 5.743e-05, eta: 17:03:54, time: 0.704, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2676, decode.acc_seg: 89.3249, loss: 0.2676
2023-11-11 00:56:28,171 - mmseg - INFO - Iter [72450/160000]	lr: 5.739e-05, eta: 17:03:16, time: 0.655, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2668, decode.acc_seg: 89.3676, loss: 0.2668
2023-11-11 00:57:00,829 - mmseg - INFO - Iter [72500/160000]	lr: 5.734e-05, eta: 17:02:38, time: 0.652, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2729, decode.acc_seg: 89.1910, loss: 0.2729
2023-11-11 00:57:36,105 - mmseg - INFO - Iter [72550/160000]	lr: 5.729e-05, eta: 17:02:03, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2615, decode.acc_seg: 89.0534, loss: 0.2615
2023-11-11 00:58:11,380 - mmseg - INFO - Iter [72600/160000]	lr: 5.724e-05, eta: 17:01:28, time: 0.705, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2583, decode.acc_seg: 89.6151, loss: 0.2583
2023-11-11 00:58:46,380 - mmseg - INFO - Iter [72650/160000]	lr: 5.719e-05, eta: 17:00:53, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2702, decode.acc_seg: 89.1757, loss: 0.2702
2023-11-11 00:59:21,872 - mmseg - INFO - Iter [72700/160000]	lr: 5.714e-05, eta: 17:00:18, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2698, decode.acc_seg: 89.2703, loss: 0.2698
2023-11-11 00:59:54,619 - mmseg - INFO - Iter [72750/160000]	lr: 5.709e-05, eta: 16:59:41, time: 0.655, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2465, decode.acc_seg: 89.6889, loss: 0.2465
2023-11-11 01:00:29,638 - mmseg - INFO - Iter [72800/160000]	lr: 5.705e-05, eta: 16:59:05, time: 0.700, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2526, decode.acc_seg: 89.7880, loss: 0.2526
2023-11-11 01:01:04,724 - mmseg - INFO - Iter [72850/160000]	lr: 5.700e-05, eta: 16:58:31, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2601, decode.acc_seg: 89.5153, loss: 0.2601
2023-11-11 01:01:39,180 - mmseg - INFO - Iter [72900/160000]	lr: 5.695e-05, eta: 16:57:55, time: 0.688, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2681, decode.acc_seg: 89.2188, loss: 0.2681
2023-11-11 01:02:14,598 - mmseg - INFO - Iter [72950/160000]	lr: 5.690e-05, eta: 16:57:20, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2783, decode.acc_seg: 88.9329, loss: 0.2783
2023-11-11 01:02:50,072 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 01:02:50,072 - mmseg - INFO - Iter [73000/160000]	lr: 5.685e-05, eta: 16:56:45, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2688, decode.acc_seg: 89.5066, loss: 0.2688
2023-11-11 01:03:25,697 - mmseg - INFO - Iter [73050/160000]	lr: 5.680e-05, eta: 16:56:11, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2493, decode.acc_seg: 90.1812, loss: 0.2493
2023-11-11 01:04:00,060 - mmseg - INFO - Iter [73100/160000]	lr: 5.675e-05, eta: 16:55:35, time: 0.688, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2734, decode.acc_seg: 89.3259, loss: 0.2734
2023-11-11 01:04:33,169 - mmseg - INFO - Iter [73150/160000]	lr: 5.671e-05, eta: 16:54:58, time: 0.662, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2576, decode.acc_seg: 89.6453, loss: 0.2576
2023-11-11 01:05:04,675 - mmseg - INFO - Iter [73200/160000]	lr: 5.666e-05, eta: 16:54:19, time: 0.630, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2491, decode.acc_seg: 90.1071, loss: 0.2491
2023-11-11 01:05:39,253 - mmseg - INFO - Iter [73250/160000]	lr: 5.661e-05, eta: 16:53:43, time: 0.690, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2650, decode.acc_seg: 89.4507, loss: 0.2650
2023-11-11 01:06:14,746 - mmseg - INFO - Iter [73300/160000]	lr: 5.656e-05, eta: 16:53:08, time: 0.710, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2672, decode.acc_seg: 89.5581, loss: 0.2672
2023-11-11 01:06:47,971 - mmseg - INFO - Iter [73350/160000]	lr: 5.651e-05, eta: 16:52:31, time: 0.666, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2748, decode.acc_seg: 89.0452, loss: 0.2748
2023-11-11 01:07:19,362 - mmseg - INFO - Iter [73400/160000]	lr: 5.646e-05, eta: 16:51:52, time: 0.628, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2658, decode.acc_seg: 89.4684, loss: 0.2658
2023-11-11 01:07:50,821 - mmseg - INFO - Iter [73450/160000]	lr: 5.641e-05, eta: 16:51:13, time: 0.629, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2598, decode.acc_seg: 89.9076, loss: 0.2598
2023-11-11 01:08:25,228 - mmseg - INFO - Iter [73500/160000]	lr: 5.637e-05, eta: 16:50:37, time: 0.687, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2479, decode.acc_seg: 90.0559, loss: 0.2479
2023-11-11 01:08:59,315 - mmseg - INFO - Iter [73550/160000]	lr: 5.632e-05, eta: 16:50:01, time: 0.683, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2671, decode.acc_seg: 89.5508, loss: 0.2671
2023-11-11 01:09:34,651 - mmseg - INFO - Iter [73600/160000]	lr: 5.627e-05, eta: 16:49:26, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2725, decode.acc_seg: 89.3380, loss: 0.2725
2023-11-11 01:10:09,127 - mmseg - INFO - Iter [73650/160000]	lr: 5.622e-05, eta: 16:48:50, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2610, decode.acc_seg: 89.2381, loss: 0.2610
2023-11-11 01:10:41,010 - mmseg - INFO - Iter [73700/160000]	lr: 5.617e-05, eta: 16:48:11, time: 0.637, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2753, decode.acc_seg: 89.7187, loss: 0.2753
2023-11-11 01:11:16,324 - mmseg - INFO - Iter [73750/160000]	lr: 5.612e-05, eta: 16:47:37, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2528, decode.acc_seg: 89.9498, loss: 0.2528
2023-11-11 01:11:52,041 - mmseg - INFO - Iter [73800/160000]	lr: 5.607e-05, eta: 16:47:02, time: 0.715, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2535, decode.acc_seg: 89.6614, loss: 0.2535
2023-11-11 01:12:27,292 - mmseg - INFO - Iter [73850/160000]	lr: 5.602e-05, eta: 16:46:28, time: 0.705, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2523, decode.acc_seg: 89.8565, loss: 0.2523
2023-11-11 01:13:00,477 - mmseg - INFO - Iter [73900/160000]	lr: 5.598e-05, eta: 16:45:50, time: 0.664, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2585, decode.acc_seg: 89.4883, loss: 0.2585
2023-11-11 01:13:32,026 - mmseg - INFO - Iter [73950/160000]	lr: 5.593e-05, eta: 16:45:11, time: 0.631, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2450, decode.acc_seg: 89.9464, loss: 0.2450
2023-11-11 01:14:03,398 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 01:14:03,398 - mmseg - INFO - Iter [74000/160000]	lr: 5.588e-05, eta: 16:44:32, time: 0.628, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2445, decode.acc_seg: 90.1227, loss: 0.2445
2023-11-11 01:14:35,849 - mmseg - INFO - Iter [74050/160000]	lr: 5.583e-05, eta: 16:43:54, time: 0.648, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2607, decode.acc_seg: 89.6113, loss: 0.2607
2023-11-11 01:15:11,144 - mmseg - INFO - Iter [74100/160000]	lr: 5.578e-05, eta: 16:43:19, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2603, decode.acc_seg: 89.8545, loss: 0.2603
2023-11-11 01:15:46,118 - mmseg - INFO - Iter [74150/160000]	lr: 5.573e-05, eta: 16:42:44, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2582, decode.acc_seg: 89.3135, loss: 0.2582
2023-11-11 01:16:21,238 - mmseg - INFO - Iter [74200/160000]	lr: 5.568e-05, eta: 16:42:09, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2566, decode.acc_seg: 89.6638, loss: 0.2566
2023-11-11 01:16:56,411 - mmseg - INFO - Iter [74250/160000]	lr: 5.563e-05, eta: 16:41:34, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2615, decode.acc_seg: 89.4051, loss: 0.2615
2023-11-11 01:17:31,906 - mmseg - INFO - Iter [74300/160000]	lr: 5.559e-05, eta: 16:41:00, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2677, decode.acc_seg: 89.4217, loss: 0.2677
2023-11-11 01:18:07,258 - mmseg - INFO - Iter [74350/160000]	lr: 5.554e-05, eta: 16:40:25, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2456, decode.acc_seg: 90.3764, loss: 0.2456
2023-11-11 01:18:40,896 - mmseg - INFO - Iter [74400/160000]	lr: 5.549e-05, eta: 16:39:48, time: 0.673, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2349, decode.acc_seg: 90.2669, loss: 0.2349
2023-11-11 01:19:15,873 - mmseg - INFO - Iter [74450/160000]	lr: 5.544e-05, eta: 16:39:13, time: 0.700, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2562, decode.acc_seg: 89.5400, loss: 0.2562
2023-11-11 01:19:49,641 - mmseg - INFO - Iter [74500/160000]	lr: 5.539e-05, eta: 16:38:37, time: 0.676, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2691, decode.acc_seg: 89.3025, loss: 0.2691
2023-11-11 01:20:22,819 - mmseg - INFO - Iter [74550/160000]	lr: 5.534e-05, eta: 16:38:00, time: 0.663, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2514, decode.acc_seg: 89.6858, loss: 0.2514
2023-11-11 01:20:57,082 - mmseg - INFO - Iter [74600/160000]	lr: 5.529e-05, eta: 16:37:24, time: 0.684, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2603, decode.acc_seg: 89.7088, loss: 0.2603
2023-11-11 01:21:32,745 - mmseg - INFO - Iter [74650/160000]	lr: 5.524e-05, eta: 16:36:49, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2706, decode.acc_seg: 89.0981, loss: 0.2706
2023-11-11 01:22:06,885 - mmseg - INFO - Iter [74700/160000]	lr: 5.519e-05, eta: 16:36:13, time: 0.683, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2852, decode.acc_seg: 88.6654, loss: 0.2852
2023-11-11 01:22:41,737 - mmseg - INFO - Iter [74750/160000]	lr: 5.515e-05, eta: 16:35:38, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2623, decode.acc_seg: 89.5369, loss: 0.2623
2023-11-11 01:23:15,204 - mmseg - INFO - Iter [74800/160000]	lr: 5.510e-05, eta: 16:35:01, time: 0.669, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2731, decode.acc_seg: 89.2135, loss: 0.2731
2023-11-11 01:23:48,430 - mmseg - INFO - Iter [74850/160000]	lr: 5.505e-05, eta: 16:34:24, time: 0.665, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2655, decode.acc_seg: 89.4724, loss: 0.2655
2023-11-11 01:24:22,313 - mmseg - INFO - Iter [74900/160000]	lr: 5.500e-05, eta: 16:33:48, time: 0.678, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2565, decode.acc_seg: 89.4009, loss: 0.2565
2023-11-11 01:24:56,884 - mmseg - INFO - Iter [74950/160000]	lr: 5.495e-05, eta: 16:33:12, time: 0.690, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2617, decode.acc_seg: 89.7558, loss: 0.2617
2023-11-11 01:25:33,630 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 01:25:33,630 - mmseg - INFO - Iter [75000/160000]	lr: 5.490e-05, eta: 16:32:39, time: 0.735, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2623, decode.acc_seg: 89.4754, loss: 0.2623
2023-11-11 01:26:10,096 - mmseg - INFO - Iter [75050/160000]	lr: 5.485e-05, eta: 16:32:06, time: 0.730, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2441, decode.acc_seg: 89.8397, loss: 0.2441
2023-11-11 01:26:43,460 - mmseg - INFO - Iter [75100/160000]	lr: 5.480e-05, eta: 16:31:29, time: 0.666, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2428, decode.acc_seg: 89.7401, loss: 0.2428
2023-11-11 01:27:17,954 - mmseg - INFO - Iter [75150/160000]	lr: 5.476e-05, eta: 16:30:53, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2681, decode.acc_seg: 89.1908, loss: 0.2681
2023-11-11 01:27:51,649 - mmseg - INFO - Iter [75200/160000]	lr: 5.471e-05, eta: 16:30:16, time: 0.673, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2431, decode.acc_seg: 90.3552, loss: 0.2431
2023-11-11 01:28:27,054 - mmseg - INFO - Iter [75250/160000]	lr: 5.466e-05, eta: 16:29:42, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2549, decode.acc_seg: 89.5040, loss: 0.2549
2023-11-11 01:28:59,626 - mmseg - INFO - Iter [75300/160000]	lr: 5.461e-05, eta: 16:29:04, time: 0.651, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2655, decode.acc_seg: 89.5793, loss: 0.2655
2023-11-11 01:29:34,488 - mmseg - INFO - Iter [75350/160000]	lr: 5.456e-05, eta: 16:28:29, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2590, decode.acc_seg: 89.3904, loss: 0.2590
2023-11-11 01:30:08,465 - mmseg - INFO - Iter [75400/160000]	lr: 5.451e-05, eta: 16:27:53, time: 0.680, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2442, decode.acc_seg: 89.9526, loss: 0.2442
2023-11-11 01:30:40,540 - mmseg - INFO - Iter [75450/160000]	lr: 5.446e-05, eta: 16:27:14, time: 0.640, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2468, decode.acc_seg: 89.9405, loss: 0.2468
2023-11-11 01:31:14,654 - mmseg - INFO - Iter [75500/160000]	lr: 5.441e-05, eta: 16:26:38, time: 0.684, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2860, decode.acc_seg: 89.0653, loss: 0.2860
2023-11-11 01:31:49,128 - mmseg - INFO - Iter [75550/160000]	lr: 5.436e-05, eta: 16:26:03, time: 0.689, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2632, decode.acc_seg: 89.5988, loss: 0.2632
2023-11-11 01:32:22,188 - mmseg - INFO - Iter [75600/160000]	lr: 5.432e-05, eta: 16:25:25, time: 0.661, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2508, decode.acc_seg: 89.7637, loss: 0.2508
2023-11-11 01:32:54,079 - mmseg - INFO - Iter [75650/160000]	lr: 5.427e-05, eta: 16:24:47, time: 0.639, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2483, decode.acc_seg: 90.1758, loss: 0.2483
2023-11-11 01:33:27,433 - mmseg - INFO - Iter [75700/160000]	lr: 5.422e-05, eta: 16:24:10, time: 0.667, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2452, decode.acc_seg: 90.2747, loss: 0.2452
2023-11-11 01:34:01,964 - mmseg - INFO - Iter [75750/160000]	lr: 5.417e-05, eta: 16:23:34, time: 0.690, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2521, decode.acc_seg: 89.8902, loss: 0.2521
2023-11-11 01:34:37,280 - mmseg - INFO - Iter [75800/160000]	lr: 5.412e-05, eta: 16:23:00, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2645, decode.acc_seg: 89.8125, loss: 0.2645
2023-11-11 01:35:10,535 - mmseg - INFO - Iter [75850/160000]	lr: 5.407e-05, eta: 16:22:23, time: 0.665, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2541, decode.acc_seg: 89.7234, loss: 0.2541
2023-11-11 01:35:45,420 - mmseg - INFO - Iter [75900/160000]	lr: 5.402e-05, eta: 16:21:47, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2665, decode.acc_seg: 89.3674, loss: 0.2665
2023-11-11 01:36:20,864 - mmseg - INFO - Iter [75950/160000]	lr: 5.397e-05, eta: 16:21:13, time: 0.709, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2366, decode.acc_seg: 90.4000, loss: 0.2366
2023-11-11 01:36:54,441 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 01:36:54,441 - mmseg - INFO - Iter [76000/160000]	lr: 5.392e-05, eta: 16:20:36, time: 0.672, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2505, decode.acc_seg: 90.0326, loss: 0.2505
2023-11-11 01:37:27,249 - mmseg - INFO - Iter [76050/160000]	lr: 5.387e-05, eta: 16:19:59, time: 0.657, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2431, decode.acc_seg: 90.3365, loss: 0.2431
2023-11-11 01:38:02,398 - mmseg - INFO - Iter [76100/160000]	lr: 5.383e-05, eta: 16:19:24, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2427, decode.acc_seg: 89.9865, loss: 0.2427
2023-11-11 01:38:37,744 - mmseg - INFO - Iter [76150/160000]	lr: 5.378e-05, eta: 16:18:49, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2429, decode.acc_seg: 90.1300, loss: 0.2429
2023-11-11 01:39:12,966 - mmseg - INFO - Iter [76200/160000]	lr: 5.373e-05, eta: 16:18:14, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2521, decode.acc_seg: 90.0197, loss: 0.2521
2023-11-11 01:39:44,654 - mmseg - INFO - Iter [76250/160000]	lr: 5.368e-05, eta: 16:17:36, time: 0.634, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2495, decode.acc_seg: 90.0468, loss: 0.2495
2023-11-11 01:40:19,089 - mmseg - INFO - Iter [76300/160000]	lr: 5.363e-05, eta: 16:17:00, time: 0.688, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2410, decode.acc_seg: 89.9747, loss: 0.2410
2023-11-11 01:40:54,543 - mmseg - INFO - Iter [76350/160000]	lr: 5.358e-05, eta: 16:16:26, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2583, decode.acc_seg: 89.7565, loss: 0.2583
2023-11-11 01:41:29,309 - mmseg - INFO - Iter [76400/160000]	lr: 5.353e-05, eta: 16:15:50, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2570, decode.acc_seg: 89.3537, loss: 0.2570
2023-11-11 01:42:04,604 - mmseg - INFO - Iter [76450/160000]	lr: 5.348e-05, eta: 16:15:16, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2623, decode.acc_seg: 89.6290, loss: 0.2623
2023-11-11 01:42:38,403 - mmseg - INFO - Iter [76500/160000]	lr: 5.343e-05, eta: 16:14:39, time: 0.676, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2516, decode.acc_seg: 89.9179, loss: 0.2516
2023-11-11 01:43:11,081 - mmseg - INFO - Iter [76550/160000]	lr: 5.339e-05, eta: 16:14:02, time: 0.653, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2445, decode.acc_seg: 89.9547, loss: 0.2445
2023-11-11 01:43:45,834 - mmseg - INFO - Iter [76600/160000]	lr: 5.334e-05, eta: 16:13:26, time: 0.696, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2547, decode.acc_seg: 89.8040, loss: 0.2547
2023-11-11 01:44:19,217 - mmseg - INFO - Iter [76650/160000]	lr: 5.329e-05, eta: 16:12:50, time: 0.668, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2456, decode.acc_seg: 89.9716, loss: 0.2456
2023-11-11 01:44:53,048 - mmseg - INFO - Iter [76700/160000]	lr: 5.324e-05, eta: 16:12:13, time: 0.676, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2510, decode.acc_seg: 89.8611, loss: 0.2510
2023-11-11 01:45:28,422 - mmseg - INFO - Iter [76750/160000]	lr: 5.319e-05, eta: 16:11:39, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2472, decode.acc_seg: 89.9522, loss: 0.2472
2023-11-11 01:46:03,838 - mmseg - INFO - Iter [76800/160000]	lr: 5.314e-05, eta: 16:11:04, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2410, decode.acc_seg: 90.1678, loss: 0.2410
2023-11-11 01:46:39,111 - mmseg - INFO - Iter [76850/160000]	lr: 5.309e-05, eta: 16:10:29, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2427, decode.acc_seg: 89.8203, loss: 0.2427
2023-11-11 01:47:13,256 - mmseg - INFO - Iter [76900/160000]	lr: 5.304e-05, eta: 16:09:53, time: 0.684, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2569, decode.acc_seg: 89.7768, loss: 0.2569
2023-11-11 01:47:46,736 - mmseg - INFO - Iter [76950/160000]	lr: 5.299e-05, eta: 16:09:17, time: 0.669, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2366, decode.acc_seg: 90.4666, loss: 0.2366
2023-11-11 01:48:23,133 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 01:48:23,134 - mmseg - INFO - Iter [77000/160000]	lr: 5.294e-05, eta: 16:08:43, time: 0.728, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2541, decode.acc_seg: 89.9620, loss: 0.2541
2023-11-11 01:48:57,775 - mmseg - INFO - Iter [77050/160000]	lr: 5.290e-05, eta: 16:08:08, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2810, decode.acc_seg: 89.1064, loss: 0.2810
2023-11-11 01:49:33,514 - mmseg - INFO - Iter [77100/160000]	lr: 5.285e-05, eta: 16:07:34, time: 0.714, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2513, decode.acc_seg: 89.7835, loss: 0.2513
2023-11-11 01:50:08,292 - mmseg - INFO - Iter [77150/160000]	lr: 5.280e-05, eta: 16:06:58, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2457, decode.acc_seg: 90.0677, loss: 0.2457
2023-11-11 01:50:43,443 - mmseg - INFO - Iter [77200/160000]	lr: 5.275e-05, eta: 16:06:23, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2653, decode.acc_seg: 89.3127, loss: 0.2653
2023-11-11 01:51:18,484 - mmseg - INFO - Iter [77250/160000]	lr: 5.270e-05, eta: 16:05:48, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2440, decode.acc_seg: 90.2165, loss: 0.2440
2023-11-11 01:51:52,599 - mmseg - INFO - Iter [77300/160000]	lr: 5.265e-05, eta: 16:05:12, time: 0.682, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2355, decode.acc_seg: 90.5136, loss: 0.2355
2023-11-11 01:52:28,096 - mmseg - INFO - Iter [77350/160000]	lr: 5.260e-05, eta: 16:04:38, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2412, decode.acc_seg: 90.0380, loss: 0.2412
2023-11-11 01:53:03,155 - mmseg - INFO - Iter [77400/160000]	lr: 5.255e-05, eta: 16:04:03, time: 0.701, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2462, decode.acc_seg: 90.1213, loss: 0.2462
2023-11-11 01:53:38,258 - mmseg - INFO - Iter [77450/160000]	lr: 5.250e-05, eta: 16:03:28, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2433, decode.acc_seg: 90.0577, loss: 0.2433
2023-11-11 01:54:13,552 - mmseg - INFO - Iter [77500/160000]	lr: 5.245e-05, eta: 16:02:53, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2478, decode.acc_seg: 89.9728, loss: 0.2478
2023-11-11 01:54:47,586 - mmseg - INFO - Iter [77550/160000]	lr: 5.241e-05, eta: 16:02:17, time: 0.680, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2602, decode.acc_seg: 89.6648, loss: 0.2602
2023-11-11 01:55:22,630 - mmseg - INFO - Iter [77600/160000]	lr: 5.236e-05, eta: 16:01:42, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2621, decode.acc_seg: 89.7913, loss: 0.2621
2023-11-11 01:55:53,968 - mmseg - INFO - Iter [77650/160000]	lr: 5.231e-05, eta: 16:01:03, time: 0.628, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2535, decode.acc_seg: 89.6381, loss: 0.2535
2023-11-11 01:56:25,955 - mmseg - INFO - Iter [77700/160000]	lr: 5.226e-05, eta: 16:00:25, time: 0.640, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2373, decode.acc_seg: 90.2430, loss: 0.2373
2023-11-11 01:57:01,140 - mmseg - INFO - Iter [77750/160000]	lr: 5.221e-05, eta: 15:59:50, time: 0.702, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2440, decode.acc_seg: 89.9488, loss: 0.2440
2023-11-11 01:57:35,763 - mmseg - INFO - Iter [77800/160000]	lr: 5.216e-05, eta: 15:59:15, time: 0.694, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2528, decode.acc_seg: 89.7581, loss: 0.2528
2023-11-11 01:58:07,433 - mmseg - INFO - Iter [77850/160000]	lr: 5.211e-05, eta: 15:58:36, time: 0.633, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2611, decode.acc_seg: 89.5582, loss: 0.2611
2023-11-11 01:58:39,575 - mmseg - INFO - Iter [77900/160000]	lr: 5.206e-05, eta: 15:57:58, time: 0.642, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2490, decode.acc_seg: 90.0774, loss: 0.2490
2023-11-11 01:59:13,628 - mmseg - INFO - Iter [77950/160000]	lr: 5.201e-05, eta: 15:57:22, time: 0.681, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2459, decode.acc_seg: 90.1168, loss: 0.2459
2023-11-11 01:59:50,462 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 01:59:50,462 - mmseg - INFO - Iter [78000/160000]	lr: 5.196e-05, eta: 15:56:49, time: 0.737, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2427, decode.acc_seg: 89.8913, loss: 0.2427
2023-11-11 02:00:25,440 - mmseg - INFO - Iter [78050/160000]	lr: 5.191e-05, eta: 15:56:14, time: 0.701, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2404, decode.acc_seg: 90.3229, loss: 0.2404
2023-11-11 02:00:58,916 - mmseg - INFO - Iter [78100/160000]	lr: 5.187e-05, eta: 15:55:38, time: 0.669, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2675, decode.acc_seg: 89.0979, loss: 0.2675
2023-11-11 02:01:33,988 - mmseg - INFO - Iter [78150/160000]	lr: 5.182e-05, eta: 15:55:03, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2497, decode.acc_seg: 89.9230, loss: 0.2497
2023-11-11 02:02:05,719 - mmseg - INFO - Iter [78200/160000]	lr: 5.177e-05, eta: 15:54:24, time: 0.635, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2460, decode.acc_seg: 90.2401, loss: 0.2460
2023-11-11 02:02:41,423 - mmseg - INFO - Iter [78250/160000]	lr: 5.172e-05, eta: 15:53:50, time: 0.713, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2521, decode.acc_seg: 89.6372, loss: 0.2521
2023-11-11 02:03:16,882 - mmseg - INFO - Iter [78300/160000]	lr: 5.167e-05, eta: 15:53:15, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2648, decode.acc_seg: 89.3868, loss: 0.2648
2023-11-11 02:03:52,459 - mmseg - INFO - Iter [78350/160000]	lr: 5.162e-05, eta: 15:52:41, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2526, decode.acc_seg: 89.6408, loss: 0.2526
2023-11-11 02:04:27,937 - mmseg - INFO - Iter [78400/160000]	lr: 5.157e-05, eta: 15:52:07, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2574, decode.acc_seg: 89.8604, loss: 0.2574
2023-11-11 02:05:03,266 - mmseg - INFO - Iter [78450/160000]	lr: 5.152e-05, eta: 15:51:32, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2403, decode.acc_seg: 90.1995, loss: 0.2403
2023-11-11 02:05:38,529 - mmseg - INFO - Iter [78500/160000]	lr: 5.147e-05, eta: 15:50:57, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2424, decode.acc_seg: 90.1760, loss: 0.2424
2023-11-11 02:06:13,616 - mmseg - INFO - Iter [78550/160000]	lr: 5.142e-05, eta: 15:50:22, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2474, decode.acc_seg: 90.1350, loss: 0.2474
2023-11-11 02:06:46,827 - mmseg - INFO - Iter [78600/160000]	lr: 5.138e-05, eta: 15:49:45, time: 0.664, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2520, decode.acc_seg: 89.5958, loss: 0.2520
2023-11-11 02:07:21,877 - mmseg - INFO - Iter [78650/160000]	lr: 5.133e-05, eta: 15:49:10, time: 0.701, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2492, decode.acc_seg: 89.9927, loss: 0.2492
2023-11-11 02:07:57,392 - mmseg - INFO - Iter [78700/160000]	lr: 5.128e-05, eta: 15:48:36, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2470, decode.acc_seg: 89.7724, loss: 0.2470
2023-11-11 02:08:32,241 - mmseg - INFO - Iter [78750/160000]	lr: 5.123e-05, eta: 15:48:01, time: 0.698, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2709, decode.acc_seg: 89.2737, loss: 0.2709
2023-11-11 02:09:05,818 - mmseg - INFO - Iter [78800/160000]	lr: 5.118e-05, eta: 15:47:24, time: 0.671, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2583, decode.acc_seg: 89.8143, loss: 0.2583
2023-11-11 02:09:41,137 - mmseg - INFO - Iter [78850/160000]	lr: 5.113e-05, eta: 15:46:50, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2411, decode.acc_seg: 90.2553, loss: 0.2411
2023-11-11 02:10:15,293 - mmseg - INFO - Iter [78900/160000]	lr: 5.108e-05, eta: 15:46:14, time: 0.684, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2366, decode.acc_seg: 90.3142, loss: 0.2366
2023-11-11 02:10:48,292 - mmseg - INFO - Iter [78950/160000]	lr: 5.103e-05, eta: 15:45:37, time: 0.659, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2340, decode.acc_seg: 90.4843, loss: 0.2340
2023-11-11 02:11:23,555 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 02:11:23,556 - mmseg - INFO - Iter [79000/160000]	lr: 5.098e-05, eta: 15:45:02, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2574, decode.acc_seg: 89.4190, loss: 0.2574
2023-11-11 02:11:59,468 - mmseg - INFO - Iter [79050/160000]	lr: 5.093e-05, eta: 15:44:28, time: 0.718, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2372, decode.acc_seg: 90.2479, loss: 0.2372
2023-11-11 02:12:34,779 - mmseg - INFO - Iter [79100/160000]	lr: 5.088e-05, eta: 15:43:53, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2333, decode.acc_seg: 90.5930, loss: 0.2333
2023-11-11 02:13:09,183 - mmseg - INFO - Iter [79150/160000]	lr: 5.084e-05, eta: 15:43:18, time: 0.689, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2572, decode.acc_seg: 89.7791, loss: 0.2572
2023-11-11 02:13:40,669 - mmseg - INFO - Iter [79200/160000]	lr: 5.079e-05, eta: 15:42:39, time: 0.630, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2582, decode.acc_seg: 89.7450, loss: 0.2582
2023-11-11 02:14:11,945 - mmseg - INFO - Iter [79250/160000]	lr: 5.074e-05, eta: 15:42:00, time: 0.625, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2455, decode.acc_seg: 90.2929, loss: 0.2455
2023-11-11 02:14:44,975 - mmseg - INFO - Iter [79300/160000]	lr: 5.069e-05, eta: 15:41:23, time: 0.661, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2453, decode.acc_seg: 90.0893, loss: 0.2453
2023-11-11 02:15:19,654 - mmseg - INFO - Iter [79350/160000]	lr: 5.064e-05, eta: 15:40:48, time: 0.692, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2532, decode.acc_seg: 90.0220, loss: 0.2532
2023-11-11 02:15:55,382 - mmseg - INFO - Iter [79400/160000]	lr: 5.059e-05, eta: 15:40:14, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2403, decode.acc_seg: 90.1987, loss: 0.2403
2023-11-11 02:16:30,833 - mmseg - INFO - Iter [79450/160000]	lr: 5.054e-05, eta: 15:39:39, time: 0.709, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2469, decode.acc_seg: 89.8876, loss: 0.2469
2023-11-11 02:17:02,956 - mmseg - INFO - Iter [79500/160000]	lr: 5.049e-05, eta: 15:39:01, time: 0.644, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2426, decode.acc_seg: 90.0262, loss: 0.2426
2023-11-11 02:17:36,681 - mmseg - INFO - Iter [79550/160000]	lr: 5.044e-05, eta: 15:38:25, time: 0.673, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2346, decode.acc_seg: 90.5548, loss: 0.2346
2023-11-11 02:18:11,506 - mmseg - INFO - Iter [79600/160000]	lr: 5.039e-05, eta: 15:37:50, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2337, decode.acc_seg: 90.4787, loss: 0.2337
2023-11-11 02:18:46,747 - mmseg - INFO - Iter [79650/160000]	lr: 5.034e-05, eta: 15:37:15, time: 0.704, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2422, decode.acc_seg: 90.3556, loss: 0.2422
2023-11-11 02:19:22,453 - mmseg - INFO - Iter [79700/160000]	lr: 5.030e-05, eta: 15:36:41, time: 0.714, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2529, decode.acc_seg: 89.8243, loss: 0.2529
2023-11-11 02:19:57,739 - mmseg - INFO - Iter [79750/160000]	lr: 5.025e-05, eta: 15:36:06, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2405, decode.acc_seg: 89.9744, loss: 0.2405
2023-11-11 02:20:32,970 - mmseg - INFO - Iter [79800/160000]	lr: 5.020e-05, eta: 15:35:31, time: 0.705, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2364, decode.acc_seg: 90.3795, loss: 0.2364
2023-11-11 02:21:08,331 - mmseg - INFO - Iter [79850/160000]	lr: 5.015e-05, eta: 15:34:57, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2493, decode.acc_seg: 90.0762, loss: 0.2493
2023-11-11 02:21:43,749 - mmseg - INFO - Iter [79900/160000]	lr: 5.010e-05, eta: 15:34:22, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2357, decode.acc_seg: 90.1903, loss: 0.2357
2023-11-11 02:22:18,581 - mmseg - INFO - Iter [79950/160000]	lr: 5.005e-05, eta: 15:33:47, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2409, decode.acc_seg: 90.4736, loss: 0.2409
2023-11-11 02:22:54,381 - mmseg - INFO - Saving checkpoint at 80000 iterations
2023-11-11 02:23:00,322 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 02:23:00,323 - mmseg - INFO - Iter [80000/160000]	lr: 5.000e-05, eta: 15:33:19, time: 0.836, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2476, decode.acc_seg: 89.9981, loss: 0.2476
2023-11-11 02:24:31,504 - mmseg - INFO - per class results:
2023-11-11 02:24:31,518 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 76.84 | 87.66 |
|       building      | 81.56 | 91.39 |
|         sky         | 94.34 |  97.6 |
|        floor        | 81.43 | 89.17 |
|         tree        | 73.57 | 88.48 |
|       ceiling       | 83.07 | 91.42 |
|         road        | 81.97 | 89.23 |
|         bed         |  88.4 | 96.24 |
|      windowpane     | 61.67 | 79.44 |
|        grass        | 66.26 | 82.27 |
|       cabinet       | 59.98 | 73.88 |
|       sidewalk      | 61.94 | 81.48 |
|        person       | 79.54 | 93.49 |
|        earth        | 33.45 | 45.99 |
|         door        | 49.85 |  63.1 |
|        table        | 57.52 | 76.89 |
|       mountain      | 59.02 | 72.84 |
|        plant        | 53.19 | 63.54 |
|       curtain       | 73.98 | 83.85 |
|        chair        | 54.74 | 68.71 |
|         car         | 83.16 | 92.55 |
|        water        | 58.43 | 75.83 |
|       painting      | 71.65 | 86.34 |
|         sofa        | 61.14 | 84.12 |
|        shelf        | 43.59 |  59.5 |
|        house        | 40.52 | 53.17 |
|         sea         | 54.63 | 70.52 |
|        mirror       | 64.89 | 74.02 |
|         rug         | 61.91 | 73.68 |
|        field        | 26.37 | 40.48 |
|       armchair      | 35.15 | 50.33 |
|         seat        |  57.0 | 70.25 |
|        fence        |  46.4 |  61.4 |
|         desk        | 49.65 | 70.73 |
|         rock        | 38.52 | 57.45 |
|       wardrobe      | 51.71 | 64.14 |
|         lamp        | 62.78 |  76.4 |
|       bathtub       | 80.21 | 86.72 |
|       railing       | 30.61 | 44.08 |
|       cushion       | 57.39 | 72.34 |
|         base        | 31.07 | 39.72 |
|         box         |  24.4 | 29.68 |
|        column       | 45.18 | 54.89 |
|      signboard      | 36.84 | 55.47 |
|   chest of drawers  |  36.7 | 53.14 |
|       counter       | 23.78 | 27.62 |
|         sand        | 46.79 | 72.81 |
|         sink        | 68.28 | 80.91 |
|      skyscraper     | 40.89 | 49.81 |
|      fireplace      | 71.26 | 88.26 |
|     refrigerator    | 77.85 | 90.66 |
|      grandstand     | 39.54 | 68.65 |
|         path        |  24.1 | 42.94 |
|        stairs       | 32.87 | 37.85 |
|        runway       | 65.83 | 84.29 |
|         case        | 45.85 |  53.4 |
|      pool table     | 92.93 | 96.78 |
|        pillow       |  58.4 | 66.51 |
|     screen door     | 63.92 | 83.03 |
|       stairway      | 31.03 | 40.88 |
|        river        | 14.19 | 30.54 |
|        bridge       | 61.73 |  81.7 |
|       bookcase      | 36.66 | 46.03 |
|        blind        | 43.52 | 48.95 |
|     coffee table    | 52.91 | 82.51 |
|        toilet       | 85.46 | 91.31 |
|        flower       | 37.31 | 46.62 |
|         book        |  45.1 | 62.63 |
|         hill        | 11.35 |  19.0 |
|        bench        | 44.12 | 50.12 |
|      countertop     | 59.76 | 79.87 |
|        stove        | 66.86 | 73.77 |
|         palm        | 44.63 | 74.26 |
|    kitchen island   |  37.0 | 72.96 |
|       computer      | 61.17 | 70.84 |
|     swivel chair    | 47.43 | 67.38 |
|         boat        | 42.69 | 59.74 |
|         bar         | 27.56 | 34.02 |
|    arcade machine   | 41.67 | 44.81 |
|        hovel        | 52.01 | 56.92 |
|         bus         | 86.54 | 95.46 |
|        towel        | 62.78 | 73.55 |
|        light        | 50.01 |  57.8 |
|        truck        | 36.79 | 50.29 |
|        tower        | 24.61 |  41.0 |
|      chandelier     | 65.29 | 80.64 |
|        awning       | 25.77 |  32.4 |
|     streetlight     | 26.07 | 35.64 |
|        booth        | 63.58 | 69.35 |
| television receiver | 67.95 | 79.17 |
|       airplane      | 54.32 | 66.41 |
|      dirt track     |  0.0  |  0.0  |
|       apparel       |  43.6 | 70.65 |
|         pole        |  22.8 | 29.05 |
|         land        |  1.26 |  2.99 |
|      bannister      | 11.98 |  15.5 |
|      escalator      | 48.63 | 60.42 |
|       ottoman       |  50.7 | 61.65 |
|        bottle       | 36.48 | 61.91 |
|        buffet       | 43.07 | 55.65 |
|        poster       | 30.71 | 37.41 |
|        stage        | 19.39 | 27.89 |
|         van         | 38.61 | 54.78 |
|         ship        | 47.58 | 83.34 |
|       fountain      | 37.46 | 37.76 |
|    conveyer belt    | 74.41 | 95.05 |
|        canopy       |  18.9 |  32.4 |
|        washer       | 73.75 | 76.07 |
|      plaything      | 19.43 | 38.26 |
|    swimming pool    | 72.77 | 80.39 |
|        stool        | 36.86 | 50.52 |
|        barrel       | 51.66 |  66.5 |
|        basket       | 39.01 | 51.82 |
|      waterfall      | 54.87 |  64.4 |
|         tent        | 95.11 | 97.91 |
|         bag         | 16.67 |  22.3 |
|       minibike      | 62.17 | 77.23 |
|        cradle       | 77.19 | 96.05 |
|         oven        |  45.1 | 58.17 |
|         ball        | 45.28 | 63.18 |
|         food        | 30.15 | 37.03 |
|         step        |  8.01 |  9.78 |
|         tank        | 47.11 |  53.0 |
|      trade name     | 21.51 | 25.59 |
|      microwave      | 79.51 | 93.59 |
|         pot         | 41.81 | 48.06 |
|        animal       | 51.47 | 55.02 |
|       bicycle       | 55.78 | 74.49 |
|         lake        | 55.08 | 61.91 |
|      dishwasher     | 45.45 | 74.72 |
|        screen       | 67.17 | 90.53 |
|       blanket       |  8.69 |  9.42 |
|      sculpture      | 54.27 |  61.7 |
|         hood        | 50.11 | 58.87 |
|        sconce       | 44.74 | 55.19 |
|         vase        | 33.13 | 45.79 |
|    traffic light    | 28.97 | 50.15 |
|         tray        |  8.35 | 22.41 |
|        ashcan       | 41.55 | 53.91 |
|         fan         | 57.72 | 72.96 |
|         pier        | 15.45 | 18.28 |
|      crt screen     |  1.05 |  3.87 |
|        plate        | 49.67 | 68.85 |
|       monitor       | 13.42 | 16.53 |
|    bulletin board   |  48.7 | 57.78 |
|        shower       |  0.56 |  0.58 |
|       radiator      |  64.0 | 73.39 |
|        glass        | 11.82 | 12.76 |
|        clock        | 30.85 | 40.63 |
|         flag        | 57.53 | 61.87 |
+---------------------+-------+-------+
2023-11-11 02:24:31,518 - mmseg - INFO - Summary:
2023-11-11 02:24:31,518 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 82.42 | 48.14 | 60.34 |
+-------+-------+-------+
2023-11-11 02:24:31,658 - mmseg - INFO - The previous best checkpoint /data/Next-ViT/segmentation/work_dirs/fpn_512_debi_base_160k/best_mIoU_iter_64000.pth was removed
2023-11-11 02:24:34,546 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_80000.pth.
2023-11-11 02:24:34,546 - mmseg - INFO - Best mIoU is 0.4814 at 80000 iter.
2023-11-11 02:24:34,588 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 02:24:34,588 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8242, mIoU: 0.4814, mAcc: 0.6034, IoU.wall: 0.7684, IoU.building: 0.8156, IoU.sky: 0.9434, IoU.floor: 0.8143, IoU.tree: 0.7357, IoU.ceiling: 0.8307, IoU.road: 0.8197, IoU.bed : 0.8840, IoU.windowpane: 0.6167, IoU.grass: 0.6626, IoU.cabinet: 0.5998, IoU.sidewalk: 0.6194, IoU.person: 0.7954, IoU.earth: 0.3345, IoU.door: 0.4985, IoU.table: 0.5752, IoU.mountain: 0.5902, IoU.plant: 0.5319, IoU.curtain: 0.7398, IoU.chair: 0.5474, IoU.car: 0.8316, IoU.water: 0.5843, IoU.painting: 0.7165, IoU.sofa: 0.6114, IoU.shelf: 0.4359, IoU.house: 0.4052, IoU.sea: 0.5463, IoU.mirror: 0.6489, IoU.rug: 0.6191, IoU.field: 0.2637, IoU.armchair: 0.3515, IoU.seat: 0.5700, IoU.fence: 0.4640, IoU.desk: 0.4965, IoU.rock: 0.3852, IoU.wardrobe: 0.5171, IoU.lamp: 0.6278, IoU.bathtub: 0.8021, IoU.railing: 0.3061, IoU.cushion: 0.5739, IoU.base: 0.3107, IoU.box: 0.2440, IoU.column: 0.4518, IoU.signboard: 0.3684, IoU.chest of drawers: 0.3670, IoU.counter: 0.2378, IoU.sand: 0.4679, IoU.sink: 0.6828, IoU.skyscraper: 0.4089, IoU.fireplace: 0.7126, IoU.refrigerator: 0.7785, IoU.grandstand: 0.3954, IoU.path: 0.2410, IoU.stairs: 0.3287, IoU.runway: 0.6583, IoU.case: 0.4585, IoU.pool table: 0.9293, IoU.pillow: 0.5840, IoU.screen door: 0.6392, IoU.stairway: 0.3103, IoU.river: 0.1419, IoU.bridge: 0.6173, IoU.bookcase: 0.3666, IoU.blind: 0.4352, IoU.coffee table: 0.5291, IoU.toilet: 0.8546, IoU.flower: 0.3731, IoU.book: 0.4510, IoU.hill: 0.1135, IoU.bench: 0.4412, IoU.countertop: 0.5976, IoU.stove: 0.6686, IoU.palm: 0.4463, IoU.kitchen island: 0.3700, IoU.computer: 0.6117, IoU.swivel chair: 0.4743, IoU.boat: 0.4269, IoU.bar: 0.2756, IoU.arcade machine: 0.4167, IoU.hovel: 0.5201, IoU.bus: 0.8654, IoU.towel: 0.6278, IoU.light: 0.5001, IoU.truck: 0.3679, IoU.tower: 0.2461, IoU.chandelier: 0.6529, IoU.awning: 0.2577, IoU.streetlight: 0.2607, IoU.booth: 0.6358, IoU.television receiver: 0.6795, IoU.airplane: 0.5432, IoU.dirt track: 0.0000, IoU.apparel: 0.4360, IoU.pole: 0.2280, IoU.land: 0.0126, IoU.bannister: 0.1198, IoU.escalator: 0.4863, IoU.ottoman: 0.5070, IoU.bottle: 0.3648, IoU.buffet: 0.4307, IoU.poster: 0.3071, IoU.stage: 0.1939, IoU.van: 0.3861, IoU.ship: 0.4758, IoU.fountain: 0.3746, IoU.conveyer belt: 0.7441, IoU.canopy: 0.1890, IoU.washer: 0.7375, IoU.plaything: 0.1943, IoU.swimming pool: 0.7277, IoU.stool: 0.3686, IoU.barrel: 0.5166, IoU.basket: 0.3901, IoU.waterfall: 0.5487, IoU.tent: 0.9511, IoU.bag: 0.1667, IoU.minibike: 0.6217, IoU.cradle: 0.7719, IoU.oven: 0.4510, IoU.ball: 0.4528, IoU.food: 0.3015, IoU.step: 0.0801, IoU.tank: 0.4711, IoU.trade name: 0.2151, IoU.microwave: 0.7951, IoU.pot: 0.4181, IoU.animal: 0.5147, IoU.bicycle: 0.5578, IoU.lake: 0.5508, IoU.dishwasher: 0.4545, IoU.screen: 0.6717, IoU.blanket: 0.0869, IoU.sculpture: 0.5427, IoU.hood: 0.5011, IoU.sconce: 0.4474, IoU.vase: 0.3313, IoU.traffic light: 0.2897, IoU.tray: 0.0835, IoU.ashcan: 0.4155, IoU.fan: 0.5772, IoU.pier: 0.1545, IoU.crt screen: 0.0105, IoU.plate: 0.4967, IoU.monitor: 0.1342, IoU.bulletin board: 0.4870, IoU.shower: 0.0056, IoU.radiator: 0.6400, IoU.glass: 0.1182, IoU.clock: 0.3085, IoU.flag: 0.5753, Acc.wall: 0.8766, Acc.building: 0.9139, Acc.sky: 0.9760, Acc.floor: 0.8917, Acc.tree: 0.8848, Acc.ceiling: 0.9142, Acc.road: 0.8923, Acc.bed : 0.9624, Acc.windowpane: 0.7944, Acc.grass: 0.8227, Acc.cabinet: 0.7388, Acc.sidewalk: 0.8148, Acc.person: 0.9349, Acc.earth: 0.4599, Acc.door: 0.6310, Acc.table: 0.7689, Acc.mountain: 0.7284, Acc.plant: 0.6354, Acc.curtain: 0.8385, Acc.chair: 0.6871, Acc.car: 0.9255, Acc.water: 0.7583, Acc.painting: 0.8634, Acc.sofa: 0.8412, Acc.shelf: 0.5950, Acc.house: 0.5317, Acc.sea: 0.7052, Acc.mirror: 0.7402, Acc.rug: 0.7368, Acc.field: 0.4048, Acc.armchair: 0.5033, Acc.seat: 0.7025, Acc.fence: 0.6140, Acc.desk: 0.7073, Acc.rock: 0.5745, Acc.wardrobe: 0.6414, Acc.lamp: 0.7640, Acc.bathtub: 0.8672, Acc.railing: 0.4408, Acc.cushion: 0.7234, Acc.base: 0.3972, Acc.box: 0.2968, Acc.column: 0.5489, Acc.signboard: 0.5547, Acc.chest of drawers: 0.5314, Acc.counter: 0.2762, Acc.sand: 0.7281, Acc.sink: 0.8091, Acc.skyscraper: 0.4981, Acc.fireplace: 0.8826, Acc.refrigerator: 0.9066, Acc.grandstand: 0.6865, Acc.path: 0.4294, Acc.stairs: 0.3785, Acc.runway: 0.8429, Acc.case: 0.5340, Acc.pool table: 0.9678, Acc.pillow: 0.6651, Acc.screen door: 0.8303, Acc.stairway: 0.4088, Acc.river: 0.3054, Acc.bridge: 0.8170, Acc.bookcase: 0.4603, Acc.blind: 0.4895, Acc.coffee table: 0.8251, Acc.toilet: 0.9131, Acc.flower: 0.4662, Acc.book: 0.6263, Acc.hill: 0.1900, Acc.bench: 0.5012, Acc.countertop: 0.7987, Acc.stove: 0.7377, Acc.palm: 0.7426, Acc.kitchen island: 0.7296, Acc.computer: 0.7084, Acc.swivel chair: 0.6738, Acc.boat: 0.5974, Acc.bar: 0.3402, Acc.arcade machine: 0.4481, Acc.hovel: 0.5692, Acc.bus: 0.9546, Acc.towel: 0.7355, Acc.light: 0.5780, Acc.truck: 0.5029, Acc.tower: 0.4100, Acc.chandelier: 0.8064, Acc.awning: 0.3240, Acc.streetlight: 0.3564, Acc.booth: 0.6935, Acc.television receiver: 0.7917, Acc.airplane: 0.6641, Acc.dirt track: 0.0000, Acc.apparel: 0.7065, Acc.pole: 0.2905, Acc.land: 0.0299, Acc.bannister: 0.1550, Acc.escalator: 0.6042, Acc.ottoman: 0.6165, Acc.bottle: 0.6191, Acc.buffet: 0.5565, Acc.poster: 0.3741, Acc.stage: 0.2789, Acc.van: 0.5478, Acc.ship: 0.8334, Acc.fountain: 0.3776, Acc.conveyer belt: 0.9505, Acc.canopy: 0.3240, Acc.washer: 0.7607, Acc.plaything: 0.3826, Acc.swimming pool: 0.8039, Acc.stool: 0.5052, Acc.barrel: 0.6650, Acc.basket: 0.5182, Acc.waterfall: 0.6440, Acc.tent: 0.9791, Acc.bag: 0.2230, Acc.minibike: 0.7723, Acc.cradle: 0.9605, Acc.oven: 0.5817, Acc.ball: 0.6318, Acc.food: 0.3703, Acc.step: 0.0978, Acc.tank: 0.5300, Acc.trade name: 0.2559, Acc.microwave: 0.9359, Acc.pot: 0.4806, Acc.animal: 0.5502, Acc.bicycle: 0.7449, Acc.lake: 0.6191, Acc.dishwasher: 0.7472, Acc.screen: 0.9053, Acc.blanket: 0.0942, Acc.sculpture: 0.6170, Acc.hood: 0.5887, Acc.sconce: 0.5519, Acc.vase: 0.4579, Acc.traffic light: 0.5015, Acc.tray: 0.2241, Acc.ashcan: 0.5391, Acc.fan: 0.7296, Acc.pier: 0.1828, Acc.crt screen: 0.0387, Acc.plate: 0.6885, Acc.monitor: 0.1653, Acc.bulletin board: 0.5778, Acc.shower: 0.0058, Acc.radiator: 0.7339, Acc.glass: 0.1276, Acc.clock: 0.4063, Acc.flag: 0.6187
2023-11-11 02:25:09,773 - mmseg - INFO - Iter [80050/160000]	lr: 4.995e-05, eta: 15:34:18, time: 2.588, data_time: 1.894, memory: 23129, decode.loss_ce: 0.2518, decode.acc_seg: 90.1240, loss: 0.2518
2023-11-11 02:25:44,811 - mmseg - INFO - Iter [80100/160000]	lr: 4.990e-05, eta: 15:33:43, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2322, decode.acc_seg: 90.3915, loss: 0.2322
2023-11-11 02:26:20,211 - mmseg - INFO - Iter [80150/160000]	lr: 4.985e-05, eta: 15:33:08, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2514, decode.acc_seg: 89.6032, loss: 0.2514
2023-11-11 02:26:54,997 - mmseg - INFO - Iter [80200/160000]	lr: 4.980e-05, eta: 15:32:33, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2386, decode.acc_seg: 90.6338, loss: 0.2386
2023-11-11 02:27:26,698 - mmseg - INFO - Iter [80250/160000]	lr: 4.976e-05, eta: 15:31:54, time: 0.634, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2416, decode.acc_seg: 90.0315, loss: 0.2416
2023-11-11 02:28:01,582 - mmseg - INFO - Iter [80300/160000]	lr: 4.971e-05, eta: 15:31:19, time: 0.697, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2447, decode.acc_seg: 90.1958, loss: 0.2447
2023-11-11 02:28:33,088 - mmseg - INFO - Iter [80350/160000]	lr: 4.966e-05, eta: 15:30:41, time: 0.631, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2466, decode.acc_seg: 90.0465, loss: 0.2466
2023-11-11 02:29:05,234 - mmseg - INFO - Iter [80400/160000]	lr: 4.961e-05, eta: 15:30:03, time: 0.642, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2507, decode.acc_seg: 90.1188, loss: 0.2507
2023-11-11 02:29:40,808 - mmseg - INFO - Iter [80450/160000]	lr: 4.956e-05, eta: 15:29:28, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2424, decode.acc_seg: 90.0052, loss: 0.2424
2023-11-11 02:30:16,105 - mmseg - INFO - Iter [80500/160000]	lr: 4.951e-05, eta: 15:28:53, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2326, decode.acc_seg: 90.2554, loss: 0.2326
2023-11-11 02:30:51,225 - mmseg - INFO - Iter [80550/160000]	lr: 4.946e-05, eta: 15:28:18, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2311, decode.acc_seg: 90.4668, loss: 0.2311
2023-11-11 02:31:26,227 - mmseg - INFO - Iter [80600/160000]	lr: 4.941e-05, eta: 15:27:43, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2393, decode.acc_seg: 90.4764, loss: 0.2393
2023-11-11 02:32:01,288 - mmseg - INFO - Iter [80650/160000]	lr: 4.936e-05, eta: 15:27:08, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2323, decode.acc_seg: 90.6338, loss: 0.2323
2023-11-11 02:32:33,938 - mmseg - INFO - Iter [80700/160000]	lr: 4.931e-05, eta: 15:26:31, time: 0.653, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2441, decode.acc_seg: 90.0607, loss: 0.2441
2023-11-11 02:33:06,929 - mmseg - INFO - Iter [80750/160000]	lr: 4.926e-05, eta: 15:25:54, time: 0.660, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2496, decode.acc_seg: 89.9831, loss: 0.2496
2023-11-11 02:33:38,890 - mmseg - INFO - Iter [80800/160000]	lr: 4.922e-05, eta: 15:25:16, time: 0.639, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2500, decode.acc_seg: 90.1123, loss: 0.2500
2023-11-11 02:34:13,579 - mmseg - INFO - Iter [80850/160000]	lr: 4.917e-05, eta: 15:24:40, time: 0.693, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2311, decode.acc_seg: 90.4772, loss: 0.2311
2023-11-11 02:34:46,907 - mmseg - INFO - Iter [80900/160000]	lr: 4.912e-05, eta: 15:24:04, time: 0.666, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2316, decode.acc_seg: 90.6540, loss: 0.2316
2023-11-11 02:35:21,034 - mmseg - INFO - Iter [80950/160000]	lr: 4.907e-05, eta: 15:23:28, time: 0.683, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2291, decode.acc_seg: 90.3572, loss: 0.2291
2023-11-11 02:35:55,563 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 02:35:55,564 - mmseg - INFO - Iter [81000/160000]	lr: 4.902e-05, eta: 15:22:52, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2345, decode.acc_seg: 90.2241, loss: 0.2345
2023-11-11 02:36:30,733 - mmseg - INFO - Iter [81050/160000]	lr: 4.897e-05, eta: 15:22:17, time: 0.704, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2489, decode.acc_seg: 90.2194, loss: 0.2489
2023-11-11 02:37:06,316 - mmseg - INFO - Iter [81100/160000]	lr: 4.892e-05, eta: 15:21:43, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2286, decode.acc_seg: 90.6506, loss: 0.2286
2023-11-11 02:37:41,392 - mmseg - INFO - Iter [81150/160000]	lr: 4.887e-05, eta: 15:21:08, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2355, decode.acc_seg: 90.3021, loss: 0.2355
2023-11-11 02:38:17,294 - mmseg - INFO - Iter [81200/160000]	lr: 4.882e-05, eta: 15:20:33, time: 0.718, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2371, decode.acc_seg: 90.3306, loss: 0.2371
2023-11-11 02:38:53,055 - mmseg - INFO - Iter [81250/160000]	lr: 4.877e-05, eta: 15:19:59, time: 0.715, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2388, decode.acc_seg: 90.3159, loss: 0.2388
2023-11-11 02:39:26,514 - mmseg - INFO - Iter [81300/160000]	lr: 4.872e-05, eta: 15:19:22, time: 0.670, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2365, decode.acc_seg: 90.6443, loss: 0.2365
2023-11-11 02:39:57,883 - mmseg - INFO - Iter [81350/160000]	lr: 4.868e-05, eta: 15:18:44, time: 0.627, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2351, decode.acc_seg: 90.3811, loss: 0.2351
2023-11-11 02:40:32,809 - mmseg - INFO - Iter [81400/160000]	lr: 4.863e-05, eta: 15:18:09, time: 0.697, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2502, decode.acc_seg: 89.6330, loss: 0.2502
2023-11-11 02:41:06,912 - mmseg - INFO - Iter [81450/160000]	lr: 4.858e-05, eta: 15:17:33, time: 0.683, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2331, decode.acc_seg: 90.3247, loss: 0.2331
2023-11-11 02:41:39,656 - mmseg - INFO - Iter [81500/160000]	lr: 4.853e-05, eta: 15:16:55, time: 0.654, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2229, decode.acc_seg: 90.7502, loss: 0.2229
2023-11-11 02:42:16,349 - mmseg - INFO - Iter [81550/160000]	lr: 4.848e-05, eta: 15:16:22, time: 0.735, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2490, decode.acc_seg: 90.0544, loss: 0.2490
2023-11-11 02:42:48,077 - mmseg - INFO - Iter [81600/160000]	lr: 4.843e-05, eta: 15:15:44, time: 0.634, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2417, decode.acc_seg: 90.3549, loss: 0.2417
2023-11-11 02:43:23,329 - mmseg - INFO - Iter [81650/160000]	lr: 4.838e-05, eta: 15:15:09, time: 0.704, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2356, decode.acc_seg: 90.0405, loss: 0.2356
2023-11-11 02:43:58,453 - mmseg - INFO - Iter [81700/160000]	lr: 4.833e-05, eta: 15:14:34, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2397, decode.acc_seg: 90.1726, loss: 0.2397
2023-11-11 02:44:33,445 - mmseg - INFO - Iter [81750/160000]	lr: 4.828e-05, eta: 15:13:59, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2426, decode.acc_seg: 90.3388, loss: 0.2426
2023-11-11 02:45:08,633 - mmseg - INFO - Iter [81800/160000]	lr: 4.823e-05, eta: 15:13:24, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2301, decode.acc_seg: 90.4735, loss: 0.2301
2023-11-11 02:45:44,178 - mmseg - INFO - Iter [81850/160000]	lr: 4.819e-05, eta: 15:12:49, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2348, decode.acc_seg: 90.5707, loss: 0.2348
2023-11-11 02:46:19,367 - mmseg - INFO - Iter [81900/160000]	lr: 4.814e-05, eta: 15:12:14, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2465, decode.acc_seg: 90.2101, loss: 0.2465
2023-11-11 02:46:52,922 - mmseg - INFO - Iter [81950/160000]	lr: 4.809e-05, eta: 15:11:38, time: 0.672, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2535, decode.acc_seg: 89.5615, loss: 0.2535
2023-11-11 02:47:25,950 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 02:47:25,950 - mmseg - INFO - Iter [82000/160000]	lr: 4.804e-05, eta: 15:11:01, time: 0.661, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2377, decode.acc_seg: 90.4147, loss: 0.2377
2023-11-11 02:47:57,545 - mmseg - INFO - Iter [82050/160000]	lr: 4.799e-05, eta: 15:10:23, time: 0.632, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2517, decode.acc_seg: 89.7239, loss: 0.2517
2023-11-11 02:48:32,017 - mmseg - INFO - Iter [82100/160000]	lr: 4.794e-05, eta: 15:09:47, time: 0.690, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2481, decode.acc_seg: 90.0130, loss: 0.2481
2023-11-11 02:49:07,061 - mmseg - INFO - Iter [82150/160000]	lr: 4.789e-05, eta: 15:09:12, time: 0.700, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2334, decode.acc_seg: 90.4729, loss: 0.2334
2023-11-11 02:49:42,472 - mmseg - INFO - Iter [82200/160000]	lr: 4.784e-05, eta: 15:08:37, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2482, decode.acc_seg: 89.7868, loss: 0.2482
2023-11-11 02:50:17,663 - mmseg - INFO - Iter [82250/160000]	lr: 4.779e-05, eta: 15:08:03, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2428, decode.acc_seg: 90.2131, loss: 0.2428
2023-11-11 02:50:51,009 - mmseg - INFO - Iter [82300/160000]	lr: 4.774e-05, eta: 15:07:26, time: 0.666, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2418, decode.acc_seg: 90.1875, loss: 0.2418
2023-11-11 02:51:26,538 - mmseg - INFO - Iter [82350/160000]	lr: 4.769e-05, eta: 15:06:51, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2439, decode.acc_seg: 90.0686, loss: 0.2439
2023-11-11 02:52:01,607 - mmseg - INFO - Iter [82400/160000]	lr: 4.765e-05, eta: 15:06:16, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2535, decode.acc_seg: 89.8512, loss: 0.2535
2023-11-11 02:52:33,033 - mmseg - INFO - Iter [82450/160000]	lr: 4.760e-05, eta: 15:05:38, time: 0.629, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2418, decode.acc_seg: 90.0591, loss: 0.2418
2023-11-11 02:53:07,988 - mmseg - INFO - Iter [82500/160000]	lr: 4.755e-05, eta: 15:05:03, time: 0.698, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2341, decode.acc_seg: 90.5629, loss: 0.2341
2023-11-11 02:53:43,516 - mmseg - INFO - Iter [82550/160000]	lr: 4.750e-05, eta: 15:04:28, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2336, decode.acc_seg: 90.5129, loss: 0.2336
2023-11-11 02:54:15,667 - mmseg - INFO - Iter [82600/160000]	lr: 4.745e-05, eta: 15:03:51, time: 0.643, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2419, decode.acc_seg: 90.1372, loss: 0.2419
2023-11-11 02:54:50,538 - mmseg - INFO - Iter [82650/160000]	lr: 4.740e-05, eta: 15:03:15, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2346, decode.acc_seg: 90.3155, loss: 0.2346
2023-11-11 02:55:23,496 - mmseg - INFO - Iter [82700/160000]	lr: 4.735e-05, eta: 15:02:38, time: 0.659, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2556, decode.acc_seg: 89.7184, loss: 0.2556
2023-11-11 02:55:56,754 - mmseg - INFO - Iter [82750/160000]	lr: 4.730e-05, eta: 15:02:02, time: 0.665, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2427, decode.acc_seg: 90.2824, loss: 0.2427
2023-11-11 02:56:32,253 - mmseg - INFO - Iter [82800/160000]	lr: 4.725e-05, eta: 15:01:27, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2280, decode.acc_seg: 91.0846, loss: 0.2280
2023-11-11 02:57:07,050 - mmseg - INFO - Iter [82850/160000]	lr: 4.720e-05, eta: 15:00:52, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2280, decode.acc_seg: 90.5997, loss: 0.2280
2023-11-11 02:57:41,995 - mmseg - INFO - Iter [82900/160000]	lr: 4.716e-05, eta: 15:00:17, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2347, decode.acc_seg: 90.3431, loss: 0.2347
2023-11-11 02:58:15,077 - mmseg - INFO - Iter [82950/160000]	lr: 4.711e-05, eta: 14:59:40, time: 0.661, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2231, decode.acc_seg: 90.9109, loss: 0.2231
2023-11-11 02:58:48,652 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 02:58:48,652 - mmseg - INFO - Iter [83000/160000]	lr: 4.706e-05, eta: 14:59:04, time: 0.673, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2295, decode.acc_seg: 90.3518, loss: 0.2295
2023-11-11 02:59:20,470 - mmseg - INFO - Iter [83050/160000]	lr: 4.701e-05, eta: 14:58:26, time: 0.636, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2534, decode.acc_seg: 89.6454, loss: 0.2534
2023-11-11 02:59:56,701 - mmseg - INFO - Iter [83100/160000]	lr: 4.696e-05, eta: 14:57:52, time: 0.724, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2553, decode.acc_seg: 89.8279, loss: 0.2553
2023-11-11 03:00:33,880 - mmseg - INFO - Iter [83150/160000]	lr: 4.691e-05, eta: 14:57:19, time: 0.743, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2343, decode.acc_seg: 90.7723, loss: 0.2343
2023-11-11 03:01:08,671 - mmseg - INFO - Iter [83200/160000]	lr: 4.686e-05, eta: 14:56:43, time: 0.696, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2286, decode.acc_seg: 90.5652, loss: 0.2286
2023-11-11 03:01:42,973 - mmseg - INFO - Iter [83250/160000]	lr: 4.681e-05, eta: 14:56:08, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2386, decode.acc_seg: 90.1151, loss: 0.2386
2023-11-11 03:02:18,178 - mmseg - INFO - Iter [83300/160000]	lr: 4.676e-05, eta: 14:55:33, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2451, decode.acc_seg: 89.9355, loss: 0.2451
2023-11-11 03:02:53,011 - mmseg - INFO - Iter [83350/160000]	lr: 4.671e-05, eta: 14:54:58, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2353, decode.acc_seg: 90.4633, loss: 0.2353
2023-11-11 03:03:27,845 - mmseg - INFO - Iter [83400/160000]	lr: 4.667e-05, eta: 14:54:22, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2430, decode.acc_seg: 90.1843, loss: 0.2430
2023-11-11 03:04:03,197 - mmseg - INFO - Iter [83450/160000]	lr: 4.662e-05, eta: 14:53:48, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2362, decode.acc_seg: 90.6549, loss: 0.2362
2023-11-11 03:04:36,988 - mmseg - INFO - Iter [83500/160000]	lr: 4.657e-05, eta: 14:53:11, time: 0.675, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2392, decode.acc_seg: 90.2972, loss: 0.2392
2023-11-11 03:05:11,671 - mmseg - INFO - Iter [83550/160000]	lr: 4.652e-05, eta: 14:52:36, time: 0.694, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2542, decode.acc_seg: 89.8358, loss: 0.2542
2023-11-11 03:05:46,829 - mmseg - INFO - Iter [83600/160000]	lr: 4.647e-05, eta: 14:52:01, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2367, decode.acc_seg: 90.2043, loss: 0.2367
2023-11-11 03:06:22,256 - mmseg - INFO - Iter [83650/160000]	lr: 4.642e-05, eta: 14:51:27, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2527, decode.acc_seg: 89.9356, loss: 0.2527
2023-11-11 03:06:55,694 - mmseg - INFO - Iter [83700/160000]	lr: 4.637e-05, eta: 14:50:50, time: 0.669, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2322, decode.acc_seg: 90.5383, loss: 0.2322
2023-11-11 03:07:28,876 - mmseg - INFO - Iter [83750/160000]	lr: 4.632e-05, eta: 14:50:13, time: 0.663, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2308, decode.acc_seg: 90.3751, loss: 0.2308
2023-11-11 03:08:04,059 - mmseg - INFO - Iter [83800/160000]	lr: 4.627e-05, eta: 14:49:38, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2246, decode.acc_seg: 90.6494, loss: 0.2246
2023-11-11 03:08:39,277 - mmseg - INFO - Iter [83850/160000]	lr: 4.622e-05, eta: 14:49:04, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2425, decode.acc_seg: 90.0960, loss: 0.2425
2023-11-11 03:09:10,756 - mmseg - INFO - Iter [83900/160000]	lr: 4.618e-05, eta: 14:48:25, time: 0.631, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2206, decode.acc_seg: 90.8111, loss: 0.2206
2023-11-11 03:09:44,555 - mmseg - INFO - Iter [83950/160000]	lr: 4.613e-05, eta: 14:47:49, time: 0.675, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2341, decode.acc_seg: 90.7504, loss: 0.2341
2023-11-11 03:10:18,441 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 03:10:18,441 - mmseg - INFO - Iter [84000/160000]	lr: 4.608e-05, eta: 14:47:13, time: 0.678, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2491, decode.acc_seg: 90.2003, loss: 0.2491
2023-11-11 03:10:53,523 - mmseg - INFO - Iter [84050/160000]	lr: 4.603e-05, eta: 14:46:38, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2464, decode.acc_seg: 90.2585, loss: 0.2464
2023-11-11 03:11:29,165 - mmseg - INFO - Iter [84100/160000]	lr: 4.598e-05, eta: 14:46:04, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2445, decode.acc_seg: 89.7539, loss: 0.2445
2023-11-11 03:12:04,624 - mmseg - INFO - Iter [84150/160000]	lr: 4.593e-05, eta: 14:45:29, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2404, decode.acc_seg: 90.0648, loss: 0.2404
2023-11-11 03:12:40,138 - mmseg - INFO - Iter [84200/160000]	lr: 4.588e-05, eta: 14:44:55, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2537, decode.acc_seg: 89.8982, loss: 0.2537
2023-11-11 03:13:15,729 - mmseg - INFO - Iter [84250/160000]	lr: 4.583e-05, eta: 14:44:20, time: 0.712, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2330, decode.acc_seg: 90.3524, loss: 0.2330
2023-11-11 03:13:49,674 - mmseg - INFO - Iter [84300/160000]	lr: 4.578e-05, eta: 14:43:44, time: 0.679, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2361, decode.acc_seg: 90.4688, loss: 0.2361
2023-11-11 03:14:25,717 - mmseg - INFO - Iter [84350/160000]	lr: 4.574e-05, eta: 14:43:10, time: 0.721, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2361, decode.acc_seg: 90.3361, loss: 0.2361
2023-11-11 03:15:01,410 - mmseg - INFO - Iter [84400/160000]	lr: 4.569e-05, eta: 14:42:35, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2448, decode.acc_seg: 89.8677, loss: 0.2448
2023-11-11 03:15:36,728 - mmseg - INFO - Iter [84450/160000]	lr: 4.564e-05, eta: 14:42:01, time: 0.707, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2269, decode.acc_seg: 90.7156, loss: 0.2269
2023-11-11 03:16:12,166 - mmseg - INFO - Iter [84500/160000]	lr: 4.559e-05, eta: 14:41:26, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2401, decode.acc_seg: 90.3146, loss: 0.2401
2023-11-11 03:16:47,853 - mmseg - INFO - Iter [84550/160000]	lr: 4.554e-05, eta: 14:40:52, time: 0.713, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2125, decode.acc_seg: 91.2650, loss: 0.2125
2023-11-11 03:17:23,330 - mmseg - INFO - Iter [84600/160000]	lr: 4.549e-05, eta: 14:40:17, time: 0.710, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2309, decode.acc_seg: 90.5913, loss: 0.2309
2023-11-11 03:17:57,066 - mmseg - INFO - Iter [84650/160000]	lr: 4.544e-05, eta: 14:39:41, time: 0.676, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2416, decode.acc_seg: 90.2494, loss: 0.2416
2023-11-11 03:18:31,853 - mmseg - INFO - Iter [84700/160000]	lr: 4.539e-05, eta: 14:39:06, time: 0.696, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2260, decode.acc_seg: 90.8015, loss: 0.2260
2023-11-11 03:19:04,288 - mmseg - INFO - Iter [84750/160000]	lr: 4.534e-05, eta: 14:38:28, time: 0.648, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2235, decode.acc_seg: 90.5932, loss: 0.2235
2023-11-11 03:19:39,603 - mmseg - INFO - Iter [84800/160000]	lr: 4.530e-05, eta: 14:37:54, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2255, decode.acc_seg: 90.5678, loss: 0.2255
2023-11-11 03:20:14,785 - mmseg - INFO - Iter [84850/160000]	lr: 4.525e-05, eta: 14:37:19, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2317, decode.acc_seg: 90.5923, loss: 0.2317
2023-11-11 03:20:49,429 - mmseg - INFO - Iter [84900/160000]	lr: 4.520e-05, eta: 14:36:43, time: 0.693, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2210, decode.acc_seg: 90.7028, loss: 0.2210
2023-11-11 03:21:24,917 - mmseg - INFO - Iter [84950/160000]	lr: 4.515e-05, eta: 14:36:09, time: 0.710, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2173, decode.acc_seg: 91.0599, loss: 0.2173
2023-11-11 03:22:00,751 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 03:22:00,751 - mmseg - INFO - Iter [85000/160000]	lr: 4.510e-05, eta: 14:35:34, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2415, decode.acc_seg: 90.2892, loss: 0.2415
2023-11-11 03:22:36,068 - mmseg - INFO - Iter [85050/160000]	lr: 4.505e-05, eta: 14:35:00, time: 0.706, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2294, decode.acc_seg: 90.5884, loss: 0.2294
2023-11-11 03:23:10,792 - mmseg - INFO - Iter [85100/160000]	lr: 4.500e-05, eta: 14:34:24, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2293, decode.acc_seg: 90.7122, loss: 0.2293
2023-11-11 03:23:46,457 - mmseg - INFO - Iter [85150/160000]	lr: 4.495e-05, eta: 14:33:50, time: 0.712, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2386, decode.acc_seg: 90.5080, loss: 0.2386
2023-11-11 03:24:18,937 - mmseg - INFO - Iter [85200/160000]	lr: 4.490e-05, eta: 14:33:13, time: 0.650, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2251, decode.acc_seg: 91.0281, loss: 0.2251
2023-11-11 03:24:53,209 - mmseg - INFO - Iter [85250/160000]	lr: 4.486e-05, eta: 14:32:37, time: 0.686, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2195, decode.acc_seg: 90.9769, loss: 0.2195
2023-11-11 03:25:25,465 - mmseg - INFO - Iter [85300/160000]	lr: 4.481e-05, eta: 14:32:00, time: 0.645, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2138, decode.acc_seg: 91.1884, loss: 0.2138
2023-11-11 03:25:59,393 - mmseg - INFO - Iter [85350/160000]	lr: 4.476e-05, eta: 14:31:23, time: 0.677, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2370, decode.acc_seg: 90.4365, loss: 0.2370
2023-11-11 03:26:34,377 - mmseg - INFO - Iter [85400/160000]	lr: 4.471e-05, eta: 14:30:48, time: 0.700, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2288, decode.acc_seg: 90.8249, loss: 0.2288
2023-11-11 03:27:08,072 - mmseg - INFO - Iter [85450/160000]	lr: 4.466e-05, eta: 14:30:12, time: 0.674, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2322, decode.acc_seg: 90.5373, loss: 0.2322
2023-11-11 03:27:44,033 - mmseg - INFO - Iter [85500/160000]	lr: 4.461e-05, eta: 14:29:38, time: 0.719, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2259, decode.acc_seg: 90.4593, loss: 0.2259
2023-11-11 03:28:19,621 - mmseg - INFO - Iter [85550/160000]	lr: 4.456e-05, eta: 14:29:04, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2430, decode.acc_seg: 90.1959, loss: 0.2430
2023-11-11 03:28:55,099 - mmseg - INFO - Iter [85600/160000]	lr: 4.451e-05, eta: 14:28:29, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2529, decode.acc_seg: 89.9657, loss: 0.2529
2023-11-11 03:29:30,418 - mmseg - INFO - Iter [85650/160000]	lr: 4.447e-05, eta: 14:27:54, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2340, decode.acc_seg: 90.3772, loss: 0.2340
2023-11-11 03:30:05,901 - mmseg - INFO - Iter [85700/160000]	lr: 4.442e-05, eta: 14:27:20, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2297, decode.acc_seg: 90.7106, loss: 0.2297
2023-11-11 03:30:41,265 - mmseg - INFO - Iter [85750/160000]	lr: 4.437e-05, eta: 14:26:45, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2221, decode.acc_seg: 91.0539, loss: 0.2221
2023-11-11 03:31:16,808 - mmseg - INFO - Iter [85800/160000]	lr: 4.432e-05, eta: 14:26:10, time: 0.711, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2178, decode.acc_seg: 91.0012, loss: 0.2178
2023-11-11 03:31:52,434 - mmseg - INFO - Iter [85850/160000]	lr: 4.427e-05, eta: 14:25:36, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2243, decode.acc_seg: 90.8916, loss: 0.2243
2023-11-11 03:32:26,371 - mmseg - INFO - Iter [85900/160000]	lr: 4.422e-05, eta: 14:25:00, time: 0.679, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2217, decode.acc_seg: 91.1286, loss: 0.2217
2023-11-11 03:32:59,731 - mmseg - INFO - Iter [85950/160000]	lr: 4.417e-05, eta: 14:24:23, time: 0.668, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2240, decode.acc_seg: 90.9588, loss: 0.2240
2023-11-11 03:33:33,271 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 03:33:33,271 - mmseg - INFO - Iter [86000/160000]	lr: 4.412e-05, eta: 14:23:47, time: 0.670, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2270, decode.acc_seg: 90.7803, loss: 0.2270
2023-11-11 03:34:08,293 - mmseg - INFO - Iter [86050/160000]	lr: 4.408e-05, eta: 14:23:12, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2139, decode.acc_seg: 91.3114, loss: 0.2139
2023-11-11 03:34:43,208 - mmseg - INFO - Iter [86100/160000]	lr: 4.403e-05, eta: 14:22:37, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2339, decode.acc_seg: 90.7615, loss: 0.2339
2023-11-11 03:35:18,187 - mmseg - INFO - Iter [86150/160000]	lr: 4.398e-05, eta: 14:22:02, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2499, decode.acc_seg: 89.9253, loss: 0.2499
2023-11-11 03:35:53,225 - mmseg - INFO - Iter [86200/160000]	lr: 4.393e-05, eta: 14:21:27, time: 0.701, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2355, decode.acc_seg: 90.1913, loss: 0.2355
2023-11-11 03:36:28,393 - mmseg - INFO - Iter [86250/160000]	lr: 4.388e-05, eta: 14:20:52, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2326, decode.acc_seg: 90.6341, loss: 0.2326
2023-11-11 03:37:01,762 - mmseg - INFO - Iter [86300/160000]	lr: 4.383e-05, eta: 14:20:16, time: 0.667, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2261, decode.acc_seg: 91.0072, loss: 0.2261
2023-11-11 03:37:36,427 - mmseg - INFO - Iter [86350/160000]	lr: 4.378e-05, eta: 14:19:40, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2354, decode.acc_seg: 90.4233, loss: 0.2354
2023-11-11 03:38:09,377 - mmseg - INFO - Iter [86400/160000]	lr: 4.373e-05, eta: 14:19:04, time: 0.659, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2216, decode.acc_seg: 91.1612, loss: 0.2216
2023-11-11 03:38:42,238 - mmseg - INFO - Iter [86450/160000]	lr: 4.369e-05, eta: 14:18:27, time: 0.657, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2136, decode.acc_seg: 90.9490, loss: 0.2136
2023-11-11 03:39:16,230 - mmseg - INFO - Iter [86500/160000]	lr: 4.364e-05, eta: 14:17:51, time: 0.680, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2303, decode.acc_seg: 90.6514, loss: 0.2303
2023-11-11 03:39:49,713 - mmseg - INFO - Iter [86550/160000]	lr: 4.359e-05, eta: 14:17:14, time: 0.668, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2137, decode.acc_seg: 90.8584, loss: 0.2137
2023-11-11 03:40:25,073 - mmseg - INFO - Iter [86600/160000]	lr: 4.354e-05, eta: 14:16:40, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2202, decode.acc_seg: 90.7385, loss: 0.2202
2023-11-11 03:40:59,102 - mmseg - INFO - Iter [86650/160000]	lr: 4.349e-05, eta: 14:16:04, time: 0.682, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2231, decode.acc_seg: 90.8532, loss: 0.2231
2023-11-11 03:41:30,438 - mmseg - INFO - Iter [86700/160000]	lr: 4.344e-05, eta: 14:15:26, time: 0.627, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2141, decode.acc_seg: 90.9120, loss: 0.2141
2023-11-11 03:42:05,589 - mmseg - INFO - Iter [86750/160000]	lr: 4.339e-05, eta: 14:14:51, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2273, decode.acc_seg: 90.3480, loss: 0.2273
2023-11-11 03:42:41,290 - mmseg - INFO - Iter [86800/160000]	lr: 4.334e-05, eta: 14:14:16, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2306, decode.acc_seg: 90.5669, loss: 0.2306
2023-11-11 03:43:16,778 - mmseg - INFO - Iter [86850/160000]	lr: 4.330e-05, eta: 14:13:42, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2403, decode.acc_seg: 90.0874, loss: 0.2403
2023-11-11 03:43:52,226 - mmseg - INFO - Iter [86900/160000]	lr: 4.325e-05, eta: 14:13:07, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2441, decode.acc_seg: 90.0746, loss: 0.2441
2023-11-11 03:44:27,359 - mmseg - INFO - Iter [86950/160000]	lr: 4.320e-05, eta: 14:12:32, time: 0.702, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2206, decode.acc_seg: 90.8084, loss: 0.2206
2023-11-11 03:45:02,329 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 03:45:02,329 - mmseg - INFO - Iter [87000/160000]	lr: 4.315e-05, eta: 14:11:57, time: 0.700, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2253, decode.acc_seg: 90.9849, loss: 0.2253
2023-11-11 03:45:37,699 - mmseg - INFO - Iter [87050/160000]	lr: 4.310e-05, eta: 14:11:22, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2323, decode.acc_seg: 90.5578, loss: 0.2323
2023-11-11 03:46:12,689 - mmseg - INFO - Iter [87100/160000]	lr: 4.305e-05, eta: 14:10:47, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2117, decode.acc_seg: 91.0760, loss: 0.2117
2023-11-11 03:46:44,245 - mmseg - INFO - Iter [87150/160000]	lr: 4.300e-05, eta: 14:10:10, time: 0.632, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2181, decode.acc_seg: 90.9285, loss: 0.2181
2023-11-11 03:47:16,355 - mmseg - INFO - Iter [87200/160000]	lr: 4.296e-05, eta: 14:09:32, time: 0.642, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2277, decode.acc_seg: 90.9386, loss: 0.2277
2023-11-11 03:47:48,275 - mmseg - INFO - Iter [87250/160000]	lr: 4.291e-05, eta: 14:08:55, time: 0.638, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2322, decode.acc_seg: 90.7470, loss: 0.2322
2023-11-11 03:48:22,477 - mmseg - INFO - Iter [87300/160000]	lr: 4.286e-05, eta: 14:08:19, time: 0.683, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2253, decode.acc_seg: 90.8073, loss: 0.2253
2023-11-11 03:48:57,829 - mmseg - INFO - Iter [87350/160000]	lr: 4.281e-05, eta: 14:07:44, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2286, decode.acc_seg: 90.5707, loss: 0.2286
2023-11-11 03:49:33,116 - mmseg - INFO - Iter [87400/160000]	lr: 4.276e-05, eta: 14:07:09, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2367, decode.acc_seg: 90.6606, loss: 0.2367
2023-11-11 03:50:05,416 - mmseg - INFO - Iter [87450/160000]	lr: 4.271e-05, eta: 14:06:32, time: 0.647, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2362, decode.acc_seg: 90.3789, loss: 0.2362
2023-11-11 03:50:37,221 - mmseg - INFO - Iter [87500/160000]	lr: 4.266e-05, eta: 14:05:54, time: 0.635, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2337, decode.acc_seg: 90.5396, loss: 0.2337
2023-11-11 03:51:12,410 - mmseg - INFO - Iter [87550/160000]	lr: 4.262e-05, eta: 14:05:20, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2262, decode.acc_seg: 90.8918, loss: 0.2262
2023-11-11 03:51:47,428 - mmseg - INFO - Iter [87600/160000]	lr: 4.257e-05, eta: 14:04:45, time: 0.700, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2119, decode.acc_seg: 91.0784, loss: 0.2119
2023-11-11 03:52:22,355 - mmseg - INFO - Iter [87650/160000]	lr: 4.252e-05, eta: 14:04:10, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2243, decode.acc_seg: 90.9878, loss: 0.2243
2023-11-11 03:52:55,830 - mmseg - INFO - Iter [87700/160000]	lr: 4.247e-05, eta: 14:03:33, time: 0.669, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2273, decode.acc_seg: 90.8786, loss: 0.2273
2023-11-11 03:53:31,252 - mmseg - INFO - Iter [87750/160000]	lr: 4.242e-05, eta: 14:02:59, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2389, decode.acc_seg: 90.3556, loss: 0.2389
2023-11-11 03:54:06,634 - mmseg - INFO - Iter [87800/160000]	lr: 4.237e-05, eta: 14:02:24, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2288, decode.acc_seg: 90.7641, loss: 0.2288
2023-11-11 03:54:41,945 - mmseg - INFO - Iter [87850/160000]	lr: 4.232e-05, eta: 14:01:49, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2145, decode.acc_seg: 91.3135, loss: 0.2145
2023-11-11 03:55:16,654 - mmseg - INFO - Iter [87900/160000]	lr: 4.228e-05, eta: 14:01:14, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2239, decode.acc_seg: 90.8909, loss: 0.2239
2023-11-11 03:55:48,895 - mmseg - INFO - Iter [87950/160000]	lr: 4.223e-05, eta: 14:00:37, time: 0.644, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2259, decode.acc_seg: 90.7837, loss: 0.2259
2023-11-11 03:56:23,988 - mmseg - INFO - Saving checkpoint at 88000 iterations
2023-11-11 03:56:28,525 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 03:56:28,525 - mmseg - INFO - Iter [88000/160000]	lr: 4.218e-05, eta: 14:00:05, time: 0.794, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2515, decode.acc_seg: 90.3313, loss: 0.2515
2023-11-11 03:57:59,059 - mmseg - INFO - per class results:
2023-11-11 03:57:59,073 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 77.66 | 87.92 |
|       building      | 82.11 | 92.48 |
|         sky         | 93.86 |  97.3 |
|        floor        | 81.68 | 90.49 |
|         tree        | 74.19 | 87.26 |
|       ceiling       | 82.87 |  92.9 |
|         road        | 80.66 | 86.97 |
|         bed         | 89.04 | 94.95 |
|      windowpane     | 61.02 | 77.63 |
|        grass        | 67.53 | 86.41 |
|       cabinet       | 58.52 | 73.87 |
|       sidewalk      | 63.91 | 82.83 |
|        person       | 80.32 | 92.53 |
|        earth        | 39.23 | 53.68 |
|         door        | 48.11 | 60.63 |
|        table        | 60.94 | 74.09 |
|       mountain      | 54.05 | 68.49 |
|        plant        | 53.71 | 64.91 |
|       curtain       | 72.42 | 82.71 |
|        chair        |  55.3 | 68.24 |
|         car         | 83.46 | 92.63 |
|        water        | 59.44 | 75.69 |
|       painting      | 69.34 | 86.04 |
|         sofa        | 64.73 | 85.32 |
|        shelf        | 42.74 | 61.82 |
|        house        | 32.35 | 40.68 |
|         sea         | 60.72 | 82.01 |
|        mirror       | 68.17 | 75.84 |
|         rug         | 63.71 | 78.34 |
|        field        | 29.92 | 37.18 |
|       armchair      | 40.96 | 58.09 |
|         seat        | 59.63 | 82.59 |
|        fence        | 40.71 | 54.25 |
|         desk        | 48.08 | 65.55 |
|         rock        | 43.02 |  68.5 |
|       wardrobe      | 47.56 | 70.12 |
|         lamp        | 62.79 | 76.48 |
|       bathtub       | 79.46 | 87.03 |
|       railing       | 33.24 | 45.84 |
|       cushion       | 57.36 | 71.05 |
|         base        | 29.61 | 35.25 |
|         box         | 26.03 |  32.8 |
|        column       | 43.33 | 52.41 |
|      signboard      | 38.41 | 53.41 |
|   chest of drawers  | 40.99 | 55.69 |
|       counter       | 41.91 | 51.23 |
|         sand        | 50.18 | 68.22 |
|         sink        | 69.98 | 79.84 |
|      skyscraper     | 45.84 | 58.06 |
|      fireplace      | 71.38 | 91.43 |
|     refrigerator    |  74.6 | 84.99 |
|      grandstand     | 43.17 | 67.88 |
|         path        | 23.05 | 36.28 |
|        stairs       | 29.16 | 37.64 |
|        runway       | 64.14 | 82.46 |
|         case        | 42.64 | 49.63 |
|      pool table     | 93.12 | 97.05 |
|        pillow       | 58.09 | 68.95 |
|     screen door     | 68.06 | 76.53 |
|       stairway      | 25.13 | 37.83 |
|        river        |  9.57 | 14.39 |
|        bridge       | 62.05 | 83.23 |
|       bookcase      | 38.69 | 54.27 |
|        blind        | 41.59 | 48.51 |
|     coffee table    | 50.59 | 83.54 |
|        toilet       |  85.9 | 91.83 |
|        flower       | 35.65 | 52.12 |
|         book        | 42.97 | 69.51 |
|         hill        |  9.4  | 10.94 |
|        bench        | 39.93 | 45.52 |
|      countertop     | 58.98 | 84.46 |
|        stove        | 70.56 | 82.34 |
|         palm        | 45.03 | 76.28 |
|    kitchen island   | 37.28 | 63.54 |
|       computer      | 68.48 |  82.6 |
|     swivel chair    |  48.8 | 65.95 |
|         boat        | 43.06 | 57.49 |
|         bar         | 50.73 | 70.82 |
|    arcade machine   | 39.53 |  43.1 |
|        hovel        | 54.38 | 64.46 |
|         bus         | 84.46 | 93.82 |
|        towel        | 69.74 | 78.91 |
|        light        |  53.1 | 65.12 |
|        truck        | 31.16 | 37.51 |
|        tower        |  5.84 |  8.31 |
|      chandelier     | 64.81 | 84.53 |
|        awning       | 26.22 | 34.27 |
|     streetlight     | 27.97 |  36.7 |
|        booth        | 56.52 |  58.7 |
| television receiver | 66.02 | 82.93 |
|       airplane      | 35.78 | 67.72 |
|      dirt track     |  4.12 | 18.37 |
|       apparel       | 46.74 | 59.16 |
|         pole        | 26.82 | 42.16 |
|         land        |  0.2  |  0.4  |
|      bannister      | 14.35 | 19.77 |
|      escalator      | 50.38 | 74.27 |
|       ottoman       | 36.11 | 43.75 |
|        bottle       | 35.43 |  62.3 |
|        buffet       | 35.93 | 40.39 |
|        poster       | 35.13 | 47.63 |
|        stage        | 20.87 | 28.43 |
|         van         | 42.18 | 54.67 |
|         ship        | 18.94 |  25.8 |
|       fountain      | 41.68 | 41.88 |
|    conveyer belt    | 78.17 | 92.98 |
|        canopy       | 21.01 | 31.59 |
|        washer       | 72.86 |  76.2 |
|      plaything      | 23.58 | 38.14 |
|    swimming pool    | 72.43 |  77.9 |
|        stool        | 33.26 | 47.39 |
|        barrel       | 69.25 | 85.52 |
|        basket       | 33.77 | 41.95 |
|      waterfall      | 73.07 | 89.87 |
|         tent        | 95.99 | 98.38 |
|         bag         | 11.59 | 13.35 |
|       minibike      |  58.8 | 69.47 |
|        cradle       | 79.62 | 97.76 |
|         oven        | 46.47 | 64.46 |
|         ball        | 41.05 | 59.96 |
|         food        | 43.76 | 49.28 |
|         step        |  4.7  |  5.76 |
|         tank        | 39.12 | 42.97 |
|      trade name     |  28.0 | 32.61 |
|      microwave      |  80.5 | 93.34 |
|         pot         | 40.01 | 46.67 |
|        animal       | 54.87 | 57.02 |
|       bicycle       | 57.69 | 79.64 |
|         lake        | 62.84 | 63.38 |
|      dishwasher     | 52.57 | 61.48 |
|        screen       | 62.07 | 83.08 |
|       blanket       | 22.64 | 29.25 |
|      sculpture      | 60.41 | 83.71 |
|         hood        | 54.66 | 66.59 |
|        sconce       | 47.95 | 57.59 |
|         vase        | 35.86 |  53.1 |
|    traffic light    | 26.79 |  40.5 |
|         tray        |  6.23 | 11.23 |
|        ashcan       | 45.12 |  55.2 |
|         fan         | 57.41 | 73.68 |
|         pier        | 17.24 | 19.41 |
|      crt screen     |  9.45 | 24.85 |
|        plate        | 38.02 |  51.8 |
|       monitor       | 22.64 | 27.65 |
|    bulletin board   | 39.71 | 54.73 |
|        shower       |  2.38 |  3.02 |
|       radiator      |  64.0 | 72.55 |
|        glass        | 11.13 | 12.08 |
|        clock        | 28.64 | 33.12 |
|         flag        | 43.94 | 48.33 |
+---------------------+-------+-------+
2023-11-11 03:57:59,073 - mmseg - INFO - Summary:
2023-11-11 03:57:59,073 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 82.77 | 48.52 | 60.67 |
+-------+-------+-------+
2023-11-11 03:57:59,218 - mmseg - INFO - The previous best checkpoint /data/Next-ViT/segmentation/work_dirs/fpn_512_debi_base_160k/best_mIoU_iter_80000.pth was removed
2023-11-11 03:58:02,174 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_88000.pth.
2023-11-11 03:58:02,174 - mmseg - INFO - Best mIoU is 0.4852 at 88000 iter.
2023-11-11 03:58:02,216 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 03:58:02,216 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8277, mIoU: 0.4852, mAcc: 0.6067, IoU.wall: 0.7766, IoU.building: 0.8211, IoU.sky: 0.9386, IoU.floor: 0.8168, IoU.tree: 0.7419, IoU.ceiling: 0.8287, IoU.road: 0.8066, IoU.bed : 0.8904, IoU.windowpane: 0.6102, IoU.grass: 0.6753, IoU.cabinet: 0.5852, IoU.sidewalk: 0.6391, IoU.person: 0.8032, IoU.earth: 0.3923, IoU.door: 0.4811, IoU.table: 0.6094, IoU.mountain: 0.5405, IoU.plant: 0.5371, IoU.curtain: 0.7242, IoU.chair: 0.5530, IoU.car: 0.8346, IoU.water: 0.5944, IoU.painting: 0.6934, IoU.sofa: 0.6473, IoU.shelf: 0.4274, IoU.house: 0.3235, IoU.sea: 0.6072, IoU.mirror: 0.6817, IoU.rug: 0.6371, IoU.field: 0.2992, IoU.armchair: 0.4096, IoU.seat: 0.5963, IoU.fence: 0.4071, IoU.desk: 0.4808, IoU.rock: 0.4302, IoU.wardrobe: 0.4756, IoU.lamp: 0.6279, IoU.bathtub: 0.7946, IoU.railing: 0.3324, IoU.cushion: 0.5736, IoU.base: 0.2961, IoU.box: 0.2603, IoU.column: 0.4333, IoU.signboard: 0.3841, IoU.chest of drawers: 0.4099, IoU.counter: 0.4191, IoU.sand: 0.5018, IoU.sink: 0.6998, IoU.skyscraper: 0.4584, IoU.fireplace: 0.7138, IoU.refrigerator: 0.7460, IoU.grandstand: 0.4317, IoU.path: 0.2305, IoU.stairs: 0.2916, IoU.runway: 0.6414, IoU.case: 0.4264, IoU.pool table: 0.9312, IoU.pillow: 0.5809, IoU.screen door: 0.6806, IoU.stairway: 0.2513, IoU.river: 0.0957, IoU.bridge: 0.6205, IoU.bookcase: 0.3869, IoU.blind: 0.4159, IoU.coffee table: 0.5059, IoU.toilet: 0.8590, IoU.flower: 0.3565, IoU.book: 0.4297, IoU.hill: 0.0940, IoU.bench: 0.3993, IoU.countertop: 0.5898, IoU.stove: 0.7056, IoU.palm: 0.4503, IoU.kitchen island: 0.3728, IoU.computer: 0.6848, IoU.swivel chair: 0.4880, IoU.boat: 0.4306, IoU.bar: 0.5073, IoU.arcade machine: 0.3953, IoU.hovel: 0.5438, IoU.bus: 0.8446, IoU.towel: 0.6974, IoU.light: 0.5310, IoU.truck: 0.3116, IoU.tower: 0.0584, IoU.chandelier: 0.6481, IoU.awning: 0.2622, IoU.streetlight: 0.2797, IoU.booth: 0.5652, IoU.television receiver: 0.6602, IoU.airplane: 0.3578, IoU.dirt track: 0.0412, IoU.apparel: 0.4674, IoU.pole: 0.2682, IoU.land: 0.0020, IoU.bannister: 0.1435, IoU.escalator: 0.5038, IoU.ottoman: 0.3611, IoU.bottle: 0.3543, IoU.buffet: 0.3593, IoU.poster: 0.3513, IoU.stage: 0.2087, IoU.van: 0.4218, IoU.ship: 0.1894, IoU.fountain: 0.4168, IoU.conveyer belt: 0.7817, IoU.canopy: 0.2101, IoU.washer: 0.7286, IoU.plaything: 0.2358, IoU.swimming pool: 0.7243, IoU.stool: 0.3326, IoU.barrel: 0.6925, IoU.basket: 0.3377, IoU.waterfall: 0.7307, IoU.tent: 0.9599, IoU.bag: 0.1159, IoU.minibike: 0.5880, IoU.cradle: 0.7962, IoU.oven: 0.4647, IoU.ball: 0.4105, IoU.food: 0.4376, IoU.step: 0.0470, IoU.tank: 0.3912, IoU.trade name: 0.2800, IoU.microwave: 0.8050, IoU.pot: 0.4001, IoU.animal: 0.5487, IoU.bicycle: 0.5769, IoU.lake: 0.6284, IoU.dishwasher: 0.5257, IoU.screen: 0.6207, IoU.blanket: 0.2264, IoU.sculpture: 0.6041, IoU.hood: 0.5466, IoU.sconce: 0.4795, IoU.vase: 0.3586, IoU.traffic light: 0.2679, IoU.tray: 0.0623, IoU.ashcan: 0.4512, IoU.fan: 0.5741, IoU.pier: 0.1724, IoU.crt screen: 0.0945, IoU.plate: 0.3802, IoU.monitor: 0.2264, IoU.bulletin board: 0.3971, IoU.shower: 0.0238, IoU.radiator: 0.6400, IoU.glass: 0.1113, IoU.clock: 0.2864, IoU.flag: 0.4394, Acc.wall: 0.8792, Acc.building: 0.9248, Acc.sky: 0.9730, Acc.floor: 0.9049, Acc.tree: 0.8726, Acc.ceiling: 0.9290, Acc.road: 0.8697, Acc.bed : 0.9495, Acc.windowpane: 0.7763, Acc.grass: 0.8641, Acc.cabinet: 0.7387, Acc.sidewalk: 0.8283, Acc.person: 0.9253, Acc.earth: 0.5368, Acc.door: 0.6063, Acc.table: 0.7409, Acc.mountain: 0.6849, Acc.plant: 0.6491, Acc.curtain: 0.8271, Acc.chair: 0.6824, Acc.car: 0.9263, Acc.water: 0.7569, Acc.painting: 0.8604, Acc.sofa: 0.8532, Acc.shelf: 0.6182, Acc.house: 0.4068, Acc.sea: 0.8201, Acc.mirror: 0.7584, Acc.rug: 0.7834, Acc.field: 0.3718, Acc.armchair: 0.5809, Acc.seat: 0.8259, Acc.fence: 0.5425, Acc.desk: 0.6555, Acc.rock: 0.6850, Acc.wardrobe: 0.7012, Acc.lamp: 0.7648, Acc.bathtub: 0.8703, Acc.railing: 0.4584, Acc.cushion: 0.7105, Acc.base: 0.3525, Acc.box: 0.3280, Acc.column: 0.5241, Acc.signboard: 0.5341, Acc.chest of drawers: 0.5569, Acc.counter: 0.5123, Acc.sand: 0.6822, Acc.sink: 0.7984, Acc.skyscraper: 0.5806, Acc.fireplace: 0.9143, Acc.refrigerator: 0.8499, Acc.grandstand: 0.6788, Acc.path: 0.3628, Acc.stairs: 0.3764, Acc.runway: 0.8246, Acc.case: 0.4963, Acc.pool table: 0.9705, Acc.pillow: 0.6895, Acc.screen door: 0.7653, Acc.stairway: 0.3783, Acc.river: 0.1439, Acc.bridge: 0.8323, Acc.bookcase: 0.5427, Acc.blind: 0.4851, Acc.coffee table: 0.8354, Acc.toilet: 0.9183, Acc.flower: 0.5212, Acc.book: 0.6951, Acc.hill: 0.1094, Acc.bench: 0.4552, Acc.countertop: 0.8446, Acc.stove: 0.8234, Acc.palm: 0.7628, Acc.kitchen island: 0.6354, Acc.computer: 0.8260, Acc.swivel chair: 0.6595, Acc.boat: 0.5749, Acc.bar: 0.7082, Acc.arcade machine: 0.4310, Acc.hovel: 0.6446, Acc.bus: 0.9382, Acc.towel: 0.7891, Acc.light: 0.6512, Acc.truck: 0.3751, Acc.tower: 0.0831, Acc.chandelier: 0.8453, Acc.awning: 0.3427, Acc.streetlight: 0.3670, Acc.booth: 0.5870, Acc.television receiver: 0.8293, Acc.airplane: 0.6772, Acc.dirt track: 0.1837, Acc.apparel: 0.5916, Acc.pole: 0.4216, Acc.land: 0.0040, Acc.bannister: 0.1977, Acc.escalator: 0.7427, Acc.ottoman: 0.4375, Acc.bottle: 0.6230, Acc.buffet: 0.4039, Acc.poster: 0.4763, Acc.stage: 0.2843, Acc.van: 0.5467, Acc.ship: 0.2580, Acc.fountain: 0.4188, Acc.conveyer belt: 0.9298, Acc.canopy: 0.3159, Acc.washer: 0.7620, Acc.plaything: 0.3814, Acc.swimming pool: 0.7790, Acc.stool: 0.4739, Acc.barrel: 0.8552, Acc.basket: 0.4195, Acc.waterfall: 0.8987, Acc.tent: 0.9838, Acc.bag: 0.1335, Acc.minibike: 0.6947, Acc.cradle: 0.9776, Acc.oven: 0.6446, Acc.ball: 0.5996, Acc.food: 0.4928, Acc.step: 0.0576, Acc.tank: 0.4297, Acc.trade name: 0.3261, Acc.microwave: 0.9334, Acc.pot: 0.4667, Acc.animal: 0.5702, Acc.bicycle: 0.7964, Acc.lake: 0.6338, Acc.dishwasher: 0.6148, Acc.screen: 0.8308, Acc.blanket: 0.2925, Acc.sculpture: 0.8371, Acc.hood: 0.6659, Acc.sconce: 0.5759, Acc.vase: 0.5310, Acc.traffic light: 0.4050, Acc.tray: 0.1123, Acc.ashcan: 0.5520, Acc.fan: 0.7368, Acc.pier: 0.1941, Acc.crt screen: 0.2485, Acc.plate: 0.5180, Acc.monitor: 0.2765, Acc.bulletin board: 0.5473, Acc.shower: 0.0302, Acc.radiator: 0.7255, Acc.glass: 0.1208, Acc.clock: 0.3312, Acc.flag: 0.4833
2023-11-11 03:58:36,464 - mmseg - INFO - Iter [88050/160000]	lr: 4.213e-05, eta: 14:00:46, time: 2.559, data_time: 1.883, memory: 23129, decode.loss_ce: 0.2356, decode.acc_seg: 90.5620, loss: 0.2356
2023-11-11 03:59:09,685 - mmseg - INFO - Iter [88100/160000]	lr: 4.208e-05, eta: 14:00:10, time: 0.664, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2258, decode.acc_seg: 90.8927, loss: 0.2258
2023-11-11 03:59:43,464 - mmseg - INFO - Iter [88150/160000]	lr: 4.203e-05, eta: 13:59:34, time: 0.674, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2359, decode.acc_seg: 90.5465, loss: 0.2359
2023-11-11 04:00:17,339 - mmseg - INFO - Iter [88200/160000]	lr: 4.199e-05, eta: 13:58:58, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2176, decode.acc_seg: 90.7845, loss: 0.2176
2023-11-11 04:00:51,151 - mmseg - INFO - Iter [88250/160000]	lr: 4.194e-05, eta: 13:58:22, time: 0.675, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2338, decode.acc_seg: 90.4584, loss: 0.2338
2023-11-11 04:01:26,461 - mmseg - INFO - Iter [88300/160000]	lr: 4.189e-05, eta: 13:57:47, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2232, decode.acc_seg: 90.7135, loss: 0.2232
2023-11-11 04:02:01,585 - mmseg - INFO - Iter [88350/160000]	lr: 4.184e-05, eta: 13:57:12, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2234, decode.acc_seg: 91.0639, loss: 0.2234
2023-11-11 04:02:35,807 - mmseg - INFO - Iter [88400/160000]	lr: 4.179e-05, eta: 13:56:36, time: 0.684, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2190, decode.acc_seg: 90.9345, loss: 0.2190
2023-11-11 04:03:12,103 - mmseg - INFO - Iter [88450/160000]	lr: 4.174e-05, eta: 13:56:02, time: 0.726, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2186, decode.acc_seg: 90.7937, loss: 0.2186
2023-11-11 04:03:46,220 - mmseg - INFO - Iter [88500/160000]	lr: 4.169e-05, eta: 13:55:26, time: 0.683, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2108, decode.acc_seg: 91.3601, loss: 0.2108
2023-11-11 04:04:20,191 - mmseg - INFO - Iter [88550/160000]	lr: 4.165e-05, eta: 13:54:50, time: 0.679, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2439, decode.acc_seg: 90.2374, loss: 0.2439
2023-11-11 04:04:52,366 - mmseg - INFO - Iter [88600/160000]	lr: 4.160e-05, eta: 13:54:13, time: 0.644, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2262, decode.acc_seg: 90.7599, loss: 0.2262
2023-11-11 04:05:26,062 - mmseg - INFO - Iter [88650/160000]	lr: 4.155e-05, eta: 13:53:37, time: 0.673, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2208, decode.acc_seg: 90.8108, loss: 0.2208
2023-11-11 04:06:00,556 - mmseg - INFO - Iter [88700/160000]	lr: 4.150e-05, eta: 13:53:01, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2261, decode.acc_seg: 90.4664, loss: 0.2261
2023-11-11 04:06:35,896 - mmseg - INFO - Iter [88750/160000]	lr: 4.145e-05, eta: 13:52:26, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2242, decode.acc_seg: 90.7572, loss: 0.2242
2023-11-11 04:07:09,766 - mmseg - INFO - Iter [88800/160000]	lr: 4.140e-05, eta: 13:51:50, time: 0.678, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2247, decode.acc_seg: 90.8363, loss: 0.2247
2023-11-11 04:07:44,151 - mmseg - INFO - Iter [88850/160000]	lr: 4.136e-05, eta: 13:51:15, time: 0.687, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2276, decode.acc_seg: 90.7157, loss: 0.2276
2023-11-11 04:08:18,770 - mmseg - INFO - Iter [88900/160000]	lr: 4.131e-05, eta: 13:50:39, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2064, decode.acc_seg: 91.4058, loss: 0.2064
2023-11-11 04:08:54,552 - mmseg - INFO - Iter [88950/160000]	lr: 4.126e-05, eta: 13:50:05, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2184, decode.acc_seg: 91.2362, loss: 0.2184
2023-11-11 04:09:30,047 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 04:09:30,047 - mmseg - INFO - Iter [89000/160000]	lr: 4.121e-05, eta: 13:49:30, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2248, decode.acc_seg: 90.9923, loss: 0.2248
2023-11-11 04:10:05,315 - mmseg - INFO - Iter [89050/160000]	lr: 4.116e-05, eta: 13:48:55, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2421, decode.acc_seg: 90.4211, loss: 0.2421
2023-11-11 04:10:38,952 - mmseg - INFO - Iter [89100/160000]	lr: 4.111e-05, eta: 13:48:19, time: 0.673, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2225, decode.acc_seg: 90.7901, loss: 0.2225
2023-11-11 04:11:13,895 - mmseg - INFO - Iter [89150/160000]	lr: 4.107e-05, eta: 13:47:44, time: 0.698, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2394, decode.acc_seg: 90.1862, loss: 0.2394
2023-11-11 04:11:47,359 - mmseg - INFO - Iter [89200/160000]	lr: 4.102e-05, eta: 13:47:08, time: 0.670, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2307, decode.acc_seg: 90.7456, loss: 0.2307
2023-11-11 04:12:20,727 - mmseg - INFO - Iter [89250/160000]	lr: 4.097e-05, eta: 13:46:31, time: 0.666, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2223, decode.acc_seg: 91.0358, loss: 0.2223
2023-11-11 04:12:55,238 - mmseg - INFO - Iter [89300/160000]	lr: 4.092e-05, eta: 13:45:56, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2191, decode.acc_seg: 91.0530, loss: 0.2191
2023-11-11 04:13:29,235 - mmseg - INFO - Iter [89350/160000]	lr: 4.087e-05, eta: 13:45:20, time: 0.679, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2140, decode.acc_seg: 91.0684, loss: 0.2140
2023-11-11 04:14:03,169 - mmseg - INFO - Iter [89400/160000]	lr: 4.082e-05, eta: 13:44:44, time: 0.678, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2221, decode.acc_seg: 91.1792, loss: 0.2221
2023-11-11 04:14:35,949 - mmseg - INFO - Iter [89450/160000]	lr: 4.078e-05, eta: 13:44:07, time: 0.656, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2322, decode.acc_seg: 90.6484, loss: 0.2322
2023-11-11 04:15:10,217 - mmseg - INFO - Iter [89500/160000]	lr: 4.073e-05, eta: 13:43:32, time: 0.685, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2154, decode.acc_seg: 91.1073, loss: 0.2154
2023-11-11 04:15:45,852 - mmseg - INFO - Iter [89550/160000]	lr: 4.068e-05, eta: 13:42:57, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2258, decode.acc_seg: 90.7179, loss: 0.2258
2023-11-11 04:16:21,383 - mmseg - INFO - Iter [89600/160000]	lr: 4.063e-05, eta: 13:42:22, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2246, decode.acc_seg: 90.8417, loss: 0.2246
2023-11-11 04:16:54,081 - mmseg - INFO - Iter [89650/160000]	lr: 4.058e-05, eta: 13:41:46, time: 0.654, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2103, decode.acc_seg: 91.2852, loss: 0.2103
2023-11-11 04:17:26,396 - mmseg - INFO - Iter [89700/160000]	lr: 4.054e-05, eta: 13:41:08, time: 0.645, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2027, decode.acc_seg: 91.8456, loss: 0.2027
2023-11-11 04:17:59,685 - mmseg - INFO - Iter [89750/160000]	lr: 4.049e-05, eta: 13:40:32, time: 0.667, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2358, decode.acc_seg: 90.3389, loss: 0.2358
2023-11-11 04:18:33,647 - mmseg - INFO - Iter [89800/160000]	lr: 4.044e-05, eta: 13:39:56, time: 0.678, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2232, decode.acc_seg: 91.0151, loss: 0.2232
2023-11-11 04:19:09,588 - mmseg - INFO - Iter [89850/160000]	lr: 4.039e-05, eta: 13:39:22, time: 0.719, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2201, decode.acc_seg: 91.0714, loss: 0.2201
2023-11-11 04:19:45,654 - mmseg - INFO - Iter [89900/160000]	lr: 4.034e-05, eta: 13:38:47, time: 0.721, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2252, decode.acc_seg: 91.0495, loss: 0.2252
2023-11-11 04:20:19,787 - mmseg - INFO - Iter [89950/160000]	lr: 4.029e-05, eta: 13:38:12, time: 0.683, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2159, decode.acc_seg: 91.0496, loss: 0.2159
2023-11-11 04:20:55,021 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 04:20:55,021 - mmseg - INFO - Iter [90000/160000]	lr: 4.025e-05, eta: 13:37:37, time: 0.705, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2090, decode.acc_seg: 91.5643, loss: 0.2090
2023-11-11 04:21:29,941 - mmseg - INFO - Iter [90050/160000]	lr: 4.020e-05, eta: 13:37:02, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2244, decode.acc_seg: 90.8631, loss: 0.2244
2023-11-11 04:22:04,976 - mmseg - INFO - Iter [90100/160000]	lr: 4.015e-05, eta: 13:36:27, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2190, decode.acc_seg: 90.8595, loss: 0.2190
2023-11-11 04:22:39,592 - mmseg - INFO - Iter [90150/160000]	lr: 4.010e-05, eta: 13:35:51, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2206, decode.acc_seg: 90.8664, loss: 0.2206
2023-11-11 04:23:15,429 - mmseg - INFO - Iter [90200/160000]	lr: 4.005e-05, eta: 13:35:17, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2173, decode.acc_seg: 91.1316, loss: 0.2173
2023-11-11 04:23:49,702 - mmseg - INFO - Iter [90250/160000]	lr: 4.001e-05, eta: 13:34:41, time: 0.685, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2170, decode.acc_seg: 91.0909, loss: 0.2170
2023-11-11 04:24:23,520 - mmseg - INFO - Iter [90300/160000]	lr: 3.996e-05, eta: 13:34:05, time: 0.676, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2201, decode.acc_seg: 90.9212, loss: 0.2201
2023-11-11 04:24:58,850 - mmseg - INFO - Iter [90350/160000]	lr: 3.991e-05, eta: 13:33:30, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2095, decode.acc_seg: 91.2336, loss: 0.2095
2023-11-11 04:25:33,732 - mmseg - INFO - Iter [90400/160000]	lr: 3.986e-05, eta: 13:32:55, time: 0.698, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2099, decode.acc_seg: 91.1946, loss: 0.2099
2023-11-11 04:26:08,346 - mmseg - INFO - Iter [90450/160000]	lr: 3.981e-05, eta: 13:32:20, time: 0.692, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2260, decode.acc_seg: 90.8823, loss: 0.2260
2023-11-11 04:26:42,581 - mmseg - INFO - Iter [90500/160000]	lr: 3.977e-05, eta: 13:31:44, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2247, decode.acc_seg: 90.9380, loss: 0.2247
2023-11-11 04:27:16,140 - mmseg - INFO - Iter [90550/160000]	lr: 3.972e-05, eta: 13:31:08, time: 0.671, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2006, decode.acc_seg: 91.5061, loss: 0.2006
2023-11-11 04:27:51,516 - mmseg - INFO - Iter [90600/160000]	lr: 3.967e-05, eta: 13:30:33, time: 0.707, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2092, decode.acc_seg: 91.6731, loss: 0.2092
2023-11-11 04:28:27,294 - mmseg - INFO - Iter [90650/160000]	lr: 3.962e-05, eta: 13:29:59, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2357, decode.acc_seg: 90.5420, loss: 0.2357
2023-11-11 04:29:02,862 - mmseg - INFO - Iter [90700/160000]	lr: 3.957e-05, eta: 13:29:24, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2369, decode.acc_seg: 90.5831, loss: 0.2369
2023-11-11 04:29:38,730 - mmseg - INFO - Iter [90750/160000]	lr: 3.953e-05, eta: 13:28:50, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2289, decode.acc_seg: 90.7346, loss: 0.2289
2023-11-11 04:30:13,235 - mmseg - INFO - Iter [90800/160000]	lr: 3.948e-05, eta: 13:28:14, time: 0.691, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2154, decode.acc_seg: 91.1509, loss: 0.2154
2023-11-11 04:30:47,021 - mmseg - INFO - Iter [90850/160000]	lr: 3.943e-05, eta: 13:27:38, time: 0.676, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2240, decode.acc_seg: 90.8084, loss: 0.2240
2023-11-11 04:31:20,646 - mmseg - INFO - Iter [90900/160000]	lr: 3.938e-05, eta: 13:27:02, time: 0.672, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2323, decode.acc_seg: 90.5571, loss: 0.2323
2023-11-11 04:31:55,444 - mmseg - INFO - Iter [90950/160000]	lr: 3.933e-05, eta: 13:26:27, time: 0.696, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2282, decode.acc_seg: 90.4866, loss: 0.2282
2023-11-11 04:32:29,302 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 04:32:29,303 - mmseg - INFO - Iter [91000/160000]	lr: 3.929e-05, eta: 13:25:51, time: 0.677, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2210, decode.acc_seg: 90.8358, loss: 0.2210
2023-11-11 04:33:03,048 - mmseg - INFO - Iter [91050/160000]	lr: 3.924e-05, eta: 13:25:15, time: 0.675, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2316, decode.acc_seg: 90.6992, loss: 0.2316
2023-11-11 04:33:38,513 - mmseg - INFO - Iter [91100/160000]	lr: 3.919e-05, eta: 13:24:40, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2349, decode.acc_seg: 90.6935, loss: 0.2349
2023-11-11 04:34:12,613 - mmseg - INFO - Iter [91150/160000]	lr: 3.914e-05, eta: 13:24:05, time: 0.682, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2148, decode.acc_seg: 91.2741, loss: 0.2148
2023-11-11 04:34:47,374 - mmseg - INFO - Iter [91200/160000]	lr: 3.909e-05, eta: 13:23:29, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2255, decode.acc_seg: 90.8898, loss: 0.2255
2023-11-11 04:35:23,192 - mmseg - INFO - Iter [91250/160000]	lr: 3.905e-05, eta: 13:22:55, time: 0.716, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2231, decode.acc_seg: 90.9044, loss: 0.2231
2023-11-11 04:35:59,302 - mmseg - INFO - Iter [91300/160000]	lr: 3.900e-05, eta: 13:22:21, time: 0.722, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2255, decode.acc_seg: 90.7754, loss: 0.2255
2023-11-11 04:36:33,148 - mmseg - INFO - Iter [91350/160000]	lr: 3.895e-05, eta: 13:21:45, time: 0.676, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2144, decode.acc_seg: 91.1234, loss: 0.2144
2023-11-11 04:37:08,051 - mmseg - INFO - Iter [91400/160000]	lr: 3.890e-05, eta: 13:21:10, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2196, decode.acc_seg: 90.8280, loss: 0.2196
2023-11-11 04:37:43,078 - mmseg - INFO - Iter [91450/160000]	lr: 3.885e-05, eta: 13:20:35, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2177, decode.acc_seg: 90.9711, loss: 0.2177
2023-11-11 04:38:19,269 - mmseg - INFO - Iter [91500/160000]	lr: 3.881e-05, eta: 13:20:00, time: 0.724, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2025, decode.acc_seg: 91.7091, loss: 0.2025
2023-11-11 04:38:53,985 - mmseg - INFO - Iter [91550/160000]	lr: 3.876e-05, eta: 13:19:25, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2109, decode.acc_seg: 91.3118, loss: 0.2109
2023-11-11 04:39:28,710 - mmseg - INFO - Iter [91600/160000]	lr: 3.871e-05, eta: 13:18:50, time: 0.695, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2219, decode.acc_seg: 90.8523, loss: 0.2219
2023-11-11 04:40:03,369 - mmseg - INFO - Iter [91650/160000]	lr: 3.866e-05, eta: 13:18:15, time: 0.694, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2098, decode.acc_seg: 91.1729, loss: 0.2098
2023-11-11 04:40:38,182 - mmseg - INFO - Iter [91700/160000]	lr: 3.862e-05, eta: 13:17:39, time: 0.696, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2237, decode.acc_seg: 90.9831, loss: 0.2237
2023-11-11 04:41:11,428 - mmseg - INFO - Iter [91750/160000]	lr: 3.857e-05, eta: 13:17:03, time: 0.665, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2164, decode.acc_seg: 91.0367, loss: 0.2164
2023-11-11 04:41:46,245 - mmseg - INFO - Iter [91800/160000]	lr: 3.852e-05, eta: 13:16:28, time: 0.696, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2320, decode.acc_seg: 90.1200, loss: 0.2320
2023-11-11 04:42:21,405 - mmseg - INFO - Iter [91850/160000]	lr: 3.847e-05, eta: 13:15:53, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2200, decode.acc_seg: 90.8480, loss: 0.2200
2023-11-11 04:42:57,215 - mmseg - INFO - Iter [91900/160000]	lr: 3.842e-05, eta: 13:15:18, time: 0.716, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2058, decode.acc_seg: 91.4294, loss: 0.2058
2023-11-11 04:43:32,350 - mmseg - INFO - Iter [91950/160000]	lr: 3.838e-05, eta: 13:14:43, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2241, decode.acc_seg: 90.7800, loss: 0.2241
2023-11-11 04:44:08,075 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 04:44:08,075 - mmseg - INFO - Iter [92000/160000]	lr: 3.833e-05, eta: 13:14:09, time: 0.715, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2303, decode.acc_seg: 90.6196, loss: 0.2303
2023-11-11 04:44:42,005 - mmseg - INFO - Iter [92050/160000]	lr: 3.828e-05, eta: 13:13:33, time: 0.680, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2150, decode.acc_seg: 91.3599, loss: 0.2150
2023-11-11 04:45:13,799 - mmseg - INFO - Iter [92100/160000]	lr: 3.823e-05, eta: 13:12:56, time: 0.636, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2141, decode.acc_seg: 91.1949, loss: 0.2141
2023-11-11 04:45:46,164 - mmseg - INFO - Iter [92150/160000]	lr: 3.819e-05, eta: 13:12:19, time: 0.647, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2137, decode.acc_seg: 91.3607, loss: 0.2137
2023-11-11 04:46:20,551 - mmseg - INFO - Iter [92200/160000]	lr: 3.814e-05, eta: 13:11:43, time: 0.687, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2211, decode.acc_seg: 91.1485, loss: 0.2211
2023-11-11 04:46:55,448 - mmseg - INFO - Iter [92250/160000]	lr: 3.809e-05, eta: 13:11:08, time: 0.698, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2261, decode.acc_seg: 90.7716, loss: 0.2261
2023-11-11 04:47:31,160 - mmseg - INFO - Iter [92300/160000]	lr: 3.804e-05, eta: 13:10:33, time: 0.714, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2193, decode.acc_seg: 91.0773, loss: 0.2193
2023-11-11 04:48:04,440 - mmseg - INFO - Iter [92350/160000]	lr: 3.799e-05, eta: 13:09:57, time: 0.667, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2089, decode.acc_seg: 91.4711, loss: 0.2089
2023-11-11 04:48:39,308 - mmseg - INFO - Iter [92400/160000]	lr: 3.795e-05, eta: 13:09:22, time: 0.696, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2328, decode.acc_seg: 90.2149, loss: 0.2328
2023-11-11 04:49:14,985 - mmseg - INFO - Iter [92450/160000]	lr: 3.790e-05, eta: 13:08:47, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2121, decode.acc_seg: 91.2466, loss: 0.2121
2023-11-11 04:49:50,372 - mmseg - INFO - Iter [92500/160000]	lr: 3.785e-05, eta: 13:08:13, time: 0.708, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2119, decode.acc_seg: 91.1318, loss: 0.2119
2023-11-11 04:50:26,063 - mmseg - INFO - Iter [92550/160000]	lr: 3.780e-05, eta: 13:07:38, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2076, decode.acc_seg: 91.5109, loss: 0.2076
2023-11-11 04:51:01,634 - mmseg - INFO - Iter [92600/160000]	lr: 3.776e-05, eta: 13:07:03, time: 0.711, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2188, decode.acc_seg: 90.8551, loss: 0.2188
2023-11-11 04:51:36,902 - mmseg - INFO - Iter [92650/160000]	lr: 3.771e-05, eta: 13:06:29, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2126, decode.acc_seg: 91.2474, loss: 0.2126
2023-11-11 04:52:11,877 - mmseg - INFO - Iter [92700/160000]	lr: 3.766e-05, eta: 13:05:53, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2100, decode.acc_seg: 91.3371, loss: 0.2100
2023-11-11 04:52:47,679 - mmseg - INFO - Iter [92750/160000]	lr: 3.761e-05, eta: 13:05:19, time: 0.716, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2270, decode.acc_seg: 90.6057, loss: 0.2270
2023-11-11 04:53:23,765 - mmseg - INFO - Iter [92800/160000]	lr: 3.757e-05, eta: 13:04:45, time: 0.722, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2055, decode.acc_seg: 91.5664, loss: 0.2055
2023-11-11 04:53:59,434 - mmseg - INFO - Iter [92850/160000]	lr: 3.752e-05, eta: 13:04:10, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2152, decode.acc_seg: 91.0967, loss: 0.2152
2023-11-11 04:54:35,132 - mmseg - INFO - Iter [92900/160000]	lr: 3.747e-05, eta: 13:03:36, time: 0.714, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2255, decode.acc_seg: 90.9244, loss: 0.2255
2023-11-11 04:55:10,727 - mmseg - INFO - Iter [92950/160000]	lr: 3.742e-05, eta: 13:03:01, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2237, decode.acc_seg: 91.1378, loss: 0.2237
2023-11-11 04:55:46,325 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 04:55:46,325 - mmseg - INFO - Iter [93000/160000]	lr: 3.738e-05, eta: 13:02:26, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2202, decode.acc_seg: 90.9629, loss: 0.2202
2023-11-11 04:56:20,737 - mmseg - INFO - Iter [93050/160000]	lr: 3.733e-05, eta: 13:01:51, time: 0.688, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2046, decode.acc_seg: 91.8233, loss: 0.2046
2023-11-11 04:56:56,359 - mmseg - INFO - Iter [93100/160000]	lr: 3.728e-05, eta: 13:01:16, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2115, decode.acc_seg: 91.3755, loss: 0.2115
2023-11-11 04:57:32,470 - mmseg - INFO - Iter [93150/160000]	lr: 3.723e-05, eta: 13:00:42, time: 0.722, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2126, decode.acc_seg: 91.3832, loss: 0.2126
2023-11-11 04:58:05,929 - mmseg - INFO - Iter [93200/160000]	lr: 3.719e-05, eta: 13:00:06, time: 0.669, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2123, decode.acc_seg: 91.3809, loss: 0.2123
2023-11-11 04:58:39,735 - mmseg - INFO - Iter [93250/160000]	lr: 3.714e-05, eta: 12:59:30, time: 0.676, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2150, decode.acc_seg: 91.2254, loss: 0.2150
2023-11-11 04:59:15,167 - mmseg - INFO - Iter [93300/160000]	lr: 3.709e-05, eta: 12:58:55, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2238, decode.acc_seg: 90.7404, loss: 0.2238
2023-11-11 04:59:50,641 - mmseg - INFO - Iter [93350/160000]	lr: 3.704e-05, eta: 12:58:20, time: 0.709, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2149, decode.acc_seg: 91.0444, loss: 0.2149
2023-11-11 05:00:25,257 - mmseg - INFO - Iter [93400/160000]	lr: 3.700e-05, eta: 12:57:45, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2197, decode.acc_seg: 90.9069, loss: 0.2197
2023-11-11 05:01:00,979 - mmseg - INFO - Iter [93450/160000]	lr: 3.695e-05, eta: 12:57:11, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2231, decode.acc_seg: 91.0797, loss: 0.2231
2023-11-11 05:01:36,660 - mmseg - INFO - Iter [93500/160000]	lr: 3.690e-05, eta: 12:56:36, time: 0.714, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2136, decode.acc_seg: 91.4077, loss: 0.2136
2023-11-11 05:02:11,948 - mmseg - INFO - Iter [93550/160000]	lr: 3.685e-05, eta: 12:56:01, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2213, decode.acc_seg: 91.0046, loss: 0.2213
2023-11-11 05:02:45,502 - mmseg - INFO - Iter [93600/160000]	lr: 3.681e-05, eta: 12:55:25, time: 0.671, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2037, decode.acc_seg: 91.3456, loss: 0.2037
2023-11-11 05:03:19,887 - mmseg - INFO - Iter [93650/160000]	lr: 3.676e-05, eta: 12:54:50, time: 0.689, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2122, decode.acc_seg: 91.2555, loss: 0.2122
2023-11-11 05:03:54,154 - mmseg - INFO - Iter [93700/160000]	lr: 3.671e-05, eta: 12:54:14, time: 0.684, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2206, decode.acc_seg: 91.0846, loss: 0.2206
2023-11-11 05:04:30,080 - mmseg - INFO - Iter [93750/160000]	lr: 3.667e-05, eta: 12:53:40, time: 0.719, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2004, decode.acc_seg: 91.7367, loss: 0.2004
2023-11-11 05:05:04,431 - mmseg - INFO - Iter [93800/160000]	lr: 3.662e-05, eta: 12:53:04, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2163, decode.acc_seg: 91.1592, loss: 0.2163
2023-11-11 05:05:39,034 - mmseg - INFO - Iter [93850/160000]	lr: 3.657e-05, eta: 12:52:29, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2268, decode.acc_seg: 90.7817, loss: 0.2268
2023-11-11 05:06:14,412 - mmseg - INFO - Iter [93900/160000]	lr: 3.652e-05, eta: 12:51:54, time: 0.707, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2208, decode.acc_seg: 90.8365, loss: 0.2208
2023-11-11 05:06:48,603 - mmseg - INFO - Iter [93950/160000]	lr: 3.648e-05, eta: 12:51:18, time: 0.684, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2122, decode.acc_seg: 91.1736, loss: 0.2122
2023-11-11 05:07:23,269 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 05:07:23,270 - mmseg - INFO - Iter [94000/160000]	lr: 3.643e-05, eta: 12:50:43, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2210, decode.acc_seg: 91.0420, loss: 0.2210
2023-11-11 05:07:58,522 - mmseg - INFO - Iter [94050/160000]	lr: 3.638e-05, eta: 12:50:08, time: 0.704, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2244, decode.acc_seg: 90.8492, loss: 0.2244
2023-11-11 05:08:33,508 - mmseg - INFO - Iter [94100/160000]	lr: 3.633e-05, eta: 12:49:33, time: 0.700, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2035, decode.acc_seg: 91.6852, loss: 0.2035
2023-11-11 05:09:07,715 - mmseg - INFO - Iter [94150/160000]	lr: 3.629e-05, eta: 12:48:57, time: 0.684, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2146, decode.acc_seg: 91.1893, loss: 0.2146
2023-11-11 05:09:42,630 - mmseg - INFO - Iter [94200/160000]	lr: 3.624e-05, eta: 12:48:22, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2214, decode.acc_seg: 91.1523, loss: 0.2214
2023-11-11 05:10:15,829 - mmseg - INFO - Iter [94250/160000]	lr: 3.619e-05, eta: 12:47:46, time: 0.665, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2205, decode.acc_seg: 91.0873, loss: 0.2205
2023-11-11 05:10:49,242 - mmseg - INFO - Iter [94300/160000]	lr: 3.615e-05, eta: 12:47:10, time: 0.668, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2169, decode.acc_seg: 91.1371, loss: 0.2169
2023-11-11 05:11:23,474 - mmseg - INFO - Iter [94350/160000]	lr: 3.610e-05, eta: 12:46:34, time: 0.684, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2250, decode.acc_seg: 90.7994, loss: 0.2250
2023-11-11 05:11:58,307 - mmseg - INFO - Iter [94400/160000]	lr: 3.605e-05, eta: 12:45:59, time: 0.697, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2170, decode.acc_seg: 90.9558, loss: 0.2170
2023-11-11 05:12:30,393 - mmseg - INFO - Iter [94450/160000]	lr: 3.600e-05, eta: 12:45:22, time: 0.642, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2034, decode.acc_seg: 91.4932, loss: 0.2034
2023-11-11 05:13:04,300 - mmseg - INFO - Iter [94500/160000]	lr: 3.596e-05, eta: 12:44:46, time: 0.677, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2169, decode.acc_seg: 90.9824, loss: 0.2169
2023-11-11 05:13:40,465 - mmseg - INFO - Iter [94550/160000]	lr: 3.591e-05, eta: 12:44:12, time: 0.724, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2174, decode.acc_seg: 91.1382, loss: 0.2174
2023-11-11 05:14:15,090 - mmseg - INFO - Iter [94600/160000]	lr: 3.586e-05, eta: 12:43:37, time: 0.693, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1991, decode.acc_seg: 91.9197, loss: 0.1991
2023-11-11 05:14:50,482 - mmseg - INFO - Iter [94650/160000]	lr: 3.582e-05, eta: 12:43:02, time: 0.708, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2069, decode.acc_seg: 91.3435, loss: 0.2069
2023-11-11 05:15:23,923 - mmseg - INFO - Iter [94700/160000]	lr: 3.577e-05, eta: 12:42:26, time: 0.670, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2083, decode.acc_seg: 91.3953, loss: 0.2083
2023-11-11 05:15:57,719 - mmseg - INFO - Iter [94750/160000]	lr: 3.572e-05, eta: 12:41:50, time: 0.676, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2153, decode.acc_seg: 90.9242, loss: 0.2153
2023-11-11 05:16:31,971 - mmseg - INFO - Iter [94800/160000]	lr: 3.567e-05, eta: 12:41:14, time: 0.684, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2065, decode.acc_seg: 91.4549, loss: 0.2065
2023-11-11 05:17:07,874 - mmseg - INFO - Iter [94850/160000]	lr: 3.563e-05, eta: 12:40:40, time: 0.719, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2234, decode.acc_seg: 90.6957, loss: 0.2234
2023-11-11 05:17:43,058 - mmseg - INFO - Iter [94900/160000]	lr: 3.558e-05, eta: 12:40:05, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2070, decode.acc_seg: 91.4825, loss: 0.2070
2023-11-11 05:18:15,123 - mmseg - INFO - Iter [94950/160000]	lr: 3.553e-05, eta: 12:39:28, time: 0.642, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2161, decode.acc_seg: 91.2288, loss: 0.2161
2023-11-11 05:18:48,652 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 05:18:48,653 - mmseg - INFO - Iter [95000/160000]	lr: 3.549e-05, eta: 12:38:52, time: 0.670, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1943, decode.acc_seg: 91.7769, loss: 0.1943
2023-11-11 05:19:23,752 - mmseg - INFO - Iter [95050/160000]	lr: 3.544e-05, eta: 12:38:17, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2116, decode.acc_seg: 91.2391, loss: 0.2116
2023-11-11 05:19:56,369 - mmseg - INFO - Iter [95100/160000]	lr: 3.539e-05, eta: 12:37:40, time: 0.653, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2109, decode.acc_seg: 91.2241, loss: 0.2109
2023-11-11 05:20:31,820 - mmseg - INFO - Iter [95150/160000]	lr: 3.535e-05, eta: 12:37:05, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2081, decode.acc_seg: 91.4003, loss: 0.2081
2023-11-11 05:21:07,609 - mmseg - INFO - Iter [95200/160000]	lr: 3.530e-05, eta: 12:36:31, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2097, decode.acc_seg: 91.2734, loss: 0.2097
2023-11-11 05:21:42,392 - mmseg - INFO - Iter [95250/160000]	lr: 3.525e-05, eta: 12:35:56, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2024, decode.acc_seg: 91.7167, loss: 0.2024
2023-11-11 05:22:17,877 - mmseg - INFO - Iter [95300/160000]	lr: 3.521e-05, eta: 12:35:21, time: 0.710, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2053, decode.acc_seg: 91.6154, loss: 0.2053
2023-11-11 05:22:52,349 - mmseg - INFO - Iter [95350/160000]	lr: 3.516e-05, eta: 12:34:46, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2259, decode.acc_seg: 90.9071, loss: 0.2259
2023-11-11 05:23:25,791 - mmseg - INFO - Iter [95400/160000]	lr: 3.511e-05, eta: 12:34:10, time: 0.669, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2221, decode.acc_seg: 90.9585, loss: 0.2221
2023-11-11 05:23:59,747 - mmseg - INFO - Iter [95450/160000]	lr: 3.506e-05, eta: 12:33:34, time: 0.678, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2109, decode.acc_seg: 91.2344, loss: 0.2109
2023-11-11 05:24:34,256 - mmseg - INFO - Iter [95500/160000]	lr: 3.502e-05, eta: 12:32:58, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2010, decode.acc_seg: 91.4839, loss: 0.2010
2023-11-11 05:25:08,509 - mmseg - INFO - Iter [95550/160000]	lr: 3.497e-05, eta: 12:32:23, time: 0.685, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2182, decode.acc_seg: 90.9253, loss: 0.2182
2023-11-11 05:25:43,024 - mmseg - INFO - Iter [95600/160000]	lr: 3.492e-05, eta: 12:31:48, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2032, decode.acc_seg: 91.3901, loss: 0.2032
2023-11-11 05:26:17,896 - mmseg - INFO - Iter [95650/160000]	lr: 3.488e-05, eta: 12:31:12, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2163, decode.acc_seg: 91.3738, loss: 0.2163
2023-11-11 05:26:54,385 - mmseg - INFO - Iter [95700/160000]	lr: 3.483e-05, eta: 12:30:38, time: 0.730, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2228, decode.acc_seg: 90.7418, loss: 0.2228
2023-11-11 05:27:27,675 - mmseg - INFO - Iter [95750/160000]	lr: 3.478e-05, eta: 12:30:02, time: 0.665, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2154, decode.acc_seg: 90.9795, loss: 0.2154
2023-11-11 05:28:03,307 - mmseg - INFO - Iter [95800/160000]	lr: 3.474e-05, eta: 12:29:28, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2237, decode.acc_seg: 91.1121, loss: 0.2237
2023-11-11 05:28:38,259 - mmseg - INFO - Iter [95850/160000]	lr: 3.469e-05, eta: 12:28:53, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2134, decode.acc_seg: 91.0879, loss: 0.2134
2023-11-11 05:29:12,224 - mmseg - INFO - Iter [95900/160000]	lr: 3.464e-05, eta: 12:28:17, time: 0.679, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2363, decode.acc_seg: 90.2373, loss: 0.2363
2023-11-11 05:29:45,351 - mmseg - INFO - Iter [95950/160000]	lr: 3.460e-05, eta: 12:27:41, time: 0.662, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2146, decode.acc_seg: 91.2206, loss: 0.2146
2023-11-11 05:30:20,941 - mmseg - INFO - Saving checkpoint at 96000 iterations
2023-11-11 05:30:25,583 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 05:30:25,583 - mmseg - INFO - Iter [96000/160000]	lr: 3.455e-05, eta: 12:27:09, time: 0.806, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2088, decode.acc_seg: 91.2617, loss: 0.2088
2023-11-11 05:32:26,555 - mmseg - INFO - per class results:
2023-11-11 05:32:26,568 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        |  77.4 | 88.34 |
|       building      | 81.62 | 90.27 |
|         sky         | 94.35 | 97.23 |
|        floor        | 82.75 | 90.16 |
|         tree        | 74.68 | 89.27 |
|       ceiling       | 83.23 | 92.39 |
|         road        | 81.31 |  87.1 |
|         bed         | 88.77 | 95.86 |
|      windowpane     | 60.86 | 79.32 |
|        grass        | 65.11 | 84.96 |
|       cabinet       |  60.2 | 75.99 |
|       sidewalk      | 63.69 | 81.83 |
|        person       | 80.51 | 93.33 |
|        earth        | 36.05 | 50.98 |
|         door        | 49.67 | 59.82 |
|        table        | 59.06 | 76.43 |
|       mountain      | 51.89 | 66.21 |
|        plant        | 49.96 | 59.72 |
|       curtain       |  73.0 | 85.88 |
|        chair        | 54.95 | 67.21 |
|         car         | 83.99 | 91.85 |
|        water        | 60.51 | 73.09 |
|       painting      | 69.73 | 86.62 |
|         sofa        | 63.33 | 78.49 |
|        shelf        | 39.98 | 55.46 |
|        house        | 44.81 | 70.28 |
|         sea         | 57.55 | 79.24 |
|        mirror       |  67.3 | 73.78 |
|         rug         | 65.97 | 75.96 |
|        field        |  28.8 | 35.27 |
|       armchair      | 41.36 | 64.32 |
|         seat        | 59.13 | 79.68 |
|        fence        |  41.9 | 59.62 |
|         desk        | 49.88 | 67.53 |
|         rock        | 32.86 | 51.31 |
|       wardrobe      | 48.62 | 66.88 |
|         lamp        | 64.64 |  77.9 |
|       bathtub       | 82.69 | 88.78 |
|       railing       | 31.76 | 42.77 |
|       cushion       |  57.3 | 71.23 |
|         base        | 36.07 | 48.05 |
|         box         | 26.79 |  32.9 |
|        column       | 44.93 | 56.47 |
|      signboard      | 39.57 |  52.5 |
|   chest of drawers  | 43.58 | 54.47 |
|       counter       | 35.11 |  47.1 |
|         sand        | 37.16 | 59.24 |
|         sink        | 66.18 | 79.97 |
|      skyscraper     | 49.13 | 62.49 |
|      fireplace      | 72.99 | 86.91 |
|     refrigerator    | 76.61 | 83.92 |
|      grandstand     | 44.57 | 74.09 |
|         path        | 21.48 | 35.37 |
|        stairs       | 30.87 | 41.09 |
|        runway       | 64.85 | 83.47 |
|         case        | 40.06 | 46.24 |
|      pool table     | 92.75 | 97.13 |
|        pillow       | 59.79 | 68.91 |
|     screen door     | 63.76 | 82.48 |
|       stairway      | 31.54 | 39.26 |
|        river        | 16.64 | 33.16 |
|        bridge       | 67.37 | 81.07 |
|       bookcase      | 33.48 |  55.8 |
|        blind        | 40.26 | 42.94 |
|     coffee table    | 51.19 | 81.94 |
|        toilet       | 82.29 | 91.39 |
|        flower       | 41.51 | 57.77 |
|         book        | 44.73 |  66.4 |
|         hill        | 12.92 | 21.53 |
|        bench        | 42.08 | 49.84 |
|      countertop     | 61.69 | 80.36 |
|        stove        | 70.01 | 80.76 |
|         palm        |  48.8 | 71.84 |
|    kitchen island   |  41.9 | 65.47 |
|       computer      | 63.63 |  77.6 |
|     swivel chair    | 44.16 | 78.09 |
|         boat        | 43.75 | 57.81 |
|         bar         | 27.79 | 34.89 |
|    arcade machine   | 78.09 | 85.26 |
|        hovel        | 53.43 | 63.84 |
|         bus         | 85.35 | 95.86 |
|        towel        | 65.48 | 72.54 |
|        light        | 54.45 | 63.84 |
|        truck        | 38.51 | 49.37 |
|        tower        |  6.89 |  8.15 |
|      chandelier     | 66.94 | 81.43 |
|        awning       | 25.32 | 29.77 |
|     streetlight     |  26.4 | 35.85 |
|        booth        | 58.17 |  61.8 |
| television receiver | 61.99 | 85.14 |
|       airplane      | 51.02 |  65.0 |
|      dirt track     |  2.59 | 10.44 |
|       apparel       | 39.19 | 56.42 |
|         pole        | 21.65 | 30.57 |
|         land        |  0.02 |  0.04 |
|      bannister      | 15.38 | 20.65 |
|      escalator      | 51.19 | 78.66 |
|       ottoman       | 46.91 | 56.18 |
|        bottle       | 36.25 | 63.63 |
|        buffet       | 40.18 | 44.67 |
|        poster       | 34.48 | 44.35 |
|        stage        | 19.02 | 30.01 |
|         van         | 45.88 | 65.18 |
|         ship        | 57.18 | 81.06 |
|       fountain      | 39.71 | 40.02 |
|    conveyer belt    | 82.44 | 94.58 |
|        canopy       | 13.39 | 20.07 |
|        washer       | 73.24 | 73.77 |
|      plaything      | 20.41 | 33.11 |
|    swimming pool    | 56.53 | 61.19 |
|        stool        | 37.78 | 49.42 |
|        barrel       | 59.57 | 73.79 |
|        basket       | 32.69 | 40.87 |
|      waterfall      | 67.05 | 91.86 |
|         tent        | 96.03 | 98.32 |
|         bag         | 13.74 | 17.19 |
|       minibike      | 69.12 | 80.15 |
|        cradle       | 76.81 | 98.15 |
|         oven        |  45.0 | 61.98 |
|         ball        | 37.72 | 54.46 |
|         food        | 40.85 | 48.56 |
|         step        | 13.18 | 15.91 |
|         tank        | 42.04 | 48.49 |
|      trade name     | 31.46 | 39.13 |
|      microwave      | 78.93 | 91.54 |
|         pot         | 40.63 | 44.67 |
|        animal       | 56.77 | 59.91 |
|       bicycle       | 56.74 | 77.42 |
|         lake        |  62.7 | 65.95 |
|      dishwasher     | 56.63 | 65.47 |
|        screen       |  43.0 | 61.38 |
|       blanket       | 10.25 | 11.95 |
|      sculpture      | 64.05 | 78.62 |
|         hood        | 57.04 |  69.2 |
|        sconce       | 45.63 | 52.35 |
|         vase        | 39.63 | 54.36 |
|    traffic light    | 25.85 | 38.83 |
|         tray        |  7.57 | 16.66 |
|        ashcan       | 38.26 | 48.52 |
|         fan         | 60.07 | 75.81 |
|         pier        | 20.37 | 24.08 |
|      crt screen     |  2.13 |  9.46 |
|        plate        | 51.23 | 68.62 |
|       monitor       |  7.63 |  9.07 |
|    bulletin board   | 49.75 |  65.7 |
|        shower       |  0.7  |  2.37 |
|       radiator      | 64.89 | 71.05 |
|        glass        | 14.37 | 15.85 |
|        clock        | 31.53 | 38.17 |
|         flag        | 46.85 | 51.94 |
+---------------------+-------+-------+
2023-11-11 05:32:26,568 - mmseg - INFO - Summary:
2023-11-11 05:32:26,569 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 82.62 | 48.91 | 61.02 |
+-------+-------+-------+
2023-11-11 05:32:26,707 - mmseg - INFO - The previous best checkpoint /data/Next-ViT/segmentation/work_dirs/fpn_512_debi_base_160k/best_mIoU_iter_88000.pth was removed
2023-11-11 05:32:29,581 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_96000.pth.
2023-11-11 05:32:29,581 - mmseg - INFO - Best mIoU is 0.4891 at 96000 iter.
2023-11-11 05:32:29,623 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 05:32:29,623 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8262, mIoU: 0.4891, mAcc: 0.6102, IoU.wall: 0.7740, IoU.building: 0.8162, IoU.sky: 0.9435, IoU.floor: 0.8275, IoU.tree: 0.7468, IoU.ceiling: 0.8323, IoU.road: 0.8131, IoU.bed : 0.8877, IoU.windowpane: 0.6086, IoU.grass: 0.6511, IoU.cabinet: 0.6020, IoU.sidewalk: 0.6369, IoU.person: 0.8051, IoU.earth: 0.3605, IoU.door: 0.4967, IoU.table: 0.5906, IoU.mountain: 0.5189, IoU.plant: 0.4996, IoU.curtain: 0.7300, IoU.chair: 0.5495, IoU.car: 0.8399, IoU.water: 0.6051, IoU.painting: 0.6973, IoU.sofa: 0.6333, IoU.shelf: 0.3998, IoU.house: 0.4481, IoU.sea: 0.5755, IoU.mirror: 0.6730, IoU.rug: 0.6597, IoU.field: 0.2880, IoU.armchair: 0.4136, IoU.seat: 0.5913, IoU.fence: 0.4190, IoU.desk: 0.4988, IoU.rock: 0.3286, IoU.wardrobe: 0.4862, IoU.lamp: 0.6464, IoU.bathtub: 0.8269, IoU.railing: 0.3176, IoU.cushion: 0.5730, IoU.base: 0.3607, IoU.box: 0.2679, IoU.column: 0.4493, IoU.signboard: 0.3957, IoU.chest of drawers: 0.4358, IoU.counter: 0.3511, IoU.sand: 0.3716, IoU.sink: 0.6618, IoU.skyscraper: 0.4913, IoU.fireplace: 0.7299, IoU.refrigerator: 0.7661, IoU.grandstand: 0.4457, IoU.path: 0.2148, IoU.stairs: 0.3087, IoU.runway: 0.6485, IoU.case: 0.4006, IoU.pool table: 0.9275, IoU.pillow: 0.5979, IoU.screen door: 0.6376, IoU.stairway: 0.3154, IoU.river: 0.1664, IoU.bridge: 0.6737, IoU.bookcase: 0.3348, IoU.blind: 0.4026, IoU.coffee table: 0.5119, IoU.toilet: 0.8229, IoU.flower: 0.4151, IoU.book: 0.4473, IoU.hill: 0.1292, IoU.bench: 0.4208, IoU.countertop: 0.6169, IoU.stove: 0.7001, IoU.palm: 0.4880, IoU.kitchen island: 0.4190, IoU.computer: 0.6363, IoU.swivel chair: 0.4416, IoU.boat: 0.4375, IoU.bar: 0.2779, IoU.arcade machine: 0.7809, IoU.hovel: 0.5343, IoU.bus: 0.8535, IoU.towel: 0.6548, IoU.light: 0.5445, IoU.truck: 0.3851, IoU.tower: 0.0689, IoU.chandelier: 0.6694, IoU.awning: 0.2532, IoU.streetlight: 0.2640, IoU.booth: 0.5817, IoU.television receiver: 0.6199, IoU.airplane: 0.5102, IoU.dirt track: 0.0259, IoU.apparel: 0.3919, IoU.pole: 0.2165, IoU.land: 0.0002, IoU.bannister: 0.1538, IoU.escalator: 0.5119, IoU.ottoman: 0.4691, IoU.bottle: 0.3625, IoU.buffet: 0.4018, IoU.poster: 0.3448, IoU.stage: 0.1902, IoU.van: 0.4588, IoU.ship: 0.5718, IoU.fountain: 0.3971, IoU.conveyer belt: 0.8244, IoU.canopy: 0.1339, IoU.washer: 0.7324, IoU.plaything: 0.2041, IoU.swimming pool: 0.5653, IoU.stool: 0.3778, IoU.barrel: 0.5957, IoU.basket: 0.3269, IoU.waterfall: 0.6705, IoU.tent: 0.9603, IoU.bag: 0.1374, IoU.minibike: 0.6912, IoU.cradle: 0.7681, IoU.oven: 0.4500, IoU.ball: 0.3772, IoU.food: 0.4085, IoU.step: 0.1318, IoU.tank: 0.4204, IoU.trade name: 0.3146, IoU.microwave: 0.7893, IoU.pot: 0.4063, IoU.animal: 0.5677, IoU.bicycle: 0.5674, IoU.lake: 0.6270, IoU.dishwasher: 0.5663, IoU.screen: 0.4300, IoU.blanket: 0.1025, IoU.sculpture: 0.6405, IoU.hood: 0.5704, IoU.sconce: 0.4563, IoU.vase: 0.3963, IoU.traffic light: 0.2585, IoU.tray: 0.0757, IoU.ashcan: 0.3826, IoU.fan: 0.6007, IoU.pier: 0.2037, IoU.crt screen: 0.0213, IoU.plate: 0.5123, IoU.monitor: 0.0763, IoU.bulletin board: 0.4975, IoU.shower: 0.0070, IoU.radiator: 0.6489, IoU.glass: 0.1437, IoU.clock: 0.3153, IoU.flag: 0.4685, Acc.wall: 0.8834, Acc.building: 0.9027, Acc.sky: 0.9723, Acc.floor: 0.9016, Acc.tree: 0.8927, Acc.ceiling: 0.9239, Acc.road: 0.8710, Acc.bed : 0.9586, Acc.windowpane: 0.7932, Acc.grass: 0.8496, Acc.cabinet: 0.7599, Acc.sidewalk: 0.8183, Acc.person: 0.9333, Acc.earth: 0.5098, Acc.door: 0.5982, Acc.table: 0.7643, Acc.mountain: 0.6621, Acc.plant: 0.5972, Acc.curtain: 0.8588, Acc.chair: 0.6721, Acc.car: 0.9185, Acc.water: 0.7309, Acc.painting: 0.8662, Acc.sofa: 0.7849, Acc.shelf: 0.5546, Acc.house: 0.7028, Acc.sea: 0.7924, Acc.mirror: 0.7378, Acc.rug: 0.7596, Acc.field: 0.3527, Acc.armchair: 0.6432, Acc.seat: 0.7968, Acc.fence: 0.5962, Acc.desk: 0.6753, Acc.rock: 0.5131, Acc.wardrobe: 0.6688, Acc.lamp: 0.7790, Acc.bathtub: 0.8878, Acc.railing: 0.4277, Acc.cushion: 0.7123, Acc.base: 0.4805, Acc.box: 0.3290, Acc.column: 0.5647, Acc.signboard: 0.5250, Acc.chest of drawers: 0.5447, Acc.counter: 0.4710, Acc.sand: 0.5924, Acc.sink: 0.7997, Acc.skyscraper: 0.6249, Acc.fireplace: 0.8691, Acc.refrigerator: 0.8392, Acc.grandstand: 0.7409, Acc.path: 0.3537, Acc.stairs: 0.4109, Acc.runway: 0.8347, Acc.case: 0.4624, Acc.pool table: 0.9713, Acc.pillow: 0.6891, Acc.screen door: 0.8248, Acc.stairway: 0.3926, Acc.river: 0.3316, Acc.bridge: 0.8107, Acc.bookcase: 0.5580, Acc.blind: 0.4294, Acc.coffee table: 0.8194, Acc.toilet: 0.9139, Acc.flower: 0.5777, Acc.book: 0.6640, Acc.hill: 0.2153, Acc.bench: 0.4984, Acc.countertop: 0.8036, Acc.stove: 0.8076, Acc.palm: 0.7184, Acc.kitchen island: 0.6547, Acc.computer: 0.7760, Acc.swivel chair: 0.7809, Acc.boat: 0.5781, Acc.bar: 0.3489, Acc.arcade machine: 0.8526, Acc.hovel: 0.6384, Acc.bus: 0.9586, Acc.towel: 0.7254, Acc.light: 0.6384, Acc.truck: 0.4937, Acc.tower: 0.0815, Acc.chandelier: 0.8143, Acc.awning: 0.2977, Acc.streetlight: 0.3585, Acc.booth: 0.6180, Acc.television receiver: 0.8514, Acc.airplane: 0.6500, Acc.dirt track: 0.1044, Acc.apparel: 0.5642, Acc.pole: 0.3057, Acc.land: 0.0004, Acc.bannister: 0.2065, Acc.escalator: 0.7866, Acc.ottoman: 0.5618, Acc.bottle: 0.6363, Acc.buffet: 0.4467, Acc.poster: 0.4435, Acc.stage: 0.3001, Acc.van: 0.6518, Acc.ship: 0.8106, Acc.fountain: 0.4002, Acc.conveyer belt: 0.9458, Acc.canopy: 0.2007, Acc.washer: 0.7377, Acc.plaything: 0.3311, Acc.swimming pool: 0.6119, Acc.stool: 0.4942, Acc.barrel: 0.7379, Acc.basket: 0.4087, Acc.waterfall: 0.9186, Acc.tent: 0.9832, Acc.bag: 0.1719, Acc.minibike: 0.8015, Acc.cradle: 0.9815, Acc.oven: 0.6198, Acc.ball: 0.5446, Acc.food: 0.4856, Acc.step: 0.1591, Acc.tank: 0.4849, Acc.trade name: 0.3913, Acc.microwave: 0.9154, Acc.pot: 0.4467, Acc.animal: 0.5991, Acc.bicycle: 0.7742, Acc.lake: 0.6595, Acc.dishwasher: 0.6547, Acc.screen: 0.6138, Acc.blanket: 0.1195, Acc.sculpture: 0.7862, Acc.hood: 0.6920, Acc.sconce: 0.5235, Acc.vase: 0.5436, Acc.traffic light: 0.3883, Acc.tray: 0.1666, Acc.ashcan: 0.4852, Acc.fan: 0.7581, Acc.pier: 0.2408, Acc.crt screen: 0.0946, Acc.plate: 0.6862, Acc.monitor: 0.0907, Acc.bulletin board: 0.6570, Acc.shower: 0.0237, Acc.radiator: 0.7105, Acc.glass: 0.1585, Acc.clock: 0.3817, Acc.flag: 0.5194
2023-11-11 05:33:01,950 - mmseg - INFO - Iter [96050/160000]	lr: 3.450e-05, eta: 12:27:55, time: 3.127, data_time: 2.490, memory: 23129, decode.loss_ce: 0.2121, decode.acc_seg: 91.0351, loss: 0.2121
2023-11-11 05:33:36,906 - mmseg - INFO - Iter [96100/160000]	lr: 3.446e-05, eta: 12:27:20, time: 0.698, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2114, decode.acc_seg: 91.4503, loss: 0.2114
2023-11-11 05:34:11,459 - mmseg - INFO - Iter [96150/160000]	lr: 3.441e-05, eta: 12:26:44, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2132, decode.acc_seg: 91.0544, loss: 0.2132
2023-11-11 05:34:46,952 - mmseg - INFO - Iter [96200/160000]	lr: 3.436e-05, eta: 12:26:09, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2254, decode.acc_seg: 91.0558, loss: 0.2254
2023-11-11 05:35:22,701 - mmseg - INFO - Iter [96250/160000]	lr: 3.432e-05, eta: 12:25:35, time: 0.715, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2150, decode.acc_seg: 91.2352, loss: 0.2150
2023-11-11 05:35:58,229 - mmseg - INFO - Iter [96300/160000]	lr: 3.427e-05, eta: 12:25:00, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2118, decode.acc_seg: 91.0294, loss: 0.2118
2023-11-11 05:36:33,803 - mmseg - INFO - Iter [96350/160000]	lr: 3.422e-05, eta: 12:24:25, time: 0.712, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2156, decode.acc_seg: 90.9562, loss: 0.2156
2023-11-11 05:37:09,623 - mmseg - INFO - Iter [96400/160000]	lr: 3.418e-05, eta: 12:23:51, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2179, decode.acc_seg: 91.2297, loss: 0.2179
2023-11-11 05:37:44,767 - mmseg - INFO - Iter [96450/160000]	lr: 3.413e-05, eta: 12:23:15, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2221, decode.acc_seg: 90.9737, loss: 0.2221
2023-11-11 05:38:20,267 - mmseg - INFO - Iter [96500/160000]	lr: 3.408e-05, eta: 12:22:41, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2303, decode.acc_seg: 90.6908, loss: 0.2303
2023-11-11 05:38:55,518 - mmseg - INFO - Iter [96550/160000]	lr: 3.404e-05, eta: 12:22:06, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2211, decode.acc_seg: 90.6249, loss: 0.2211
2023-11-11 05:39:29,166 - mmseg - INFO - Iter [96600/160000]	lr: 3.399e-05, eta: 12:21:30, time: 0.673, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2289, decode.acc_seg: 90.8495, loss: 0.2289
2023-11-11 05:40:01,401 - mmseg - INFO - Iter [96650/160000]	lr: 3.394e-05, eta: 12:20:53, time: 0.645, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2395, decode.acc_seg: 90.9426, loss: 0.2395
2023-11-11 05:40:35,068 - mmseg - INFO - Iter [96700/160000]	lr: 3.390e-05, eta: 12:20:17, time: 0.674, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2040, decode.acc_seg: 91.6805, loss: 0.2040
2023-11-11 05:41:10,310 - mmseg - INFO - Iter [96750/160000]	lr: 3.385e-05, eta: 12:19:42, time: 0.704, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1917, decode.acc_seg: 91.9478, loss: 0.1917
2023-11-11 05:41:46,038 - mmseg - INFO - Iter [96800/160000]	lr: 3.381e-05, eta: 12:19:07, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1965, decode.acc_seg: 91.8514, loss: 0.1965
2023-11-11 05:42:22,037 - mmseg - INFO - Iter [96850/160000]	lr: 3.376e-05, eta: 12:18:33, time: 0.720, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2074, decode.acc_seg: 91.3991, loss: 0.2074
2023-11-11 05:42:57,987 - mmseg - INFO - Iter [96900/160000]	lr: 3.371e-05, eta: 12:17:58, time: 0.719, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2040, decode.acc_seg: 91.5628, loss: 0.2040
2023-11-11 05:43:33,676 - mmseg - INFO - Iter [96950/160000]	lr: 3.367e-05, eta: 12:17:23, time: 0.714, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2085, decode.acc_seg: 91.4884, loss: 0.2085
2023-11-11 05:44:09,087 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 05:44:09,088 - mmseg - INFO - Iter [97000/160000]	lr: 3.362e-05, eta: 12:16:48, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2136, decode.acc_seg: 91.2758, loss: 0.2136
2023-11-11 05:44:44,477 - mmseg - INFO - Iter [97050/160000]	lr: 3.357e-05, eta: 12:16:14, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2088, decode.acc_seg: 91.3872, loss: 0.2088
2023-11-11 05:45:17,759 - mmseg - INFO - Iter [97100/160000]	lr: 3.353e-05, eta: 12:15:37, time: 0.667, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2176, decode.acc_seg: 90.8990, loss: 0.2176
2023-11-11 05:45:49,279 - mmseg - INFO - Iter [97150/160000]	lr: 3.348e-05, eta: 12:15:00, time: 0.630, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2152, decode.acc_seg: 91.1520, loss: 0.2152
2023-11-11 05:46:20,742 - mmseg - INFO - Iter [97200/160000]	lr: 3.343e-05, eta: 12:14:23, time: 0.629, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2155, decode.acc_seg: 91.2629, loss: 0.2155
2023-11-11 05:46:54,607 - mmseg - INFO - Iter [97250/160000]	lr: 3.339e-05, eta: 12:13:47, time: 0.676, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1981, decode.acc_seg: 91.7004, loss: 0.1981
2023-11-11 05:47:30,185 - mmseg - INFO - Iter [97300/160000]	lr: 3.334e-05, eta: 12:13:12, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2178, decode.acc_seg: 90.8370, loss: 0.2178
2023-11-11 05:48:04,970 - mmseg - INFO - Iter [97350/160000]	lr: 3.330e-05, eta: 12:12:37, time: 0.695, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2078, decode.acc_seg: 91.2661, loss: 0.2078
2023-11-11 05:48:39,654 - mmseg - INFO - Iter [97400/160000]	lr: 3.325e-05, eta: 12:12:01, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2069, decode.acc_seg: 91.5759, loss: 0.2069
2023-11-11 05:49:14,268 - mmseg - INFO - Iter [97450/160000]	lr: 3.320e-05, eta: 12:11:26, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2167, decode.acc_seg: 91.0053, loss: 0.2167
2023-11-11 05:49:48,989 - mmseg - INFO - Iter [97500/160000]	lr: 3.316e-05, eta: 12:10:51, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2087, decode.acc_seg: 91.3706, loss: 0.2087
2023-11-11 05:50:23,807 - mmseg - INFO - Iter [97550/160000]	lr: 3.311e-05, eta: 12:10:15, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2098, decode.acc_seg: 91.4011, loss: 0.2098
2023-11-11 05:50:58,683 - mmseg - INFO - Iter [97600/160000]	lr: 3.306e-05, eta: 12:09:40, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2102, decode.acc_seg: 91.0627, loss: 0.2102
2023-11-11 05:51:31,849 - mmseg - INFO - Iter [97650/160000]	lr: 3.302e-05, eta: 12:09:04, time: 0.663, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2162, decode.acc_seg: 91.0468, loss: 0.2162
2023-11-11 05:52:07,841 - mmseg - INFO - Iter [97700/160000]	lr: 3.297e-05, eta: 12:08:29, time: 0.719, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2111, decode.acc_seg: 91.2946, loss: 0.2111
2023-11-11 05:52:42,576 - mmseg - INFO - Iter [97750/160000]	lr: 3.293e-05, eta: 12:07:54, time: 0.695, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2045, decode.acc_seg: 91.5380, loss: 0.2045
2023-11-11 05:53:18,147 - mmseg - INFO - Iter [97800/160000]	lr: 3.288e-05, eta: 12:07:19, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2094, decode.acc_seg: 91.4085, loss: 0.2094
2023-11-11 05:53:53,180 - mmseg - INFO - Iter [97850/160000]	lr: 3.283e-05, eta: 12:06:44, time: 0.701, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2131, decode.acc_seg: 91.3920, loss: 0.2131
2023-11-11 05:54:28,195 - mmseg - INFO - Iter [97900/160000]	lr: 3.279e-05, eta: 12:06:09, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2125, decode.acc_seg: 91.3206, loss: 0.2125
2023-11-11 05:55:02,651 - mmseg - INFO - Iter [97950/160000]	lr: 3.274e-05, eta: 12:05:34, time: 0.690, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2113, decode.acc_seg: 91.4355, loss: 0.2113
2023-11-11 05:55:34,983 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 05:55:34,983 - mmseg - INFO - Iter [98000/160000]	lr: 3.270e-05, eta: 12:04:57, time: 0.647, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2019, decode.acc_seg: 91.6929, loss: 0.2019
2023-11-11 05:56:08,770 - mmseg - INFO - Iter [98050/160000]	lr: 3.265e-05, eta: 12:04:21, time: 0.675, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2194, decode.acc_seg: 90.7064, loss: 0.2194
2023-11-11 05:56:42,428 - mmseg - INFO - Iter [98100/160000]	lr: 3.260e-05, eta: 12:03:45, time: 0.673, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2106, decode.acc_seg: 91.2882, loss: 0.2106
2023-11-11 05:57:14,577 - mmseg - INFO - Iter [98150/160000]	lr: 3.256e-05, eta: 12:03:08, time: 0.644, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2110, decode.acc_seg: 91.1971, loss: 0.2110
2023-11-11 05:57:49,182 - mmseg - INFO - Iter [98200/160000]	lr: 3.251e-05, eta: 12:02:33, time: 0.691, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1946, decode.acc_seg: 91.9979, loss: 0.1946
2023-11-11 05:58:23,425 - mmseg - INFO - Iter [98250/160000]	lr: 3.247e-05, eta: 12:01:57, time: 0.684, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1940, decode.acc_seg: 91.7418, loss: 0.1940
2023-11-11 05:58:58,779 - mmseg - INFO - Iter [98300/160000]	lr: 3.242e-05, eta: 12:01:22, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2284, decode.acc_seg: 90.6659, loss: 0.2284
2023-11-11 05:59:34,046 - mmseg - INFO - Iter [98350/160000]	lr: 3.237e-05, eta: 12:00:47, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2000, decode.acc_seg: 91.7483, loss: 0.2000
2023-11-11 06:00:09,248 - mmseg - INFO - Iter [98400/160000]	lr: 3.233e-05, eta: 12:00:12, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2144, decode.acc_seg: 91.2757, loss: 0.2144
2023-11-11 06:00:44,434 - mmseg - INFO - Iter [98450/160000]	lr: 3.228e-05, eta: 11:59:37, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2081, decode.acc_seg: 91.4310, loss: 0.2081
2023-11-11 06:01:19,629 - mmseg - INFO - Iter [98500/160000]	lr: 3.224e-05, eta: 11:59:02, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2191, decode.acc_seg: 91.3522, loss: 0.2191
2023-11-11 06:01:54,704 - mmseg - INFO - Iter [98550/160000]	lr: 3.219e-05, eta: 11:58:27, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2043, decode.acc_seg: 91.5200, loss: 0.2043
2023-11-11 06:02:27,682 - mmseg - INFO - Iter [98600/160000]	lr: 3.214e-05, eta: 11:57:51, time: 0.661, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1991, decode.acc_seg: 91.5458, loss: 0.1991
2023-11-11 06:03:01,804 - mmseg - INFO - Iter [98650/160000]	lr: 3.210e-05, eta: 11:57:15, time: 0.682, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2045, decode.acc_seg: 91.4616, loss: 0.2045
2023-11-11 06:03:34,849 - mmseg - INFO - Iter [98700/160000]	lr: 3.205e-05, eta: 11:56:39, time: 0.662, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2009, decode.acc_seg: 91.6487, loss: 0.2009
2023-11-11 06:04:06,839 - mmseg - INFO - Iter [98750/160000]	lr: 3.201e-05, eta: 11:56:02, time: 0.640, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2134, decode.acc_seg: 91.3910, loss: 0.2134
2023-11-11 06:04:38,229 - mmseg - INFO - Iter [98800/160000]	lr: 3.196e-05, eta: 11:55:24, time: 0.628, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2067, decode.acc_seg: 91.5585, loss: 0.2067
2023-11-11 06:05:10,125 - mmseg - INFO - Iter [98850/160000]	lr: 3.191e-05, eta: 11:54:47, time: 0.638, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2030, decode.acc_seg: 91.5970, loss: 0.2030
2023-11-11 06:05:44,173 - mmseg - INFO - Iter [98900/160000]	lr: 3.187e-05, eta: 11:54:12, time: 0.680, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2015, decode.acc_seg: 91.5935, loss: 0.2015
2023-11-11 06:06:16,782 - mmseg - INFO - Iter [98950/160000]	lr: 3.182e-05, eta: 11:53:35, time: 0.653, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2058, decode.acc_seg: 91.1480, loss: 0.2058
2023-11-11 06:06:49,840 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 06:06:49,840 - mmseg - INFO - Iter [99000/160000]	lr: 3.178e-05, eta: 11:52:59, time: 0.660, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2156, decode.acc_seg: 91.1248, loss: 0.2156
2023-11-11 06:07:23,845 - mmseg - INFO - Iter [99050/160000]	lr: 3.173e-05, eta: 11:52:23, time: 0.681, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2071, decode.acc_seg: 91.5936, loss: 0.2071
2023-11-11 06:07:55,921 - mmseg - INFO - Iter [99100/160000]	lr: 3.169e-05, eta: 11:51:46, time: 0.640, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1960, decode.acc_seg: 91.6339, loss: 0.1960
2023-11-11 06:08:30,972 - mmseg - INFO - Iter [99150/160000]	lr: 3.164e-05, eta: 11:51:11, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2068, decode.acc_seg: 91.5896, loss: 0.2068
2023-11-11 06:09:06,112 - mmseg - INFO - Iter [99200/160000]	lr: 3.159e-05, eta: 11:50:36, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2147, decode.acc_seg: 91.3891, loss: 0.2147
2023-11-11 06:09:41,436 - mmseg - INFO - Iter [99250/160000]	lr: 3.155e-05, eta: 11:50:01, time: 0.707, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2049, decode.acc_seg: 91.4992, loss: 0.2049
2023-11-11 06:10:16,735 - mmseg - INFO - Iter [99300/160000]	lr: 3.150e-05, eta: 11:49:26, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2068, decode.acc_seg: 91.5809, loss: 0.2068
2023-11-11 06:10:49,497 - mmseg - INFO - Iter [99350/160000]	lr: 3.146e-05, eta: 11:48:50, time: 0.656, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2001, decode.acc_seg: 91.7064, loss: 0.2001
2023-11-11 06:11:24,911 - mmseg - INFO - Iter [99400/160000]	lr: 3.141e-05, eta: 11:48:15, time: 0.707, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2137, decode.acc_seg: 91.0311, loss: 0.2137
2023-11-11 06:12:01,627 - mmseg - INFO - Iter [99450/160000]	lr: 3.137e-05, eta: 11:47:41, time: 0.734, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2063, decode.acc_seg: 91.6168, loss: 0.2063
2023-11-11 06:12:38,252 - mmseg - INFO - Iter [99500/160000]	lr: 3.132e-05, eta: 11:47:07, time: 0.733, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2046, decode.acc_seg: 91.4905, loss: 0.2046
2023-11-11 06:13:11,770 - mmseg - INFO - Iter [99550/160000]	lr: 3.128e-05, eta: 11:46:31, time: 0.669, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2178, decode.acc_seg: 91.2351, loss: 0.2178
2023-11-11 06:13:47,339 - mmseg - INFO - Iter [99600/160000]	lr: 3.123e-05, eta: 11:45:56, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1952, decode.acc_seg: 91.8406, loss: 0.1952
2023-11-11 06:14:22,836 - mmseg - INFO - Iter [99650/160000]	lr: 3.118e-05, eta: 11:45:21, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2130, decode.acc_seg: 91.3000, loss: 0.2130
2023-11-11 06:14:57,475 - mmseg - INFO - Iter [99700/160000]	lr: 3.114e-05, eta: 11:44:46, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1975, decode.acc_seg: 91.8699, loss: 0.1975
2023-11-11 06:15:31,990 - mmseg - INFO - Iter [99750/160000]	lr: 3.109e-05, eta: 11:44:10, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2094, decode.acc_seg: 91.3960, loss: 0.2094
2023-11-11 06:16:07,087 - mmseg - INFO - Iter [99800/160000]	lr: 3.105e-05, eta: 11:43:35, time: 0.701, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1923, decode.acc_seg: 91.8352, loss: 0.1923
2023-11-11 06:16:42,573 - mmseg - INFO - Iter [99850/160000]	lr: 3.100e-05, eta: 11:43:01, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2031, decode.acc_seg: 91.6067, loss: 0.2031
2023-11-11 06:17:15,114 - mmseg - INFO - Iter [99900/160000]	lr: 3.096e-05, eta: 11:42:24, time: 0.652, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2070, decode.acc_seg: 91.5930, loss: 0.2070
2023-11-11 06:17:48,136 - mmseg - INFO - Iter [99950/160000]	lr: 3.091e-05, eta: 11:41:48, time: 0.660, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2057, decode.acc_seg: 91.5211, loss: 0.2057
2023-11-11 06:18:23,265 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 06:18:23,266 - mmseg - INFO - Iter [100000/160000]	lr: 3.087e-05, eta: 11:41:13, time: 0.701, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2114, decode.acc_seg: 91.2370, loss: 0.2114
2023-11-11 06:18:57,299 - mmseg - INFO - Iter [100050/160000]	lr: 3.082e-05, eta: 11:40:37, time: 0.681, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2019, decode.acc_seg: 91.5336, loss: 0.2019
2023-11-11 06:19:32,293 - mmseg - INFO - Iter [100100/160000]	lr: 3.078e-05, eta: 11:40:02, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2003, decode.acc_seg: 91.7700, loss: 0.2003
2023-11-11 06:20:07,279 - mmseg - INFO - Iter [100150/160000]	lr: 3.073e-05, eta: 11:39:27, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2095, decode.acc_seg: 91.2662, loss: 0.2095
2023-11-11 06:20:42,763 - mmseg - INFO - Iter [100200/160000]	lr: 3.069e-05, eta: 11:38:52, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2019, decode.acc_seg: 91.6664, loss: 0.2019
2023-11-11 06:21:15,823 - mmseg - INFO - Iter [100250/160000]	lr: 3.064e-05, eta: 11:38:16, time: 0.662, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2050, decode.acc_seg: 91.5496, loss: 0.2050
2023-11-11 06:21:48,040 - mmseg - INFO - Iter [100300/160000]	lr: 3.059e-05, eta: 11:37:39, time: 0.645, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2012, decode.acc_seg: 91.6589, loss: 0.2012
2023-11-11 06:22:21,205 - mmseg - INFO - Iter [100350/160000]	lr: 3.055e-05, eta: 11:37:03, time: 0.662, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2066, decode.acc_seg: 91.5031, loss: 0.2066
2023-11-11 06:22:55,696 - mmseg - INFO - Iter [100400/160000]	lr: 3.050e-05, eta: 11:36:27, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2056, decode.acc_seg: 91.4586, loss: 0.2056
2023-11-11 06:23:30,336 - mmseg - INFO - Iter [100450/160000]	lr: 3.046e-05, eta: 11:35:52, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2031, decode.acc_seg: 91.4825, loss: 0.2031
2023-11-11 06:24:05,493 - mmseg - INFO - Iter [100500/160000]	lr: 3.041e-05, eta: 11:35:17, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2168, decode.acc_seg: 91.4214, loss: 0.2168
2023-11-11 06:24:40,788 - mmseg - INFO - Iter [100550/160000]	lr: 3.037e-05, eta: 11:34:42, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1985, decode.acc_seg: 91.6998, loss: 0.1985
2023-11-11 06:25:15,858 - mmseg - INFO - Iter [100600/160000]	lr: 3.032e-05, eta: 11:34:07, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2139, decode.acc_seg: 91.3687, loss: 0.2139
2023-11-11 06:25:48,764 - mmseg - INFO - Iter [100650/160000]	lr: 3.028e-05, eta: 11:33:31, time: 0.659, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2153, decode.acc_seg: 91.1585, loss: 0.2153
2023-11-11 06:26:22,839 - mmseg - INFO - Iter [100700/160000]	lr: 3.023e-05, eta: 11:32:55, time: 0.681, data_time: 0.008, memory: 23129, decode.loss_ce: 0.2037, decode.acc_seg: 91.6603, loss: 0.2037
2023-11-11 06:26:58,460 - mmseg - INFO - Iter [100750/160000]	lr: 3.019e-05, eta: 11:32:20, time: 0.712, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2049, decode.acc_seg: 91.4801, loss: 0.2049
2023-11-11 06:27:34,206 - mmseg - INFO - Iter [100800/160000]	lr: 3.014e-05, eta: 11:31:46, time: 0.715, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2235, decode.acc_seg: 91.0315, loss: 0.2235
2023-11-11 06:28:09,728 - mmseg - INFO - Iter [100850/160000]	lr: 3.010e-05, eta: 11:31:11, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1960, decode.acc_seg: 91.7920, loss: 0.1960
2023-11-11 06:28:45,140 - mmseg - INFO - Iter [100900/160000]	lr: 3.005e-05, eta: 11:30:36, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2125, decode.acc_seg: 91.1998, loss: 0.2125
2023-11-11 06:29:20,173 - mmseg - INFO - Iter [100950/160000]	lr: 3.001e-05, eta: 11:30:01, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1953, decode.acc_seg: 91.9022, loss: 0.1953
2023-11-11 06:29:52,420 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 06:29:52,420 - mmseg - INFO - Iter [101000/160000]	lr: 2.996e-05, eta: 11:29:24, time: 0.644, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1918, decode.acc_seg: 91.7615, loss: 0.1918
2023-11-11 06:30:27,708 - mmseg - INFO - Iter [101050/160000]	lr: 2.992e-05, eta: 11:28:49, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2000, decode.acc_seg: 91.6425, loss: 0.2000
2023-11-11 06:31:03,405 - mmseg - INFO - Iter [101100/160000]	lr: 2.987e-05, eta: 11:28:15, time: 0.714, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1998, decode.acc_seg: 91.5365, loss: 0.1998
2023-11-11 06:31:37,156 - mmseg - INFO - Iter [101150/160000]	lr: 2.983e-05, eta: 11:27:39, time: 0.676, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1972, decode.acc_seg: 91.8205, loss: 0.1972
2023-11-11 06:32:11,385 - mmseg - INFO - Iter [101200/160000]	lr: 2.978e-05, eta: 11:27:03, time: 0.684, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2031, decode.acc_seg: 91.6881, loss: 0.2031
2023-11-11 06:32:46,930 - mmseg - INFO - Iter [101250/160000]	lr: 2.974e-05, eta: 11:26:29, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1935, decode.acc_seg: 92.0299, loss: 0.1935
2023-11-11 06:33:21,831 - mmseg - INFO - Iter [101300/160000]	lr: 2.969e-05, eta: 11:25:53, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1856, decode.acc_seg: 92.0191, loss: 0.1856
2023-11-11 06:33:56,885 - mmseg - INFO - Iter [101350/160000]	lr: 2.965e-05, eta: 11:25:18, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1957, decode.acc_seg: 91.8344, loss: 0.1957
2023-11-11 06:34:30,912 - mmseg - INFO - Iter [101400/160000]	lr: 2.960e-05, eta: 11:24:43, time: 0.681, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2064, decode.acc_seg: 91.4967, loss: 0.2064
2023-11-11 06:35:06,486 - mmseg - INFO - Iter [101450/160000]	lr: 2.956e-05, eta: 11:24:08, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1985, decode.acc_seg: 91.8250, loss: 0.1985
2023-11-11 06:35:41,743 - mmseg - INFO - Iter [101500/160000]	lr: 2.951e-05, eta: 11:23:33, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2029, decode.acc_seg: 91.6277, loss: 0.2029
2023-11-11 06:36:17,057 - mmseg - INFO - Iter [101550/160000]	lr: 2.947e-05, eta: 11:22:58, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2029, decode.acc_seg: 91.6132, loss: 0.2029
2023-11-11 06:36:52,645 - mmseg - INFO - Iter [101600/160000]	lr: 2.943e-05, eta: 11:22:23, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2032, decode.acc_seg: 91.5852, loss: 0.2032
2023-11-11 06:37:27,942 - mmseg - INFO - Iter [101650/160000]	lr: 2.938e-05, eta: 11:21:49, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1951, decode.acc_seg: 92.1143, loss: 0.1951
2023-11-11 06:38:03,639 - mmseg - INFO - Iter [101700/160000]	lr: 2.934e-05, eta: 11:21:14, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2031, decode.acc_seg: 91.5204, loss: 0.2031
2023-11-11 06:38:36,887 - mmseg - INFO - Iter [101750/160000]	lr: 2.929e-05, eta: 11:20:38, time: 0.665, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1921, decode.acc_seg: 92.0435, loss: 0.1921
2023-11-11 06:39:12,288 - mmseg - INFO - Iter [101800/160000]	lr: 2.925e-05, eta: 11:20:03, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2097, decode.acc_seg: 91.4329, loss: 0.2097
2023-11-11 06:39:46,243 - mmseg - INFO - Iter [101850/160000]	lr: 2.920e-05, eta: 11:19:27, time: 0.680, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1988, decode.acc_seg: 91.6592, loss: 0.1988
2023-11-11 06:40:21,504 - mmseg - INFO - Iter [101900/160000]	lr: 2.916e-05, eta: 11:18:52, time: 0.704, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2003, decode.acc_seg: 91.6209, loss: 0.2003
2023-11-11 06:40:56,761 - mmseg - INFO - Iter [101950/160000]	lr: 2.911e-05, eta: 11:18:17, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2032, decode.acc_seg: 91.8310, loss: 0.2032
2023-11-11 06:41:30,035 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 06:41:30,035 - mmseg - INFO - Iter [102000/160000]	lr: 2.907e-05, eta: 11:17:41, time: 0.667, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2167, decode.acc_seg: 91.0878, loss: 0.2167
2023-11-11 06:42:05,199 - mmseg - INFO - Iter [102050/160000]	lr: 2.902e-05, eta: 11:17:06, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1871, decode.acc_seg: 92.3904, loss: 0.1871
2023-11-11 06:42:40,203 - mmseg - INFO - Iter [102100/160000]	lr: 2.898e-05, eta: 11:16:31, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1941, decode.acc_seg: 91.7460, loss: 0.1941
2023-11-11 06:43:13,457 - mmseg - INFO - Iter [102150/160000]	lr: 2.893e-05, eta: 11:15:55, time: 0.664, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1874, decode.acc_seg: 91.9540, loss: 0.1874
2023-11-11 06:43:46,230 - mmseg - INFO - Iter [102200/160000]	lr: 2.889e-05, eta: 11:15:19, time: 0.655, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2103, decode.acc_seg: 91.3377, loss: 0.2103
2023-11-11 06:44:20,803 - mmseg - INFO - Iter [102250/160000]	lr: 2.885e-05, eta: 11:14:44, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2054, decode.acc_seg: 91.6892, loss: 0.2054
2023-11-11 06:44:54,690 - mmseg - INFO - Iter [102300/160000]	lr: 2.880e-05, eta: 11:14:08, time: 0.677, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1952, decode.acc_seg: 92.1106, loss: 0.1952
2023-11-11 06:45:29,188 - mmseg - INFO - Iter [102350/160000]	lr: 2.876e-05, eta: 11:13:32, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2040, decode.acc_seg: 91.4407, loss: 0.2040
2023-11-11 06:46:03,329 - mmseg - INFO - Iter [102400/160000]	lr: 2.871e-05, eta: 11:12:57, time: 0.682, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2016, decode.acc_seg: 91.6810, loss: 0.2016
2023-11-11 06:46:38,576 - mmseg - INFO - Iter [102450/160000]	lr: 2.867e-05, eta: 11:12:22, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1995, decode.acc_seg: 91.7068, loss: 0.1995
2023-11-11 06:47:13,351 - mmseg - INFO - Iter [102500/160000]	lr: 2.862e-05, eta: 11:11:47, time: 0.694, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2009, decode.acc_seg: 91.6561, loss: 0.2009
2023-11-11 06:47:46,865 - mmseg - INFO - Iter [102550/160000]	lr: 2.858e-05, eta: 11:11:11, time: 0.671, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2033, decode.acc_seg: 91.7118, loss: 0.2033
2023-11-11 06:48:19,517 - mmseg - INFO - Iter [102600/160000]	lr: 2.853e-05, eta: 11:10:34, time: 0.653, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1898, decode.acc_seg: 92.0691, loss: 0.1898
2023-11-11 06:48:53,230 - mmseg - INFO - Iter [102650/160000]	lr: 2.849e-05, eta: 11:09:59, time: 0.674, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2051, decode.acc_seg: 91.6045, loss: 0.2051
2023-11-11 06:49:28,214 - mmseg - INFO - Iter [102700/160000]	lr: 2.845e-05, eta: 11:09:24, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2078, decode.acc_seg: 91.3273, loss: 0.2078
2023-11-11 06:50:03,506 - mmseg - INFO - Iter [102750/160000]	lr: 2.840e-05, eta: 11:08:49, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1892, decode.acc_seg: 92.0359, loss: 0.1892
2023-11-11 06:50:36,275 - mmseg - INFO - Iter [102800/160000]	lr: 2.836e-05, eta: 11:08:12, time: 0.657, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1932, decode.acc_seg: 91.8970, loss: 0.1932
2023-11-11 06:51:08,721 - mmseg - INFO - Iter [102850/160000]	lr: 2.831e-05, eta: 11:07:36, time: 0.648, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1905, decode.acc_seg: 91.9760, loss: 0.1905
2023-11-11 06:51:42,753 - mmseg - INFO - Iter [102900/160000]	lr: 2.827e-05, eta: 11:07:00, time: 0.682, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2065, decode.acc_seg: 91.5261, loss: 0.2065
2023-11-11 06:52:16,790 - mmseg - INFO - Iter [102950/160000]	lr: 2.822e-05, eta: 11:06:25, time: 0.680, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2077, decode.acc_seg: 91.2711, loss: 0.2077
2023-11-11 06:52:52,007 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 06:52:52,008 - mmseg - INFO - Iter [103000/160000]	lr: 2.818e-05, eta: 11:05:50, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1917, decode.acc_seg: 91.8988, loss: 0.1917
2023-11-11 06:53:27,018 - mmseg - INFO - Iter [103050/160000]	lr: 2.814e-05, eta: 11:05:15, time: 0.700, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1948, decode.acc_seg: 92.0929, loss: 0.1948
2023-11-11 06:54:01,338 - mmseg - INFO - Iter [103100/160000]	lr: 2.809e-05, eta: 11:04:39, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1987, decode.acc_seg: 92.0710, loss: 0.1987
2023-11-11 06:54:36,117 - mmseg - INFO - Iter [103150/160000]	lr: 2.805e-05, eta: 11:04:04, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1927, decode.acc_seg: 91.9828, loss: 0.1927
2023-11-11 06:55:11,598 - mmseg - INFO - Iter [103200/160000]	lr: 2.800e-05, eta: 11:03:29, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2039, decode.acc_seg: 91.3597, loss: 0.2039
2023-11-11 06:55:46,933 - mmseg - INFO - Iter [103250/160000]	lr: 2.796e-05, eta: 11:02:54, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2047, decode.acc_seg: 91.7156, loss: 0.2047
2023-11-11 06:56:22,207 - mmseg - INFO - Iter [103300/160000]	lr: 2.792e-05, eta: 11:02:19, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1941, decode.acc_seg: 91.8944, loss: 0.1941
2023-11-11 06:56:57,805 - mmseg - INFO - Iter [103350/160000]	lr: 2.787e-05, eta: 11:01:45, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1937, decode.acc_seg: 92.1921, loss: 0.1937
2023-11-11 06:57:32,977 - mmseg - INFO - Iter [103400/160000]	lr: 2.783e-05, eta: 11:01:10, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1974, decode.acc_seg: 91.8702, loss: 0.1974
2023-11-11 06:58:08,948 - mmseg - INFO - Iter [103450/160000]	lr: 2.778e-05, eta: 11:00:35, time: 0.718, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2119, decode.acc_seg: 91.2408, loss: 0.2119
2023-11-11 06:58:45,294 - mmseg - INFO - Iter [103500/160000]	lr: 2.774e-05, eta: 11:00:01, time: 0.727, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2012, decode.acc_seg: 91.7570, loss: 0.2012
2023-11-11 06:59:17,657 - mmseg - INFO - Iter [103550/160000]	lr: 2.770e-05, eta: 10:59:24, time: 0.647, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2097, decode.acc_seg: 91.3036, loss: 0.2097
2023-11-11 06:59:53,151 - mmseg - INFO - Iter [103600/160000]	lr: 2.765e-05, eta: 10:58:49, time: 0.710, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1900, decode.acc_seg: 92.1683, loss: 0.1900
2023-11-11 07:00:28,862 - mmseg - INFO - Iter [103650/160000]	lr: 2.761e-05, eta: 10:58:15, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1907, decode.acc_seg: 91.9619, loss: 0.1907
2023-11-11 07:01:04,372 - mmseg - INFO - Iter [103700/160000]	lr: 2.756e-05, eta: 10:57:40, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2083, decode.acc_seg: 91.4088, loss: 0.2083
2023-11-11 07:01:39,685 - mmseg - INFO - Iter [103750/160000]	lr: 2.752e-05, eta: 10:57:05, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2015, decode.acc_seg: 91.6884, loss: 0.2015
2023-11-11 07:02:15,331 - mmseg - INFO - Iter [103800/160000]	lr: 2.748e-05, eta: 10:56:30, time: 0.713, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2013, decode.acc_seg: 91.6647, loss: 0.2013
2023-11-11 07:02:50,486 - mmseg - INFO - Iter [103850/160000]	lr: 2.743e-05, eta: 10:55:55, time: 0.704, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1906, decode.acc_seg: 91.9806, loss: 0.1906
2023-11-11 07:03:23,170 - mmseg - INFO - Iter [103900/160000]	lr: 2.739e-05, eta: 10:55:19, time: 0.654, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2071, decode.acc_seg: 91.6002, loss: 0.2071
2023-11-11 07:03:56,282 - mmseg - INFO - Iter [103950/160000]	lr: 2.735e-05, eta: 10:54:43, time: 0.662, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2108, decode.acc_seg: 91.3323, loss: 0.2108
2023-11-11 07:04:28,935 - mmseg - INFO - Saving checkpoint at 104000 iterations
2023-11-11 07:04:33,628 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 07:04:33,628 - mmseg - INFO - Iter [104000/160000]	lr: 2.730e-05, eta: 10:54:09, time: 0.747, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1941, decode.acc_seg: 91.8085, loss: 0.1941
2023-11-11 07:06:04,241 - mmseg - INFO - per class results:
2023-11-11 07:06:04,255 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 77.56 | 87.06 |
|       building      | 81.44 | 92.06 |
|         sky         | 94.14 | 97.38 |
|        floor        | 82.17 | 90.24 |
|         tree        | 74.04 | 88.62 |
|       ceiling       | 83.56 | 92.95 |
|         road        | 82.95 | 90.19 |
|         bed         | 89.18 | 96.34 |
|      windowpane     |  61.7 | 78.67 |
|        grass        |  65.2 | 81.68 |
|       cabinet       | 60.46 | 75.41 |
|       sidewalk      | 66.29 | 82.18 |
|        person       | 80.61 | 93.51 |
|        earth        | 35.93 | 50.12 |
|         door        | 50.71 | 65.84 |
|        table        | 59.12 | 76.54 |
|       mountain      | 56.22 |  72.5 |
|        plant        | 49.77 | 58.89 |
|       curtain       | 73.11 | 88.05 |
|        chair        | 57.16 | 73.04 |
|         car         | 83.25 | 92.66 |
|        water        | 59.32 | 72.79 |
|       painting      | 72.03 | 86.92 |
|         sofa        | 63.35 | 77.47 |
|        shelf        | 42.93 | 61.14 |
|        house        | 28.58 | 34.42 |
|         sea         | 56.24 | 75.86 |
|        mirror       | 65.75 | 72.59 |
|         rug         | 60.66 | 73.91 |
|        field        | 29.06 | 43.97 |
|       armchair      | 40.19 | 60.95 |
|         seat        | 62.54 | 81.89 |
|        fence        | 42.77 | 59.78 |
|         desk        | 48.04 | 66.31 |
|         rock        | 39.52 |  63.0 |
|       wardrobe      | 50.98 | 67.49 |
|         lamp        |  64.1 | 75.83 |
|       bathtub       |  79.3 | 85.16 |
|       railing       | 32.48 | 41.36 |
|       cushion       | 58.12 | 72.73 |
|         base        | 31.55 | 42.73 |
|         box         | 28.86 | 37.67 |
|        column       | 44.53 | 53.75 |
|      signboard      | 37.06 | 48.68 |
|   chest of drawers  |  45.8 | 64.65 |
|       counter       | 30.45 | 34.59 |
|         sand        | 38.81 | 59.41 |
|         sink        | 67.91 | 79.81 |
|      skyscraper     | 48.96 | 61.59 |
|      fireplace      | 72.72 | 89.49 |
|     refrigerator    |  74.9 | 83.65 |
|      grandstand     | 36.83 | 77.71 |
|         path        |  23.4 |  34.0 |
|        stairs       | 31.78 | 39.71 |
|        runway       | 64.14 | 83.78 |
|         case        | 49.04 | 60.44 |
|      pool table     |  92.9 | 96.96 |
|        pillow       | 57.87 | 66.44 |
|     screen door     | 74.61 | 88.42 |
|       stairway      | 32.16 | 36.82 |
|        river        | 10.62 |  23.5 |
|        bridge       | 66.85 | 86.36 |
|       bookcase      | 35.63 | 55.57 |
|        blind        | 40.12 | 44.07 |
|     coffee table    | 52.55 |  81.1 |
|        toilet       | 85.66 | 92.51 |
|        flower       | 37.32 | 61.55 |
|         book        | 46.57 | 68.41 |
|         hill        | 10.89 | 18.43 |
|        bench        | 43.22 | 52.17 |
|      countertop     | 63.74 | 84.78 |
|        stove        | 73.96 | 85.09 |
|         palm        | 48.14 | 68.59 |
|    kitchen island   | 39.74 | 67.79 |
|       computer      | 63.39 | 75.16 |
|     swivel chair    | 48.78 | 69.28 |
|         boat        | 43.09 | 54.58 |
|         bar         | 34.87 | 47.06 |
|    arcade machine   | 78.38 | 82.61 |
|        hovel        | 62.17 | 72.02 |
|         bus         | 84.24 | 96.41 |
|        towel        |  62.9 | 75.48 |
|        light        | 53.94 | 62.29 |
|        truck        |  30.8 | 37.54 |
|        tower        |  5.94 |  8.48 |
|      chandelier     | 65.85 | 82.47 |
|        awning       | 25.51 |  30.6 |
|     streetlight     | 26.09 | 33.94 |
|        booth        | 58.99 | 61.38 |
| television receiver | 65.19 | 82.46 |
|       airplane      | 59.49 | 72.23 |
|      dirt track     |  1.91 |  1.99 |
|       apparel       | 40.82 | 59.01 |
|         pole        | 23.51 | 31.56 |
|         land        |  0.1  |  0.15 |
|      bannister      | 15.27 | 19.27 |
|      escalator      |  50.7 | 64.19 |
|       ottoman       | 47.37 | 62.54 |
|        bottle       | 35.64 | 59.58 |
|        buffet       | 44.88 | 54.72 |
|        poster       | 33.63 | 43.62 |
|        stage        | 18.47 | 28.13 |
|         van         |  39.9 |  54.4 |
|         ship        | 52.51 | 74.73 |
|       fountain      | 43.92 | 44.27 |
|    conveyer belt    | 82.49 | 93.69 |
|        canopy       | 33.52 |  49.3 |
|        washer       | 76.13 | 77.44 |
|      plaything      | 19.31 | 32.43 |
|    swimming pool    | 75.37 | 84.19 |
|        stool        | 34.73 |  48.8 |
|        barrel       | 56.67 | 71.02 |
|        basket       | 34.49 | 44.66 |
|      waterfall      | 76.81 | 85.57 |
|         tent        | 83.74 | 98.16 |
|         bag         |  17.3 | 21.92 |
|       minibike      | 52.26 | 58.59 |
|        cradle       | 75.46 | 97.55 |
|         oven        | 46.67 | 56.05 |
|         ball        | 36.79 | 62.41 |
|         food        | 49.05 | 53.49 |
|         step        | 13.64 | 17.22 |
|         tank        | 38.15 |  43.4 |
|      trade name     | 28.54 | 35.02 |
|      microwave      | 80.36 | 92.95 |
|         pot         | 41.02 | 46.19 |
|        animal       | 53.52 | 55.96 |
|       bicycle       | 52.66 | 81.41 |
|         lake        |  32.7 | 41.91 |
|      dishwasher     | 55.96 | 64.83 |
|        screen       | 57.38 | 81.81 |
|       blanket       | 11.38 | 12.51 |
|      sculpture      | 64.47 | 75.42 |
|         hood        | 56.04 | 69.98 |
|        sconce       | 46.23 | 54.02 |
|         vase        | 37.31 | 55.76 |
|    traffic light    | 24.44 | 48.43 |
|         tray        |  7.55 | 13.74 |
|        ashcan       | 44.37 | 60.58 |
|         fan         | 57.58 | 75.45 |
|         pier        |  18.3 | 19.61 |
|      crt screen     |  1.09 |  4.17 |
|        plate        |  45.8 | 58.88 |
|       monitor       | 10.26 |  11.9 |
|    bulletin board   | 46.74 | 58.97 |
|        shower       |  6.31 |  7.33 |
|       radiator      |  61.9 | 72.93 |
|        glass        | 11.74 | 12.33 |
|        clock        | 33.99 | 43.22 |
|         flag        |  44.0 | 46.55 |
+---------------------+-------+-------+
2023-11-11 07:06:04,255 - mmseg - INFO - Summary:
2023-11-11 07:06:04,256 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 82.78 | 49.02 | 61.12 |
+-------+-------+-------+
2023-11-11 07:06:04,389 - mmseg - INFO - The previous best checkpoint /data/Next-ViT/segmentation/work_dirs/fpn_512_debi_base_160k/best_mIoU_iter_96000.pth was removed
2023-11-11 07:06:07,353 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_104000.pth.
2023-11-11 07:06:07,353 - mmseg - INFO - Best mIoU is 0.4902 at 104000 iter.
2023-11-11 07:06:07,373 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 07:06:07,374 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8278, mIoU: 0.4902, mAcc: 0.6112, IoU.wall: 0.7756, IoU.building: 0.8144, IoU.sky: 0.9414, IoU.floor: 0.8217, IoU.tree: 0.7404, IoU.ceiling: 0.8356, IoU.road: 0.8295, IoU.bed : 0.8918, IoU.windowpane: 0.6170, IoU.grass: 0.6520, IoU.cabinet: 0.6046, IoU.sidewalk: 0.6629, IoU.person: 0.8061, IoU.earth: 0.3593, IoU.door: 0.5071, IoU.table: 0.5912, IoU.mountain: 0.5622, IoU.plant: 0.4977, IoU.curtain: 0.7311, IoU.chair: 0.5716, IoU.car: 0.8325, IoU.water: 0.5932, IoU.painting: 0.7203, IoU.sofa: 0.6335, IoU.shelf: 0.4293, IoU.house: 0.2858, IoU.sea: 0.5624, IoU.mirror: 0.6575, IoU.rug: 0.6066, IoU.field: 0.2906, IoU.armchair: 0.4019, IoU.seat: 0.6254, IoU.fence: 0.4277, IoU.desk: 0.4804, IoU.rock: 0.3952, IoU.wardrobe: 0.5098, IoU.lamp: 0.6410, IoU.bathtub: 0.7930, IoU.railing: 0.3248, IoU.cushion: 0.5812, IoU.base: 0.3155, IoU.box: 0.2886, IoU.column: 0.4453, IoU.signboard: 0.3706, IoU.chest of drawers: 0.4580, IoU.counter: 0.3045, IoU.sand: 0.3881, IoU.sink: 0.6791, IoU.skyscraper: 0.4896, IoU.fireplace: 0.7272, IoU.refrigerator: 0.7490, IoU.grandstand: 0.3683, IoU.path: 0.2340, IoU.stairs: 0.3178, IoU.runway: 0.6414, IoU.case: 0.4904, IoU.pool table: 0.9290, IoU.pillow: 0.5787, IoU.screen door: 0.7461, IoU.stairway: 0.3216, IoU.river: 0.1062, IoU.bridge: 0.6685, IoU.bookcase: 0.3563, IoU.blind: 0.4012, IoU.coffee table: 0.5255, IoU.toilet: 0.8566, IoU.flower: 0.3732, IoU.book: 0.4657, IoU.hill: 0.1089, IoU.bench: 0.4322, IoU.countertop: 0.6374, IoU.stove: 0.7396, IoU.palm: 0.4814, IoU.kitchen island: 0.3974, IoU.computer: 0.6339, IoU.swivel chair: 0.4878, IoU.boat: 0.4309, IoU.bar: 0.3487, IoU.arcade machine: 0.7838, IoU.hovel: 0.6217, IoU.bus: 0.8424, IoU.towel: 0.6290, IoU.light: 0.5394, IoU.truck: 0.3080, IoU.tower: 0.0594, IoU.chandelier: 0.6585, IoU.awning: 0.2551, IoU.streetlight: 0.2609, IoU.booth: 0.5899, IoU.television receiver: 0.6519, IoU.airplane: 0.5949, IoU.dirt track: 0.0191, IoU.apparel: 0.4082, IoU.pole: 0.2351, IoU.land: 0.0010, IoU.bannister: 0.1527, IoU.escalator: 0.5070, IoU.ottoman: 0.4737, IoU.bottle: 0.3564, IoU.buffet: 0.4488, IoU.poster: 0.3363, IoU.stage: 0.1847, IoU.van: 0.3990, IoU.ship: 0.5251, IoU.fountain: 0.4392, IoU.conveyer belt: 0.8249, IoU.canopy: 0.3352, IoU.washer: 0.7613, IoU.plaything: 0.1931, IoU.swimming pool: 0.7537, IoU.stool: 0.3473, IoU.barrel: 0.5667, IoU.basket: 0.3449, IoU.waterfall: 0.7681, IoU.tent: 0.8374, IoU.bag: 0.1730, IoU.minibike: 0.5226, IoU.cradle: 0.7546, IoU.oven: 0.4667, IoU.ball: 0.3679, IoU.food: 0.4905, IoU.step: 0.1364, IoU.tank: 0.3815, IoU.trade name: 0.2854, IoU.microwave: 0.8036, IoU.pot: 0.4102, IoU.animal: 0.5352, IoU.bicycle: 0.5266, IoU.lake: 0.3270, IoU.dishwasher: 0.5596, IoU.screen: 0.5738, IoU.blanket: 0.1138, IoU.sculpture: 0.6447, IoU.hood: 0.5604, IoU.sconce: 0.4623, IoU.vase: 0.3731, IoU.traffic light: 0.2444, IoU.tray: 0.0755, IoU.ashcan: 0.4437, IoU.fan: 0.5758, IoU.pier: 0.1830, IoU.crt screen: 0.0109, IoU.plate: 0.4580, IoU.monitor: 0.1026, IoU.bulletin board: 0.4674, IoU.shower: 0.0631, IoU.radiator: 0.6190, IoU.glass: 0.1174, IoU.clock: 0.3399, IoU.flag: 0.4400, Acc.wall: 0.8706, Acc.building: 0.9206, Acc.sky: 0.9738, Acc.floor: 0.9024, Acc.tree: 0.8862, Acc.ceiling: 0.9295, Acc.road: 0.9019, Acc.bed : 0.9634, Acc.windowpane: 0.7867, Acc.grass: 0.8168, Acc.cabinet: 0.7541, Acc.sidewalk: 0.8218, Acc.person: 0.9351, Acc.earth: 0.5012, Acc.door: 0.6584, Acc.table: 0.7654, Acc.mountain: 0.7250, Acc.plant: 0.5889, Acc.curtain: 0.8805, Acc.chair: 0.7304, Acc.car: 0.9266, Acc.water: 0.7279, Acc.painting: 0.8692, Acc.sofa: 0.7747, Acc.shelf: 0.6114, Acc.house: 0.3442, Acc.sea: 0.7586, Acc.mirror: 0.7259, Acc.rug: 0.7391, Acc.field: 0.4397, Acc.armchair: 0.6095, Acc.seat: 0.8189, Acc.fence: 0.5978, Acc.desk: 0.6631, Acc.rock: 0.6300, Acc.wardrobe: 0.6749, Acc.lamp: 0.7583, Acc.bathtub: 0.8516, Acc.railing: 0.4136, Acc.cushion: 0.7273, Acc.base: 0.4273, Acc.box: 0.3767, Acc.column: 0.5375, Acc.signboard: 0.4868, Acc.chest of drawers: 0.6465, Acc.counter: 0.3459, Acc.sand: 0.5941, Acc.sink: 0.7981, Acc.skyscraper: 0.6159, Acc.fireplace: 0.8949, Acc.refrigerator: 0.8365, Acc.grandstand: 0.7771, Acc.path: 0.3400, Acc.stairs: 0.3971, Acc.runway: 0.8378, Acc.case: 0.6044, Acc.pool table: 0.9696, Acc.pillow: 0.6644, Acc.screen door: 0.8842, Acc.stairway: 0.3682, Acc.river: 0.2350, Acc.bridge: 0.8636, Acc.bookcase: 0.5557, Acc.blind: 0.4407, Acc.coffee table: 0.8110, Acc.toilet: 0.9251, Acc.flower: 0.6155, Acc.book: 0.6841, Acc.hill: 0.1843, Acc.bench: 0.5217, Acc.countertop: 0.8478, Acc.stove: 0.8509, Acc.palm: 0.6859, Acc.kitchen island: 0.6779, Acc.computer: 0.7516, Acc.swivel chair: 0.6928, Acc.boat: 0.5458, Acc.bar: 0.4706, Acc.arcade machine: 0.8261, Acc.hovel: 0.7202, Acc.bus: 0.9641, Acc.towel: 0.7548, Acc.light: 0.6229, Acc.truck: 0.3754, Acc.tower: 0.0848, Acc.chandelier: 0.8247, Acc.awning: 0.3060, Acc.streetlight: 0.3394, Acc.booth: 0.6138, Acc.television receiver: 0.8246, Acc.airplane: 0.7223, Acc.dirt track: 0.0199, Acc.apparel: 0.5901, Acc.pole: 0.3156, Acc.land: 0.0015, Acc.bannister: 0.1927, Acc.escalator: 0.6419, Acc.ottoman: 0.6254, Acc.bottle: 0.5958, Acc.buffet: 0.5472, Acc.poster: 0.4362, Acc.stage: 0.2813, Acc.van: 0.5440, Acc.ship: 0.7473, Acc.fountain: 0.4427, Acc.conveyer belt: 0.9369, Acc.canopy: 0.4930, Acc.washer: 0.7744, Acc.plaything: 0.3243, Acc.swimming pool: 0.8419, Acc.stool: 0.4880, Acc.barrel: 0.7102, Acc.basket: 0.4466, Acc.waterfall: 0.8557, Acc.tent: 0.9816, Acc.bag: 0.2192, Acc.minibike: 0.5859, Acc.cradle: 0.9755, Acc.oven: 0.5605, Acc.ball: 0.6241, Acc.food: 0.5349, Acc.step: 0.1722, Acc.tank: 0.4340, Acc.trade name: 0.3502, Acc.microwave: 0.9295, Acc.pot: 0.4619, Acc.animal: 0.5596, Acc.bicycle: 0.8141, Acc.lake: 0.4191, Acc.dishwasher: 0.6483, Acc.screen: 0.8181, Acc.blanket: 0.1251, Acc.sculpture: 0.7542, Acc.hood: 0.6998, Acc.sconce: 0.5402, Acc.vase: 0.5576, Acc.traffic light: 0.4843, Acc.tray: 0.1374, Acc.ashcan: 0.6058, Acc.fan: 0.7545, Acc.pier: 0.1961, Acc.crt screen: 0.0417, Acc.plate: 0.5888, Acc.monitor: 0.1190, Acc.bulletin board: 0.5897, Acc.shower: 0.0733, Acc.radiator: 0.7293, Acc.glass: 0.1233, Acc.clock: 0.4322, Acc.flag: 0.4655
2023-11-11 07:06:40,909 - mmseg - INFO - Iter [104050/160000]	lr: 2.726e-05, eta: 10:54:24, time: 2.544, data_time: 1.884, memory: 23129, decode.loss_ce: 0.1984, decode.acc_seg: 91.7569, loss: 0.1984
2023-11-11 07:07:16,776 - mmseg - INFO - Iter [104100/160000]	lr: 2.721e-05, eta: 10:53:49, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2132, decode.acc_seg: 91.3510, loss: 0.2132
2023-11-11 07:07:52,029 - mmseg - INFO - Iter [104150/160000]	lr: 2.717e-05, eta: 10:53:14, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1998, decode.acc_seg: 91.7085, loss: 0.1998
2023-11-11 07:08:27,701 - mmseg - INFO - Iter [104200/160000]	lr: 2.713e-05, eta: 10:52:39, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1906, decode.acc_seg: 92.1016, loss: 0.1906
2023-11-11 07:09:02,950 - mmseg - INFO - Iter [104250/160000]	lr: 2.708e-05, eta: 10:52:04, time: 0.705, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1940, decode.acc_seg: 91.9266, loss: 0.1940
2023-11-11 07:09:38,370 - mmseg - INFO - Iter [104300/160000]	lr: 2.704e-05, eta: 10:51:29, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1915, decode.acc_seg: 92.1975, loss: 0.1915
2023-11-11 07:10:12,784 - mmseg - INFO - Iter [104350/160000]	lr: 2.700e-05, eta: 10:50:54, time: 0.689, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1956, decode.acc_seg: 91.9054, loss: 0.1956
2023-11-11 07:10:44,219 - mmseg - INFO - Iter [104400/160000]	lr: 2.695e-05, eta: 10:50:17, time: 0.629, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1978, decode.acc_seg: 91.9176, loss: 0.1978
2023-11-11 07:11:15,762 - mmseg - INFO - Iter [104450/160000]	lr: 2.691e-05, eta: 10:49:40, time: 0.631, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1894, decode.acc_seg: 92.1048, loss: 0.1894
2023-11-11 07:11:49,212 - mmseg - INFO - Iter [104500/160000]	lr: 2.687e-05, eta: 10:49:04, time: 0.668, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2069, decode.acc_seg: 91.3571, loss: 0.2069
2023-11-11 07:12:22,506 - mmseg - INFO - Iter [104550/160000]	lr: 2.682e-05, eta: 10:48:28, time: 0.667, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1935, decode.acc_seg: 91.9313, loss: 0.1935
2023-11-11 07:12:57,449 - mmseg - INFO - Iter [104600/160000]	lr: 2.678e-05, eta: 10:47:53, time: 0.698, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2108, decode.acc_seg: 91.3177, loss: 0.2108
2023-11-11 07:13:30,755 - mmseg - INFO - Iter [104650/160000]	lr: 2.673e-05, eta: 10:47:17, time: 0.666, data_time: 0.011, memory: 23129, decode.loss_ce: 0.2115, decode.acc_seg: 91.6588, loss: 0.2115
2023-11-11 07:14:05,822 - mmseg - INFO - Iter [104700/160000]	lr: 2.669e-05, eta: 10:46:42, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1975, decode.acc_seg: 91.8706, loss: 0.1975
2023-11-11 07:14:41,299 - mmseg - INFO - Iter [104750/160000]	lr: 2.665e-05, eta: 10:46:07, time: 0.709, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2019, decode.acc_seg: 91.7653, loss: 0.2019
2023-11-11 07:15:16,408 - mmseg - INFO - Iter [104800/160000]	lr: 2.660e-05, eta: 10:45:32, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1940, decode.acc_seg: 91.6842, loss: 0.1940
2023-11-11 07:15:51,686 - mmseg - INFO - Iter [104850/160000]	lr: 2.656e-05, eta: 10:44:57, time: 0.706, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1858, decode.acc_seg: 92.1998, loss: 0.1858
2023-11-11 07:16:23,432 - mmseg - INFO - Iter [104900/160000]	lr: 2.652e-05, eta: 10:44:20, time: 0.636, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1931, decode.acc_seg: 91.9125, loss: 0.1931
2023-11-11 07:16:57,538 - mmseg - INFO - Iter [104950/160000]	lr: 2.647e-05, eta: 10:43:44, time: 0.681, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1857, decode.acc_seg: 92.2711, loss: 0.1857
2023-11-11 07:17:33,607 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 07:17:33,607 - mmseg - INFO - Iter [105000/160000]	lr: 2.643e-05, eta: 10:43:10, time: 0.721, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1970, decode.acc_seg: 91.7348, loss: 0.1970
2023-11-11 07:18:09,111 - mmseg - INFO - Iter [105050/160000]	lr: 2.639e-05, eta: 10:42:35, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2160, decode.acc_seg: 91.3568, loss: 0.2160
2023-11-11 07:18:44,923 - mmseg - INFO - Iter [105100/160000]	lr: 2.634e-05, eta: 10:42:00, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1998, decode.acc_seg: 91.5316, loss: 0.1998
2023-11-11 07:19:20,569 - mmseg - INFO - Iter [105150/160000]	lr: 2.630e-05, eta: 10:41:25, time: 0.713, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1978, decode.acc_seg: 91.5196, loss: 0.1978
2023-11-11 07:19:54,596 - mmseg - INFO - Iter [105200/160000]	lr: 2.626e-05, eta: 10:40:50, time: 0.682, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1884, decode.acc_seg: 92.2221, loss: 0.1884
2023-11-11 07:20:28,603 - mmseg - INFO - Iter [105250/160000]	lr: 2.621e-05, eta: 10:40:14, time: 0.679, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1908, decode.acc_seg: 92.0132, loss: 0.1908
2023-11-11 07:21:03,621 - mmseg - INFO - Iter [105300/160000]	lr: 2.617e-05, eta: 10:39:39, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2077, decode.acc_seg: 91.2773, loss: 0.2077
2023-11-11 07:21:36,570 - mmseg - INFO - Iter [105350/160000]	lr: 2.613e-05, eta: 10:39:03, time: 0.659, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2089, decode.acc_seg: 91.3960, loss: 0.2089
2023-11-11 07:22:11,926 - mmseg - INFO - Iter [105400/160000]	lr: 2.609e-05, eta: 10:38:28, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1802, decode.acc_seg: 92.6641, loss: 0.1802
2023-11-11 07:22:46,693 - mmseg - INFO - Iter [105450/160000]	lr: 2.604e-05, eta: 10:37:53, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2098, decode.acc_seg: 91.1579, loss: 0.2098
2023-11-11 07:23:22,469 - mmseg - INFO - Iter [105500/160000]	lr: 2.600e-05, eta: 10:37:18, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1940, decode.acc_seg: 91.8493, loss: 0.1940
2023-11-11 07:23:55,675 - mmseg - INFO - Iter [105550/160000]	lr: 2.596e-05, eta: 10:36:42, time: 0.665, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1960, decode.acc_seg: 92.1199, loss: 0.1960
2023-11-11 07:24:30,718 - mmseg - INFO - Iter [105600/160000]	lr: 2.591e-05, eta: 10:36:07, time: 0.700, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2051, decode.acc_seg: 91.7606, loss: 0.2051
2023-11-11 07:25:06,144 - mmseg - INFO - Iter [105650/160000]	lr: 2.587e-05, eta: 10:35:32, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1912, decode.acc_seg: 92.2791, loss: 0.1912
2023-11-11 07:25:41,447 - mmseg - INFO - Iter [105700/160000]	lr: 2.583e-05, eta: 10:34:57, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1888, decode.acc_seg: 91.9946, loss: 0.1888
2023-11-11 07:26:16,663 - mmseg - INFO - Iter [105750/160000]	lr: 2.578e-05, eta: 10:34:22, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1913, decode.acc_seg: 91.9616, loss: 0.1913
2023-11-11 07:26:50,683 - mmseg - INFO - Iter [105800/160000]	lr: 2.574e-05, eta: 10:33:46, time: 0.681, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2057, decode.acc_seg: 91.4627, loss: 0.2057
2023-11-11 07:27:25,630 - mmseg - INFO - Iter [105850/160000]	lr: 2.570e-05, eta: 10:33:11, time: 0.698, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1929, decode.acc_seg: 91.8964, loss: 0.1929
2023-11-11 07:28:00,292 - mmseg - INFO - Iter [105900/160000]	lr: 2.566e-05, eta: 10:32:36, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2036, decode.acc_seg: 91.7146, loss: 0.2036
2023-11-11 07:28:34,564 - mmseg - INFO - Iter [105950/160000]	lr: 2.561e-05, eta: 10:32:00, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1979, decode.acc_seg: 91.8767, loss: 0.1979
2023-11-11 07:29:05,918 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 07:29:05,918 - mmseg - INFO - Iter [106000/160000]	lr: 2.557e-05, eta: 10:31:23, time: 0.627, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2021, decode.acc_seg: 91.9326, loss: 0.2021
2023-11-11 07:29:38,837 - mmseg - INFO - Iter [106050/160000]	lr: 2.553e-05, eta: 10:30:47, time: 0.658, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2037, decode.acc_seg: 91.5541, loss: 0.2037
2023-11-11 07:30:14,298 - mmseg - INFO - Iter [106100/160000]	lr: 2.548e-05, eta: 10:30:12, time: 0.710, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2017, decode.acc_seg: 91.6049, loss: 0.2017
2023-11-11 07:30:49,317 - mmseg - INFO - Iter [106150/160000]	lr: 2.544e-05, eta: 10:29:37, time: 0.700, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2015, decode.acc_seg: 91.8574, loss: 0.2015
2023-11-11 07:31:22,618 - mmseg - INFO - Iter [106200/160000]	lr: 2.540e-05, eta: 10:29:01, time: 0.667, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2144, decode.acc_seg: 91.1117, loss: 0.2144
2023-11-11 07:31:56,749 - mmseg - INFO - Iter [106250/160000]	lr: 2.536e-05, eta: 10:28:26, time: 0.683, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1915, decode.acc_seg: 91.9249, loss: 0.1915
2023-11-11 07:32:31,580 - mmseg - INFO - Iter [106300/160000]	lr: 2.531e-05, eta: 10:27:50, time: 0.697, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2099, decode.acc_seg: 91.3491, loss: 0.2099
2023-11-11 07:33:06,217 - mmseg - INFO - Iter [106350/160000]	lr: 2.527e-05, eta: 10:27:15, time: 0.692, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2000, decode.acc_seg: 91.6639, loss: 0.2000
2023-11-11 07:33:41,086 - mmseg - INFO - Iter [106400/160000]	lr: 2.523e-05, eta: 10:26:40, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1855, decode.acc_seg: 92.1045, loss: 0.1855
2023-11-11 07:34:13,358 - mmseg - INFO - Iter [106450/160000]	lr: 2.519e-05, eta: 10:26:03, time: 0.646, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1933, decode.acc_seg: 92.0634, loss: 0.1933
2023-11-11 07:34:48,043 - mmseg - INFO - Iter [106500/160000]	lr: 2.514e-05, eta: 10:25:28, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1871, decode.acc_seg: 92.1372, loss: 0.1871
2023-11-11 07:35:23,198 - mmseg - INFO - Iter [106550/160000]	lr: 2.510e-05, eta: 10:24:53, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2064, decode.acc_seg: 91.6915, loss: 0.2064
2023-11-11 07:35:58,692 - mmseg - INFO - Iter [106600/160000]	lr: 2.506e-05, eta: 10:24:18, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1945, decode.acc_seg: 91.9665, loss: 0.1945
2023-11-11 07:36:33,907 - mmseg - INFO - Iter [106650/160000]	lr: 2.502e-05, eta: 10:23:43, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1986, decode.acc_seg: 91.7034, loss: 0.1986
2023-11-11 07:37:09,363 - mmseg - INFO - Iter [106700/160000]	lr: 2.497e-05, eta: 10:23:08, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1997, decode.acc_seg: 91.7077, loss: 0.1997
2023-11-11 07:37:45,042 - mmseg - INFO - Iter [106750/160000]	lr: 2.493e-05, eta: 10:22:34, time: 0.713, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1971, decode.acc_seg: 91.8793, loss: 0.1971
2023-11-11 07:38:20,534 - mmseg - INFO - Iter [106800/160000]	lr: 2.489e-05, eta: 10:21:59, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2065, decode.acc_seg: 91.4979, loss: 0.2065
2023-11-11 07:38:53,351 - mmseg - INFO - Iter [106850/160000]	lr: 2.485e-05, eta: 10:21:23, time: 0.657, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1927, decode.acc_seg: 91.8761, loss: 0.1927
2023-11-11 07:39:26,997 - mmseg - INFO - Iter [106900/160000]	lr: 2.480e-05, eta: 10:20:47, time: 0.673, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1991, decode.acc_seg: 91.9072, loss: 0.1991
2023-11-11 07:39:59,890 - mmseg - INFO - Iter [106950/160000]	lr: 2.476e-05, eta: 10:20:11, time: 0.656, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2066, decode.acc_seg: 91.3964, loss: 0.2066
2023-11-11 07:40:35,199 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 07:40:35,199 - mmseg - INFO - Iter [107000/160000]	lr: 2.472e-05, eta: 10:19:36, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2091, decode.acc_seg: 91.5181, loss: 0.2091
2023-11-11 07:41:10,157 - mmseg - INFO - Iter [107050/160000]	lr: 2.468e-05, eta: 10:19:01, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1890, decode.acc_seg: 92.1698, loss: 0.1890
2023-11-11 07:41:46,080 - mmseg - INFO - Iter [107100/160000]	lr: 2.463e-05, eta: 10:18:26, time: 0.718, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1984, decode.acc_seg: 91.7183, loss: 0.1984
2023-11-11 07:42:21,622 - mmseg - INFO - Iter [107150/160000]	lr: 2.459e-05, eta: 10:17:51, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1895, decode.acc_seg: 92.0432, loss: 0.1895
2023-11-11 07:42:57,272 - mmseg - INFO - Iter [107200/160000]	lr: 2.455e-05, eta: 10:17:16, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2023, decode.acc_seg: 91.6699, loss: 0.2023
2023-11-11 07:43:32,692 - mmseg - INFO - Iter [107250/160000]	lr: 2.451e-05, eta: 10:16:41, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1883, decode.acc_seg: 92.4768, loss: 0.1883
2023-11-11 07:44:04,968 - mmseg - INFO - Iter [107300/160000]	lr: 2.446e-05, eta: 10:16:05, time: 0.645, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1886, decode.acc_seg: 91.9758, loss: 0.1886
2023-11-11 07:44:40,514 - mmseg - INFO - Iter [107350/160000]	lr: 2.442e-05, eta: 10:15:30, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1901, decode.acc_seg: 92.0754, loss: 0.1901
2023-11-11 07:45:14,444 - mmseg - INFO - Iter [107400/160000]	lr: 2.438e-05, eta: 10:14:54, time: 0.680, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1948, decode.acc_seg: 91.7464, loss: 0.1948
2023-11-11 07:45:47,730 - mmseg - INFO - Iter [107450/160000]	lr: 2.434e-05, eta: 10:14:18, time: 0.665, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1919, decode.acc_seg: 92.0842, loss: 0.1919
2023-11-11 07:46:21,766 - mmseg - INFO - Iter [107500/160000]	lr: 2.430e-05, eta: 10:13:43, time: 0.682, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1993, decode.acc_seg: 91.5646, loss: 0.1993
2023-11-11 07:46:56,561 - mmseg - INFO - Iter [107550/160000]	lr: 2.425e-05, eta: 10:13:08, time: 0.695, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1886, decode.acc_seg: 92.2201, loss: 0.1886
2023-11-11 07:47:31,553 - mmseg - INFO - Iter [107600/160000]	lr: 2.421e-05, eta: 10:12:33, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2038, decode.acc_seg: 91.5398, loss: 0.2038
2023-11-11 07:48:04,318 - mmseg - INFO - Iter [107650/160000]	lr: 2.417e-05, eta: 10:11:56, time: 0.655, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1943, decode.acc_seg: 92.0271, loss: 0.1943
2023-11-11 07:48:39,765 - mmseg - INFO - Iter [107700/160000]	lr: 2.413e-05, eta: 10:11:22, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1921, decode.acc_seg: 92.0840, loss: 0.1921
2023-11-11 07:49:14,694 - mmseg - INFO - Iter [107750/160000]	lr: 2.409e-05, eta: 10:10:46, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1888, decode.acc_seg: 92.2495, loss: 0.1888
2023-11-11 07:49:47,724 - mmseg - INFO - Iter [107800/160000]	lr: 2.404e-05, eta: 10:10:10, time: 0.661, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1927, decode.acc_seg: 91.9989, loss: 0.1927
2023-11-11 07:50:20,259 - mmseg - INFO - Iter [107850/160000]	lr: 2.400e-05, eta: 10:09:34, time: 0.650, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1936, decode.acc_seg: 91.8877, loss: 0.1936
2023-11-11 07:50:55,338 - mmseg - INFO - Iter [107900/160000]	lr: 2.396e-05, eta: 10:08:59, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1960, decode.acc_seg: 91.6691, loss: 0.1960
2023-11-11 07:51:30,384 - mmseg - INFO - Iter [107950/160000]	lr: 2.392e-05, eta: 10:08:24, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1989, decode.acc_seg: 91.7930, loss: 0.1989
2023-11-11 07:52:03,414 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 07:52:03,415 - mmseg - INFO - Iter [108000/160000]	lr: 2.388e-05, eta: 10:07:48, time: 0.661, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1881, decode.acc_seg: 92.0986, loss: 0.1881
2023-11-11 07:52:39,362 - mmseg - INFO - Iter [108050/160000]	lr: 2.383e-05, eta: 10:07:13, time: 0.719, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1897, decode.acc_seg: 92.1151, loss: 0.1897
2023-11-11 07:53:14,205 - mmseg - INFO - Iter [108100/160000]	lr: 2.379e-05, eta: 10:06:38, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1837, decode.acc_seg: 92.2755, loss: 0.1837
2023-11-11 07:53:46,874 - mmseg - INFO - Iter [108150/160000]	lr: 2.375e-05, eta: 10:06:02, time: 0.652, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1943, decode.acc_seg: 91.9271, loss: 0.1943
2023-11-11 07:54:21,799 - mmseg - INFO - Iter [108200/160000]	lr: 2.371e-05, eta: 10:05:27, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1789, decode.acc_seg: 92.5311, loss: 0.1789
2023-11-11 07:54:57,753 - mmseg - INFO - Iter [108250/160000]	lr: 2.367e-05, eta: 10:04:52, time: 0.719, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2007, decode.acc_seg: 91.9095, loss: 0.2007
2023-11-11 07:55:34,274 - mmseg - INFO - Iter [108300/160000]	lr: 2.363e-05, eta: 10:04:18, time: 0.730, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1922, decode.acc_seg: 91.8802, loss: 0.1922
2023-11-11 07:56:10,673 - mmseg - INFO - Iter [108350/160000]	lr: 2.358e-05, eta: 10:03:43, time: 0.728, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1916, decode.acc_seg: 91.9793, loss: 0.1916
2023-11-11 07:56:44,964 - mmseg - INFO - Iter [108400/160000]	lr: 2.354e-05, eta: 10:03:08, time: 0.687, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2035, decode.acc_seg: 91.5577, loss: 0.2035
2023-11-11 07:57:19,417 - mmseg - INFO - Iter [108450/160000]	lr: 2.350e-05, eta: 10:02:32, time: 0.689, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2040, decode.acc_seg: 91.4844, loss: 0.2040
2023-11-11 07:57:52,958 - mmseg - INFO - Iter [108500/160000]	lr: 2.346e-05, eta: 10:01:57, time: 0.670, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1966, decode.acc_seg: 92.0594, loss: 0.1966
2023-11-11 07:58:27,620 - mmseg - INFO - Iter [108550/160000]	lr: 2.342e-05, eta: 10:01:21, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1856, decode.acc_seg: 92.2524, loss: 0.1856
2023-11-11 07:59:01,028 - mmseg - INFO - Iter [108600/160000]	lr: 2.338e-05, eta: 10:00:46, time: 0.667, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2000, decode.acc_seg: 91.6114, loss: 0.2000
2023-11-11 07:59:35,973 - mmseg - INFO - Iter [108650/160000]	lr: 2.333e-05, eta: 10:00:10, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1802, decode.acc_seg: 92.4532, loss: 0.1802
2023-11-11 08:00:10,959 - mmseg - INFO - Iter [108700/160000]	lr: 2.329e-05, eta: 9:59:35, time: 0.700, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1811, decode.acc_seg: 92.3513, loss: 0.1811
2023-11-11 08:00:43,980 - mmseg - INFO - Iter [108750/160000]	lr: 2.325e-05, eta: 9:58:59, time: 0.662, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1961, decode.acc_seg: 91.9320, loss: 0.1961
2023-11-11 08:01:17,978 - mmseg - INFO - Iter [108800/160000]	lr: 2.321e-05, eta: 9:58:24, time: 0.679, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1887, decode.acc_seg: 92.0352, loss: 0.1887
2023-11-11 08:01:52,515 - mmseg - INFO - Iter [108850/160000]	lr: 2.317e-05, eta: 9:57:48, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1970, decode.acc_seg: 91.9120, loss: 0.1970
2023-11-11 08:02:25,566 - mmseg - INFO - Iter [108900/160000]	lr: 2.313e-05, eta: 9:57:12, time: 0.661, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1999, decode.acc_seg: 91.8322, loss: 0.1999
2023-11-11 08:02:59,890 - mmseg - INFO - Iter [108950/160000]	lr: 2.309e-05, eta: 9:56:37, time: 0.685, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1977, decode.acc_seg: 91.4235, loss: 0.1977
2023-11-11 08:03:32,841 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 08:03:32,842 - mmseg - INFO - Iter [109000/160000]	lr: 2.304e-05, eta: 9:56:01, time: 0.660, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1982, decode.acc_seg: 91.7682, loss: 0.1982
2023-11-11 08:04:05,300 - mmseg - INFO - Iter [109050/160000]	lr: 2.300e-05, eta: 9:55:25, time: 0.649, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1996, decode.acc_seg: 92.0112, loss: 0.1996
2023-11-11 08:04:38,967 - mmseg - INFO - Iter [109100/160000]	lr: 2.296e-05, eta: 9:54:49, time: 0.673, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1761, decode.acc_seg: 92.3520, loss: 0.1761
2023-11-11 08:05:10,611 - mmseg - INFO - Iter [109150/160000]	lr: 2.292e-05, eta: 9:54:12, time: 0.633, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1960, decode.acc_seg: 91.9403, loss: 0.1960
2023-11-11 08:05:45,354 - mmseg - INFO - Iter [109200/160000]	lr: 2.288e-05, eta: 9:53:37, time: 0.694, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1914, decode.acc_seg: 92.3832, loss: 0.1914
2023-11-11 08:06:17,596 - mmseg - INFO - Iter [109250/160000]	lr: 2.284e-05, eta: 9:53:01, time: 0.646, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1925, decode.acc_seg: 91.8998, loss: 0.1925
2023-11-11 08:06:49,437 - mmseg - INFO - Iter [109300/160000]	lr: 2.280e-05, eta: 9:52:24, time: 0.637, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1853, decode.acc_seg: 92.1262, loss: 0.1853
2023-11-11 08:07:22,519 - mmseg - INFO - Iter [109350/160000]	lr: 2.276e-05, eta: 9:51:48, time: 0.661, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1851, decode.acc_seg: 91.9742, loss: 0.1851
2023-11-11 08:07:54,311 - mmseg - INFO - Iter [109400/160000]	lr: 2.271e-05, eta: 9:51:12, time: 0.636, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1883, decode.acc_seg: 92.0467, loss: 0.1883
2023-11-11 08:08:28,048 - mmseg - INFO - Iter [109450/160000]	lr: 2.267e-05, eta: 9:50:36, time: 0.674, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1960, decode.acc_seg: 91.7085, loss: 0.1960
2023-11-11 08:09:03,226 - mmseg - INFO - Iter [109500/160000]	lr: 2.263e-05, eta: 9:50:01, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1851, decode.acc_seg: 92.2140, loss: 0.1851
2023-11-11 08:09:38,282 - mmseg - INFO - Iter [109550/160000]	lr: 2.259e-05, eta: 9:49:26, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2027, decode.acc_seg: 91.5588, loss: 0.2027
2023-11-11 08:10:13,362 - mmseg - INFO - Iter [109600/160000]	lr: 2.255e-05, eta: 9:48:51, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1870, decode.acc_seg: 92.1754, loss: 0.1870
2023-11-11 08:10:46,771 - mmseg - INFO - Iter [109650/160000]	lr: 2.251e-05, eta: 9:48:15, time: 0.668, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1933, decode.acc_seg: 92.3505, loss: 0.1933
2023-11-11 08:11:22,238 - mmseg - INFO - Iter [109700/160000]	lr: 2.247e-05, eta: 9:47:40, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1927, decode.acc_seg: 92.0765, loss: 0.1927
2023-11-11 08:11:57,231 - mmseg - INFO - Iter [109750/160000]	lr: 2.243e-05, eta: 9:47:05, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1863, decode.acc_seg: 92.0999, loss: 0.1863
2023-11-11 08:12:33,814 - mmseg - INFO - Iter [109800/160000]	lr: 2.239e-05, eta: 9:46:31, time: 0.732, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1982, decode.acc_seg: 91.8283, loss: 0.1982
2023-11-11 08:13:10,006 - mmseg - INFO - Iter [109850/160000]	lr: 2.234e-05, eta: 9:45:56, time: 0.724, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1914, decode.acc_seg: 92.1223, loss: 0.1914
2023-11-11 08:13:45,526 - mmseg - INFO - Iter [109900/160000]	lr: 2.230e-05, eta: 9:45:21, time: 0.710, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1832, decode.acc_seg: 92.3615, loss: 0.1832
2023-11-11 08:14:20,912 - mmseg - INFO - Iter [109950/160000]	lr: 2.226e-05, eta: 9:44:47, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1819, decode.acc_seg: 92.2426, loss: 0.1819
2023-11-11 08:14:53,562 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 08:14:53,563 - mmseg - INFO - Iter [110000/160000]	lr: 2.222e-05, eta: 9:44:10, time: 0.653, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1955, decode.acc_seg: 91.8535, loss: 0.1955
2023-11-11 08:15:27,713 - mmseg - INFO - Iter [110050/160000]	lr: 2.218e-05, eta: 9:43:35, time: 0.683, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2102, decode.acc_seg: 91.6500, loss: 0.2102
2023-11-11 08:16:02,038 - mmseg - INFO - Iter [110100/160000]	lr: 2.214e-05, eta: 9:43:00, time: 0.688, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1940, decode.acc_seg: 92.1301, loss: 0.1940
2023-11-11 08:16:33,226 - mmseg - INFO - Iter [110150/160000]	lr: 2.210e-05, eta: 9:42:23, time: 0.624, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2010, decode.acc_seg: 91.6066, loss: 0.2010
2023-11-11 08:17:07,371 - mmseg - INFO - Iter [110200/160000]	lr: 2.206e-05, eta: 9:41:47, time: 0.682, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1863, decode.acc_seg: 92.1756, loss: 0.1863
2023-11-11 08:17:42,395 - mmseg - INFO - Iter [110250/160000]	lr: 2.202e-05, eta: 9:41:12, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1896, decode.acc_seg: 92.1884, loss: 0.1896
2023-11-11 08:18:16,769 - mmseg - INFO - Iter [110300/160000]	lr: 2.198e-05, eta: 9:40:37, time: 0.688, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1916, decode.acc_seg: 92.1388, loss: 0.1916
2023-11-11 08:18:50,041 - mmseg - INFO - Iter [110350/160000]	lr: 2.194e-05, eta: 9:40:01, time: 0.664, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1816, decode.acc_seg: 92.1978, loss: 0.1816
2023-11-11 08:19:24,822 - mmseg - INFO - Iter [110400/160000]	lr: 2.190e-05, eta: 9:39:26, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1817, decode.acc_seg: 92.3728, loss: 0.1817
2023-11-11 08:20:00,069 - mmseg - INFO - Iter [110450/160000]	lr: 2.186e-05, eta: 9:38:51, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1932, decode.acc_seg: 91.9373, loss: 0.1932
2023-11-11 08:20:35,735 - mmseg - INFO - Iter [110500/160000]	lr: 2.182e-05, eta: 9:38:16, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1921, decode.acc_seg: 92.1202, loss: 0.1921
2023-11-11 08:21:11,081 - mmseg - INFO - Iter [110550/160000]	lr: 2.177e-05, eta: 9:37:41, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1831, decode.acc_seg: 92.2557, loss: 0.1831
2023-11-11 08:21:46,476 - mmseg - INFO - Iter [110600/160000]	lr: 2.173e-05, eta: 9:37:06, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1945, decode.acc_seg: 91.8569, loss: 0.1945
2023-11-11 08:22:22,046 - mmseg - INFO - Iter [110650/160000]	lr: 2.169e-05, eta: 9:36:32, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1875, decode.acc_seg: 92.2334, loss: 0.1875
2023-11-11 08:22:57,288 - mmseg - INFO - Iter [110700/160000]	lr: 2.165e-05, eta: 9:35:57, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1836, decode.acc_seg: 92.4106, loss: 0.1836
2023-11-11 08:23:30,787 - mmseg - INFO - Iter [110750/160000]	lr: 2.161e-05, eta: 9:35:21, time: 0.670, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2007, decode.acc_seg: 91.6976, loss: 0.2007
2023-11-11 08:24:06,365 - mmseg - INFO - Iter [110800/160000]	lr: 2.157e-05, eta: 9:34:46, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2067, decode.acc_seg: 91.4526, loss: 0.2067
2023-11-11 08:24:41,917 - mmseg - INFO - Iter [110850/160000]	lr: 2.153e-05, eta: 9:34:11, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1903, decode.acc_seg: 91.9863, loss: 0.1903
2023-11-11 08:25:17,065 - mmseg - INFO - Iter [110900/160000]	lr: 2.149e-05, eta: 9:33:36, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1848, decode.acc_seg: 92.0467, loss: 0.1848
2023-11-11 08:25:52,053 - mmseg - INFO - Iter [110950/160000]	lr: 2.145e-05, eta: 9:33:01, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1967, decode.acc_seg: 92.0053, loss: 0.1967
2023-11-11 08:26:27,267 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 08:26:27,267 - mmseg - INFO - Iter [111000/160000]	lr: 2.141e-05, eta: 9:32:26, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1850, decode.acc_seg: 92.1741, loss: 0.1850
2023-11-11 08:27:01,439 - mmseg - INFO - Iter [111050/160000]	lr: 2.137e-05, eta: 9:31:51, time: 0.683, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1800, decode.acc_seg: 92.4143, loss: 0.1800
2023-11-11 08:27:37,188 - mmseg - INFO - Iter [111100/160000]	lr: 2.133e-05, eta: 9:31:16, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1929, decode.acc_seg: 91.9487, loss: 0.1929
2023-11-11 08:28:12,910 - mmseg - INFO - Iter [111150/160000]	lr: 2.129e-05, eta: 9:30:41, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1930, decode.acc_seg: 91.9448, loss: 0.1930
2023-11-11 08:28:48,340 - mmseg - INFO - Iter [111200/160000]	lr: 2.125e-05, eta: 9:30:06, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1992, decode.acc_seg: 91.7270, loss: 0.1992
2023-11-11 08:29:21,382 - mmseg - INFO - Iter [111250/160000]	lr: 2.121e-05, eta: 9:29:30, time: 0.661, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1868, decode.acc_seg: 92.0753, loss: 0.1868
2023-11-11 08:29:54,353 - mmseg - INFO - Iter [111300/160000]	lr: 2.117e-05, eta: 9:28:55, time: 0.660, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1829, decode.acc_seg: 92.4962, loss: 0.1829
2023-11-11 08:30:26,099 - mmseg - INFO - Iter [111350/160000]	lr: 2.113e-05, eta: 9:28:18, time: 0.635, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1878, decode.acc_seg: 92.0993, loss: 0.1878
2023-11-11 08:30:58,482 - mmseg - INFO - Iter [111400/160000]	lr: 2.109e-05, eta: 9:27:42, time: 0.647, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1804, decode.acc_seg: 92.3837, loss: 0.1804
2023-11-11 08:31:33,750 - mmseg - INFO - Iter [111450/160000]	lr: 2.105e-05, eta: 9:27:07, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1797, decode.acc_seg: 92.5031, loss: 0.1797
2023-11-11 08:32:06,931 - mmseg - INFO - Iter [111500/160000]	lr: 2.101e-05, eta: 9:26:31, time: 0.664, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1853, decode.acc_seg: 92.2837, loss: 0.1853
2023-11-11 08:32:41,033 - mmseg - INFO - Iter [111550/160000]	lr: 2.097e-05, eta: 9:25:56, time: 0.681, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1868, decode.acc_seg: 92.2228, loss: 0.1868
2023-11-11 08:33:16,596 - mmseg - INFO - Iter [111600/160000]	lr: 2.093e-05, eta: 9:25:21, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1857, decode.acc_seg: 92.3670, loss: 0.1857
2023-11-11 08:33:51,289 - mmseg - INFO - Iter [111650/160000]	lr: 2.089e-05, eta: 9:24:46, time: 0.693, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1832, decode.acc_seg: 92.4970, loss: 0.1832
2023-11-11 08:34:26,446 - mmseg - INFO - Iter [111700/160000]	lr: 2.085e-05, eta: 9:24:11, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1810, decode.acc_seg: 92.4396, loss: 0.1810
2023-11-11 08:35:01,585 - mmseg - INFO - Iter [111750/160000]	lr: 2.081e-05, eta: 9:23:36, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1981, decode.acc_seg: 91.7314, loss: 0.1981
2023-11-11 08:35:36,740 - mmseg - INFO - Iter [111800/160000]	lr: 2.077e-05, eta: 9:23:01, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1920, decode.acc_seg: 91.9752, loss: 0.1920
2023-11-11 08:36:12,072 - mmseg - INFO - Iter [111850/160000]	lr: 2.073e-05, eta: 9:22:26, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1972, decode.acc_seg: 91.5773, loss: 0.1972
2023-11-11 08:36:47,292 - mmseg - INFO - Iter [111900/160000]	lr: 2.069e-05, eta: 9:21:51, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1813, decode.acc_seg: 92.2485, loss: 0.1813
2023-11-11 08:37:20,124 - mmseg - INFO - Iter [111950/160000]	lr: 2.065e-05, eta: 9:21:15, time: 0.657, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1773, decode.acc_seg: 92.3993, loss: 0.1773
2023-11-11 08:37:52,796 - mmseg - INFO - Saving checkpoint at 112000 iterations
2023-11-11 08:37:57,216 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 08:37:57,217 - mmseg - INFO - Iter [112000/160000]	lr: 2.061e-05, eta: 9:20:41, time: 0.742, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1899, decode.acc_seg: 92.3414, loss: 0.1899
2023-11-11 08:39:26,797 - mmseg - INFO - per class results:
2023-11-11 08:39:26,811 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 77.71 | 87.85 |
|       building      | 81.96 | 91.39 |
|         sky         | 94.44 | 97.34 |
|        floor        |  82.5 | 90.38 |
|         tree        |  74.9 | 88.49 |
|       ceiling       | 83.36 | 91.94 |
|         road        | 81.77 | 89.73 |
|         bed         | 89.13 | 96.71 |
|      windowpane     | 61.11 | 78.51 |
|        grass        | 66.86 | 82.23 |
|       cabinet       | 60.13 | 74.92 |
|       sidewalk      |  65.0 | 81.65 |
|        person       |  80.9 | 93.48 |
|        earth        | 39.18 | 53.02 |
|         door        | 51.23 | 62.31 |
|        table        | 60.04 |  76.9 |
|       mountain      |  55.9 |  73.4 |
|        plant        | 54.12 | 63.85 |
|       curtain       | 73.88 | 86.59 |
|        chair        | 57.82 | 72.08 |
|         car         | 82.71 | 92.31 |
|        water        | 59.27 | 81.38 |
|       painting      | 70.75 | 86.66 |
|         sofa        | 67.95 | 87.74 |
|        shelf        | 42.59 | 60.44 |
|        house        | 51.68 | 74.86 |
|         sea         | 56.45 | 69.74 |
|        mirror       | 66.56 | 75.03 |
|         rug         | 62.62 | 73.45 |
|        field        | 28.36 | 41.97 |
|       armchair      | 41.61 | 53.87 |
|         seat        | 64.49 | 84.35 |
|        fence        | 31.56 | 41.65 |
|         desk        | 49.67 | 69.98 |
|         rock        | 38.07 | 59.53 |
|       wardrobe      | 48.51 | 70.56 |
|         lamp        | 64.91 | 76.31 |
|       bathtub       | 81.11 | 86.53 |
|       railing       |  31.8 | 47.72 |
|       cushion       | 59.66 | 72.95 |
|         base        |  32.7 | 42.57 |
|         box         | 31.07 | 42.11 |
|        column       | 44.51 |  56.4 |
|      signboard      | 39.92 | 52.32 |
|   chest of drawers  | 46.24 | 60.14 |
|       counter       | 34.83 | 39.04 |
|         sand        | 50.05 | 72.25 |
|         sink        | 70.76 |  80.5 |
|      skyscraper     | 46.71 | 58.27 |
|      fireplace      | 71.82 | 90.43 |
|     refrigerator    | 76.72 | 84.57 |
|      grandstand     |  45.4 |  66.3 |
|         path        | 21.89 | 31.76 |
|        stairs       |  30.6 | 40.71 |
|        runway       | 61.14 | 78.42 |
|         case        | 41.62 | 50.57 |
|      pool table     | 92.74 | 97.02 |
|        pillow       | 58.51 | 66.49 |
|     screen door     | 59.65 | 62.98 |
|       stairway      | 31.98 | 38.49 |
|        river        | 12.19 | 21.74 |
|        bridge       | 70.67 | 82.31 |
|       bookcase      | 37.81 | 56.18 |
|        blind        | 47.95 | 59.14 |
|     coffee table    | 54.83 | 82.12 |
|        toilet       | 86.52 | 92.42 |
|        flower       | 39.66 | 52.56 |
|         book        | 47.19 | 65.95 |
|         hill        | 14.26 | 18.39 |
|        bench        | 41.98 | 49.97 |
|      countertop     | 63.63 | 83.17 |
|        stove        |  70.6 | 78.67 |
|         palm        |  47.2 | 70.83 |
|    kitchen island   | 42.58 | 76.39 |
|       computer      | 68.49 | 79.48 |
|     swivel chair    | 47.01 | 68.85 |
|         boat        | 36.62 | 55.19 |
|         bar         | 41.71 | 56.21 |
|    arcade machine   | 76.93 |  83.4 |
|        hovel        | 45.36 | 50.89 |
|         bus         |  84.5 | 96.29 |
|        towel        | 66.85 | 75.14 |
|        light        | 54.94 | 66.02 |
|        truck        | 30.52 | 41.55 |
|        tower        |  5.77 |  9.0  |
|      chandelier     | 67.18 | 82.21 |
|        awning       | 26.09 | 35.75 |
|     streetlight     | 27.66 | 36.15 |
|        booth        | 54.13 | 55.61 |
| television receiver | 65.77 | 80.67 |
|       airplane      | 54.17 | 66.12 |
|      dirt track     |  0.15 |  0.17 |
|       apparel       | 44.07 | 62.11 |
|         pole        | 25.41 | 34.24 |
|         land        |  0.01 |  0.01 |
|      bannister      | 15.77 | 19.29 |
|      escalator      | 46.57 | 60.11 |
|       ottoman       | 44.71 | 57.97 |
|        bottle       | 35.35 | 59.46 |
|        buffet       | 45.35 | 50.63 |
|        poster       | 31.83 | 43.96 |
|        stage        |  19.2 |  28.1 |
|         van         | 40.37 | 56.44 |
|         ship        | 49.21 | 70.81 |
|       fountain      | 37.72 | 38.11 |
|    conveyer belt    | 79.66 | 94.57 |
|        canopy       | 37.16 | 54.72 |
|        washer       | 75.02 | 75.87 |
|      plaything      | 18.88 | 32.37 |
|    swimming pool    | 76.79 | 85.28 |
|        stool        | 42.09 | 54.43 |
|        barrel       | 59.65 | 71.46 |
|        basket       |  32.5 | 44.48 |
|      waterfall      | 61.95 | 91.16 |
|         tent        | 94.52 | 97.79 |
|         bag         | 19.58 | 25.25 |
|       minibike      | 68.88 | 86.95 |
|        cradle       | 79.98 | 97.79 |
|         oven        | 53.67 | 64.57 |
|         ball        | 37.19 | 48.41 |
|         food        | 48.19 | 52.09 |
|         step        | 16.17 | 21.22 |
|         tank        | 38.44 | 44.11 |
|      trade name     | 22.19 | 25.16 |
|      microwave      | 83.05 | 92.93 |
|         pot         | 41.68 | 47.47 |
|        animal       | 51.07 | 53.78 |
|       bicycle       | 57.65 | 80.65 |
|         lake        | 62.79 | 63.34 |
|      dishwasher     | 61.15 | 72.66 |
|        screen       | 65.79 | 91.31 |
|       blanket       |  14.0 | 15.75 |
|      sculpture      | 60.94 | 79.89 |
|         hood        | 59.56 | 72.84 |
|        sconce       | 45.59 |  53.4 |
|         vase        | 39.31 | 50.99 |
|    traffic light    | 27.48 | 43.03 |
|         tray        |  6.79 | 15.72 |
|        ashcan       | 43.38 | 56.09 |
|         fan         | 58.31 | 77.51 |
|         pier        | 14.14 | 16.48 |
|      crt screen     |  1.94 |  6.49 |
|        plate        | 54.16 | 68.54 |
|       monitor       |  3.3  |  3.52 |
|    bulletin board   | 55.03 | 65.18 |
|        shower       |  1.09 |  6.61 |
|       radiator      | 64.05 |  71.3 |
|        glass        | 12.95 |  13.8 |
|        clock        | 33.04 | 42.08 |
|         flag        | 44.57 | 48.08 |
+---------------------+-------+-------+
2023-11-11 08:39:26,811 - mmseg - INFO - Summary:
2023-11-11 08:39:26,811 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 83.16 | 49.82 | 61.59 |
+-------+-------+-------+
2023-11-11 08:39:26,941 - mmseg - INFO - The previous best checkpoint /data/Next-ViT/segmentation/work_dirs/fpn_512_debi_base_160k/best_mIoU_iter_104000.pth was removed
2023-11-11 08:39:29,814 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_112000.pth.
2023-11-11 08:39:29,814 - mmseg - INFO - Best mIoU is 0.4982 at 112000 iter.
2023-11-11 08:39:29,829 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 08:39:29,830 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8316, mIoU: 0.4982, mAcc: 0.6159, IoU.wall: 0.7771, IoU.building: 0.8196, IoU.sky: 0.9444, IoU.floor: 0.8250, IoU.tree: 0.7490, IoU.ceiling: 0.8336, IoU.road: 0.8177, IoU.bed : 0.8913, IoU.windowpane: 0.6111, IoU.grass: 0.6686, IoU.cabinet: 0.6013, IoU.sidewalk: 0.6500, IoU.person: 0.8090, IoU.earth: 0.3918, IoU.door: 0.5123, IoU.table: 0.6004, IoU.mountain: 0.5590, IoU.plant: 0.5412, IoU.curtain: 0.7388, IoU.chair: 0.5782, IoU.car: 0.8271, IoU.water: 0.5927, IoU.painting: 0.7075, IoU.sofa: 0.6795, IoU.shelf: 0.4259, IoU.house: 0.5168, IoU.sea: 0.5645, IoU.mirror: 0.6656, IoU.rug: 0.6262, IoU.field: 0.2836, IoU.armchair: 0.4161, IoU.seat: 0.6449, IoU.fence: 0.3156, IoU.desk: 0.4967, IoU.rock: 0.3807, IoU.wardrobe: 0.4851, IoU.lamp: 0.6491, IoU.bathtub: 0.8111, IoU.railing: 0.3180, IoU.cushion: 0.5966, IoU.base: 0.3270, IoU.box: 0.3107, IoU.column: 0.4451, IoU.signboard: 0.3992, IoU.chest of drawers: 0.4624, IoU.counter: 0.3483, IoU.sand: 0.5005, IoU.sink: 0.7076, IoU.skyscraper: 0.4671, IoU.fireplace: 0.7182, IoU.refrigerator: 0.7672, IoU.grandstand: 0.4540, IoU.path: 0.2189, IoU.stairs: 0.3060, IoU.runway: 0.6114, IoU.case: 0.4162, IoU.pool table: 0.9274, IoU.pillow: 0.5851, IoU.screen door: 0.5965, IoU.stairway: 0.3198, IoU.river: 0.1219, IoU.bridge: 0.7067, IoU.bookcase: 0.3781, IoU.blind: 0.4795, IoU.coffee table: 0.5483, IoU.toilet: 0.8652, IoU.flower: 0.3966, IoU.book: 0.4719, IoU.hill: 0.1426, IoU.bench: 0.4198, IoU.countertop: 0.6363, IoU.stove: 0.7060, IoU.palm: 0.4720, IoU.kitchen island: 0.4258, IoU.computer: 0.6849, IoU.swivel chair: 0.4701, IoU.boat: 0.3662, IoU.bar: 0.4171, IoU.arcade machine: 0.7693, IoU.hovel: 0.4536, IoU.bus: 0.8450, IoU.towel: 0.6685, IoU.light: 0.5494, IoU.truck: 0.3052, IoU.tower: 0.0577, IoU.chandelier: 0.6718, IoU.awning: 0.2609, IoU.streetlight: 0.2766, IoU.booth: 0.5413, IoU.television receiver: 0.6577, IoU.airplane: 0.5417, IoU.dirt track: 0.0015, IoU.apparel: 0.4407, IoU.pole: 0.2541, IoU.land: 0.0001, IoU.bannister: 0.1577, IoU.escalator: 0.4657, IoU.ottoman: 0.4471, IoU.bottle: 0.3535, IoU.buffet: 0.4535, IoU.poster: 0.3183, IoU.stage: 0.1920, IoU.van: 0.4037, IoU.ship: 0.4921, IoU.fountain: 0.3772, IoU.conveyer belt: 0.7966, IoU.canopy: 0.3716, IoU.washer: 0.7502, IoU.plaything: 0.1888, IoU.swimming pool: 0.7679, IoU.stool: 0.4209, IoU.barrel: 0.5965, IoU.basket: 0.3250, IoU.waterfall: 0.6195, IoU.tent: 0.9452, IoU.bag: 0.1958, IoU.minibike: 0.6888, IoU.cradle: 0.7998, IoU.oven: 0.5367, IoU.ball: 0.3719, IoU.food: 0.4819, IoU.step: 0.1617, IoU.tank: 0.3844, IoU.trade name: 0.2219, IoU.microwave: 0.8305, IoU.pot: 0.4168, IoU.animal: 0.5107, IoU.bicycle: 0.5765, IoU.lake: 0.6279, IoU.dishwasher: 0.6115, IoU.screen: 0.6579, IoU.blanket: 0.1400, IoU.sculpture: 0.6094, IoU.hood: 0.5956, IoU.sconce: 0.4559, IoU.vase: 0.3931, IoU.traffic light: 0.2748, IoU.tray: 0.0679, IoU.ashcan: 0.4338, IoU.fan: 0.5831, IoU.pier: 0.1414, IoU.crt screen: 0.0194, IoU.plate: 0.5416, IoU.monitor: 0.0330, IoU.bulletin board: 0.5503, IoU.shower: 0.0109, IoU.radiator: 0.6405, IoU.glass: 0.1295, IoU.clock: 0.3304, IoU.flag: 0.4457, Acc.wall: 0.8785, Acc.building: 0.9139, Acc.sky: 0.9734, Acc.floor: 0.9038, Acc.tree: 0.8849, Acc.ceiling: 0.9194, Acc.road: 0.8973, Acc.bed : 0.9671, Acc.windowpane: 0.7851, Acc.grass: 0.8223, Acc.cabinet: 0.7492, Acc.sidewalk: 0.8165, Acc.person: 0.9348, Acc.earth: 0.5302, Acc.door: 0.6231, Acc.table: 0.7690, Acc.mountain: 0.7340, Acc.plant: 0.6385, Acc.curtain: 0.8659, Acc.chair: 0.7208, Acc.car: 0.9231, Acc.water: 0.8138, Acc.painting: 0.8666, Acc.sofa: 0.8774, Acc.shelf: 0.6044, Acc.house: 0.7486, Acc.sea: 0.6974, Acc.mirror: 0.7503, Acc.rug: 0.7345, Acc.field: 0.4197, Acc.armchair: 0.5387, Acc.seat: 0.8435, Acc.fence: 0.4165, Acc.desk: 0.6998, Acc.rock: 0.5953, Acc.wardrobe: 0.7056, Acc.lamp: 0.7631, Acc.bathtub: 0.8653, Acc.railing: 0.4772, Acc.cushion: 0.7295, Acc.base: 0.4257, Acc.box: 0.4211, Acc.column: 0.5640, Acc.signboard: 0.5232, Acc.chest of drawers: 0.6014, Acc.counter: 0.3904, Acc.sand: 0.7225, Acc.sink: 0.8050, Acc.skyscraper: 0.5827, Acc.fireplace: 0.9043, Acc.refrigerator: 0.8457, Acc.grandstand: 0.6630, Acc.path: 0.3176, Acc.stairs: 0.4071, Acc.runway: 0.7842, Acc.case: 0.5057, Acc.pool table: 0.9702, Acc.pillow: 0.6649, Acc.screen door: 0.6298, Acc.stairway: 0.3849, Acc.river: 0.2174, Acc.bridge: 0.8231, Acc.bookcase: 0.5618, Acc.blind: 0.5914, Acc.coffee table: 0.8212, Acc.toilet: 0.9242, Acc.flower: 0.5256, Acc.book: 0.6595, Acc.hill: 0.1839, Acc.bench: 0.4997, Acc.countertop: 0.8317, Acc.stove: 0.7867, Acc.palm: 0.7083, Acc.kitchen island: 0.7639, Acc.computer: 0.7948, Acc.swivel chair: 0.6885, Acc.boat: 0.5519, Acc.bar: 0.5621, Acc.arcade machine: 0.8340, Acc.hovel: 0.5089, Acc.bus: 0.9629, Acc.towel: 0.7514, Acc.light: 0.6602, Acc.truck: 0.4155, Acc.tower: 0.0900, Acc.chandelier: 0.8221, Acc.awning: 0.3575, Acc.streetlight: 0.3615, Acc.booth: 0.5561, Acc.television receiver: 0.8067, Acc.airplane: 0.6612, Acc.dirt track: 0.0017, Acc.apparel: 0.6211, Acc.pole: 0.3424, Acc.land: 0.0001, Acc.bannister: 0.1929, Acc.escalator: 0.6011, Acc.ottoman: 0.5797, Acc.bottle: 0.5946, Acc.buffet: 0.5063, Acc.poster: 0.4396, Acc.stage: 0.2810, Acc.van: 0.5644, Acc.ship: 0.7081, Acc.fountain: 0.3811, Acc.conveyer belt: 0.9457, Acc.canopy: 0.5472, Acc.washer: 0.7587, Acc.plaything: 0.3237, Acc.swimming pool: 0.8528, Acc.stool: 0.5443, Acc.barrel: 0.7146, Acc.basket: 0.4448, Acc.waterfall: 0.9116, Acc.tent: 0.9779, Acc.bag: 0.2525, Acc.minibike: 0.8695, Acc.cradle: 0.9779, Acc.oven: 0.6457, Acc.ball: 0.4841, Acc.food: 0.5209, Acc.step: 0.2122, Acc.tank: 0.4411, Acc.trade name: 0.2516, Acc.microwave: 0.9293, Acc.pot: 0.4747, Acc.animal: 0.5378, Acc.bicycle: 0.8065, Acc.lake: 0.6334, Acc.dishwasher: 0.7266, Acc.screen: 0.9131, Acc.blanket: 0.1575, Acc.sculpture: 0.7989, Acc.hood: 0.7284, Acc.sconce: 0.5340, Acc.vase: 0.5099, Acc.traffic light: 0.4303, Acc.tray: 0.1572, Acc.ashcan: 0.5609, Acc.fan: 0.7751, Acc.pier: 0.1648, Acc.crt screen: 0.0649, Acc.plate: 0.6854, Acc.monitor: 0.0352, Acc.bulletin board: 0.6518, Acc.shower: 0.0661, Acc.radiator: 0.7130, Acc.glass: 0.1380, Acc.clock: 0.4208, Acc.flag: 0.4808
2023-11-11 08:40:02,642 - mmseg - INFO - Iter [112050/160000]	lr: 2.057e-05, eta: 9:20:44, time: 2.508, data_time: 1.861, memory: 23129, decode.loss_ce: 0.1937, decode.acc_seg: 92.0892, loss: 0.1937
2023-11-11 08:40:36,495 - mmseg - INFO - Iter [112100/160000]	lr: 2.053e-05, eta: 9:20:09, time: 0.676, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1934, decode.acc_seg: 91.7161, loss: 0.1934
2023-11-11 08:41:11,485 - mmseg - INFO - Iter [112150/160000]	lr: 2.049e-05, eta: 9:19:33, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2006, decode.acc_seg: 91.6051, loss: 0.2006
2023-11-11 08:41:47,181 - mmseg - INFO - Iter [112200/160000]	lr: 2.045e-05, eta: 9:18:59, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1826, decode.acc_seg: 92.3720, loss: 0.1826
2023-11-11 08:42:22,025 - mmseg - INFO - Iter [112250/160000]	lr: 2.041e-05, eta: 9:18:23, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1816, decode.acc_seg: 92.4155, loss: 0.1816
2023-11-11 08:42:55,434 - mmseg - INFO - Iter [112300/160000]	lr: 2.037e-05, eta: 9:17:48, time: 0.669, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1912, decode.acc_seg: 92.0508, loss: 0.1912
2023-11-11 08:43:31,650 - mmseg - INFO - Iter [112350/160000]	lr: 2.033e-05, eta: 9:17:13, time: 0.723, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1898, decode.acc_seg: 91.9981, loss: 0.1898
2023-11-11 08:44:06,229 - mmseg - INFO - Iter [112400/160000]	lr: 2.029e-05, eta: 9:16:38, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1818, decode.acc_seg: 92.1634, loss: 0.1818
2023-11-11 08:44:41,868 - mmseg - INFO - Iter [112450/160000]	lr: 2.026e-05, eta: 9:16:03, time: 0.712, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1875, decode.acc_seg: 92.1229, loss: 0.1875
2023-11-11 08:45:17,236 - mmseg - INFO - Iter [112500/160000]	lr: 2.022e-05, eta: 9:15:28, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2047, decode.acc_seg: 91.3768, loss: 0.2047
2023-11-11 08:45:49,080 - mmseg - INFO - Iter [112550/160000]	lr: 2.018e-05, eta: 9:14:52, time: 0.638, data_time: 0.010, memory: 23129, decode.loss_ce: 0.2011, decode.acc_seg: 91.7158, loss: 0.2011
2023-11-11 08:46:20,554 - mmseg - INFO - Iter [112600/160000]	lr: 2.014e-05, eta: 9:14:15, time: 0.630, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1969, decode.acc_seg: 91.7833, loss: 0.1969
2023-11-11 08:46:52,132 - mmseg - INFO - Iter [112650/160000]	lr: 2.010e-05, eta: 9:13:38, time: 0.631, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1789, decode.acc_seg: 92.4185, loss: 0.1789
2023-11-11 08:47:24,560 - mmseg - INFO - Iter [112700/160000]	lr: 2.006e-05, eta: 9:13:02, time: 0.649, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1934, decode.acc_seg: 91.9504, loss: 0.1934
2023-11-11 08:47:58,492 - mmseg - INFO - Iter [112750/160000]	lr: 2.002e-05, eta: 9:12:27, time: 0.678, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1777, decode.acc_seg: 92.4510, loss: 0.1777
2023-11-11 08:48:33,220 - mmseg - INFO - Iter [112800/160000]	lr: 1.998e-05, eta: 9:11:51, time: 0.696, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1933, decode.acc_seg: 92.1736, loss: 0.1933
2023-11-11 08:49:07,097 - mmseg - INFO - Iter [112850/160000]	lr: 1.994e-05, eta: 9:11:16, time: 0.676, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1873, decode.acc_seg: 92.1060, loss: 0.1873
2023-11-11 08:49:40,378 - mmseg - INFO - Iter [112900/160000]	lr: 1.990e-05, eta: 9:10:40, time: 0.667, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1935, decode.acc_seg: 91.8622, loss: 0.1935
2023-11-11 08:50:14,101 - mmseg - INFO - Iter [112950/160000]	lr: 1.986e-05, eta: 9:10:04, time: 0.673, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1885, decode.acc_seg: 92.0734, loss: 0.1885
2023-11-11 08:50:46,724 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 08:50:46,724 - mmseg - INFO - Iter [113000/160000]	lr: 1.982e-05, eta: 9:09:28, time: 0.653, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1918, decode.acc_seg: 92.0636, loss: 0.1918
2023-11-11 08:51:21,215 - mmseg - INFO - Iter [113050/160000]	lr: 1.978e-05, eta: 9:08:53, time: 0.689, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1911, decode.acc_seg: 91.9962, loss: 0.1911
2023-11-11 08:51:56,945 - mmseg - INFO - Iter [113100/160000]	lr: 1.974e-05, eta: 9:08:18, time: 0.716, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1913, decode.acc_seg: 91.9900, loss: 0.1913
2023-11-11 08:52:31,722 - mmseg - INFO - Iter [113150/160000]	lr: 1.971e-05, eta: 9:07:43, time: 0.694, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1831, decode.acc_seg: 92.4384, loss: 0.1831
2023-11-11 08:53:07,020 - mmseg - INFO - Iter [113200/160000]	lr: 1.967e-05, eta: 9:07:08, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1930, decode.acc_seg: 91.9803, loss: 0.1930
2023-11-11 08:53:42,414 - mmseg - INFO - Iter [113250/160000]	lr: 1.963e-05, eta: 9:06:33, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1976, decode.acc_seg: 91.6628, loss: 0.1976
2023-11-11 08:54:17,585 - mmseg - INFO - Iter [113300/160000]	lr: 1.959e-05, eta: 9:05:58, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1866, decode.acc_seg: 92.1455, loss: 0.1866
2023-11-11 08:54:52,394 - mmseg - INFO - Iter [113350/160000]	lr: 1.955e-05, eta: 9:05:23, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1890, decode.acc_seg: 91.9546, loss: 0.1890
2023-11-11 08:55:27,516 - mmseg - INFO - Iter [113400/160000]	lr: 1.951e-05, eta: 9:04:48, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1915, decode.acc_seg: 92.1678, loss: 0.1915
2023-11-11 08:56:01,627 - mmseg - INFO - Iter [113450/160000]	lr: 1.947e-05, eta: 9:04:12, time: 0.683, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1941, decode.acc_seg: 91.7963, loss: 0.1941
2023-11-11 08:56:36,627 - mmseg - INFO - Iter [113500/160000]	lr: 1.943e-05, eta: 9:03:37, time: 0.701, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1967, decode.acc_seg: 91.8422, loss: 0.1967
2023-11-11 08:57:08,286 - mmseg - INFO - Iter [113550/160000]	lr: 1.939e-05, eta: 9:03:01, time: 0.632, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1915, decode.acc_seg: 92.1091, loss: 0.1915
2023-11-11 08:57:42,103 - mmseg - INFO - Iter [113600/160000]	lr: 1.936e-05, eta: 9:02:25, time: 0.676, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1943, decode.acc_seg: 91.9925, loss: 0.1943
2023-11-11 08:58:17,070 - mmseg - INFO - Iter [113650/160000]	lr: 1.932e-05, eta: 9:01:50, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1952, decode.acc_seg: 91.6958, loss: 0.1952
2023-11-11 08:58:49,991 - mmseg - INFO - Iter [113700/160000]	lr: 1.928e-05, eta: 9:01:14, time: 0.659, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1883, decode.acc_seg: 92.1075, loss: 0.1883
2023-11-11 08:59:21,750 - mmseg - INFO - Iter [113750/160000]	lr: 1.924e-05, eta: 9:00:38, time: 0.635, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1917, decode.acc_seg: 92.2866, loss: 0.1917
2023-11-11 08:59:56,856 - mmseg - INFO - Iter [113800/160000]	lr: 1.920e-05, eta: 9:00:03, time: 0.701, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1858, decode.acc_seg: 92.2542, loss: 0.1858
2023-11-11 09:00:31,785 - mmseg - INFO - Iter [113850/160000]	lr: 1.916e-05, eta: 8:59:27, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1775, decode.acc_seg: 92.3922, loss: 0.1775
2023-11-11 09:01:05,280 - mmseg - INFO - Iter [113900/160000]	lr: 1.912e-05, eta: 8:58:52, time: 0.669, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1806, decode.acc_seg: 92.2194, loss: 0.1806
2023-11-11 09:01:37,262 - mmseg - INFO - Iter [113950/160000]	lr: 1.908e-05, eta: 8:58:15, time: 0.640, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1794, decode.acc_seg: 92.2591, loss: 0.1794
2023-11-11 09:02:10,058 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 09:02:10,059 - mmseg - INFO - Iter [114000/160000]	lr: 1.905e-05, eta: 8:57:39, time: 0.656, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1827, decode.acc_seg: 92.1237, loss: 0.1827
2023-11-11 09:02:44,328 - mmseg - INFO - Iter [114050/160000]	lr: 1.901e-05, eta: 8:57:04, time: 0.684, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1846, decode.acc_seg: 92.3656, loss: 0.1846
2023-11-11 09:03:21,252 - mmseg - INFO - Iter [114100/160000]	lr: 1.897e-05, eta: 8:56:30, time: 0.739, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1775, decode.acc_seg: 92.5933, loss: 0.1775
2023-11-11 09:03:54,193 - mmseg - INFO - Iter [114150/160000]	lr: 1.893e-05, eta: 8:55:54, time: 0.660, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1912, decode.acc_seg: 91.9011, loss: 0.1912
2023-11-11 09:04:26,331 - mmseg - INFO - Iter [114200/160000]	lr: 1.889e-05, eta: 8:55:18, time: 0.642, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1884, decode.acc_seg: 92.0769, loss: 0.1884
2023-11-11 09:04:59,584 - mmseg - INFO - Iter [114250/160000]	lr: 1.885e-05, eta: 8:54:42, time: 0.666, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1832, decode.acc_seg: 92.2944, loss: 0.1832
2023-11-11 09:05:32,596 - mmseg - INFO - Iter [114300/160000]	lr: 1.882e-05, eta: 8:54:06, time: 0.659, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1815, decode.acc_seg: 92.3513, loss: 0.1815
2023-11-11 09:06:08,265 - mmseg - INFO - Iter [114350/160000]	lr: 1.878e-05, eta: 8:53:31, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1965, decode.acc_seg: 92.0937, loss: 0.1965
2023-11-11 09:06:43,468 - mmseg - INFO - Iter [114400/160000]	lr: 1.874e-05, eta: 8:52:56, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1772, decode.acc_seg: 92.4561, loss: 0.1772
2023-11-11 09:07:16,877 - mmseg - INFO - Iter [114450/160000]	lr: 1.870e-05, eta: 8:52:20, time: 0.667, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1800, decode.acc_seg: 92.2857, loss: 0.1800
2023-11-11 09:07:51,832 - mmseg - INFO - Iter [114500/160000]	lr: 1.866e-05, eta: 8:51:45, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1862, decode.acc_seg: 92.1457, loss: 0.1862
2023-11-11 09:08:24,253 - mmseg - INFO - Iter [114550/160000]	lr: 1.862e-05, eta: 8:51:09, time: 0.648, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1825, decode.acc_seg: 92.2517, loss: 0.1825
2023-11-11 09:08:57,526 - mmseg - INFO - Iter [114600/160000]	lr: 1.859e-05, eta: 8:50:33, time: 0.666, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1943, decode.acc_seg: 91.8824, loss: 0.1943
2023-11-11 09:09:31,669 - mmseg - INFO - Iter [114650/160000]	lr: 1.855e-05, eta: 8:49:58, time: 0.682, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1865, decode.acc_seg: 92.2779, loss: 0.1865
2023-11-11 09:10:05,823 - mmseg - INFO - Iter [114700/160000]	lr: 1.851e-05, eta: 8:49:23, time: 0.683, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1880, decode.acc_seg: 92.2266, loss: 0.1880
2023-11-11 09:10:39,414 - mmseg - INFO - Iter [114750/160000]	lr: 1.847e-05, eta: 8:48:47, time: 0.671, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1813, decode.acc_seg: 92.5723, loss: 0.1813
2023-11-11 09:11:14,522 - mmseg - INFO - Iter [114800/160000]	lr: 1.843e-05, eta: 8:48:12, time: 0.702, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1822, decode.acc_seg: 92.2615, loss: 0.1822
2023-11-11 09:11:49,064 - mmseg - INFO - Iter [114850/160000]	lr: 1.840e-05, eta: 8:47:37, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1832, decode.acc_seg: 92.1044, loss: 0.1832
2023-11-11 09:12:23,244 - mmseg - INFO - Iter [114900/160000]	lr: 1.836e-05, eta: 8:47:01, time: 0.682, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1976, decode.acc_seg: 91.7768, loss: 0.1976
2023-11-11 09:12:57,806 - mmseg - INFO - Iter [114950/160000]	lr: 1.832e-05, eta: 8:46:26, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1762, decode.acc_seg: 92.4191, loss: 0.1762
2023-11-11 09:13:30,715 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 09:13:30,715 - mmseg - INFO - Iter [115000/160000]	lr: 1.828e-05, eta: 8:45:50, time: 0.657, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1995, decode.acc_seg: 91.5634, loss: 0.1995
2023-11-11 09:14:06,376 - mmseg - INFO - Iter [115050/160000]	lr: 1.824e-05, eta: 8:45:15, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1801, decode.acc_seg: 92.3470, loss: 0.1801
2023-11-11 09:14:43,222 - mmseg - INFO - Iter [115100/160000]	lr: 1.821e-05, eta: 8:44:41, time: 0.737, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1957, decode.acc_seg: 91.9296, loss: 0.1957
2023-11-11 09:15:20,643 - mmseg - INFO - Iter [115150/160000]	lr: 1.817e-05, eta: 8:44:07, time: 0.748, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1925, decode.acc_seg: 92.2396, loss: 0.1925
2023-11-11 09:15:57,985 - mmseg - INFO - Iter [115200/160000]	lr: 1.813e-05, eta: 8:43:33, time: 0.747, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1844, decode.acc_seg: 92.4740, loss: 0.1844
2023-11-11 09:16:31,777 - mmseg - INFO - Iter [115250/160000]	lr: 1.809e-05, eta: 8:42:57, time: 0.677, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1844, decode.acc_seg: 92.2771, loss: 0.1844
2023-11-11 09:17:03,903 - mmseg - INFO - Iter [115300/160000]	lr: 1.805e-05, eta: 8:42:21, time: 0.642, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1942, decode.acc_seg: 91.7292, loss: 0.1942
2023-11-11 09:17:39,210 - mmseg - INFO - Iter [115350/160000]	lr: 1.802e-05, eta: 8:41:46, time: 0.707, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1867, decode.acc_seg: 91.9571, loss: 0.1867
2023-11-11 09:18:14,328 - mmseg - INFO - Iter [115400/160000]	lr: 1.798e-05, eta: 8:41:11, time: 0.701, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1873, decode.acc_seg: 92.2455, loss: 0.1873
2023-11-11 09:18:46,625 - mmseg - INFO - Iter [115450/160000]	lr: 1.794e-05, eta: 8:40:35, time: 0.647, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1966, decode.acc_seg: 91.7855, loss: 0.1966
2023-11-11 09:19:21,233 - mmseg - INFO - Iter [115500/160000]	lr: 1.790e-05, eta: 8:39:59, time: 0.691, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1840, decode.acc_seg: 92.5761, loss: 0.1840
2023-11-11 09:19:55,738 - mmseg - INFO - Iter [115550/160000]	lr: 1.787e-05, eta: 8:39:24, time: 0.691, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1851, decode.acc_seg: 92.4538, loss: 0.1851
2023-11-11 09:20:27,725 - mmseg - INFO - Iter [115600/160000]	lr: 1.783e-05, eta: 8:38:48, time: 0.640, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1896, decode.acc_seg: 92.1019, loss: 0.1896
2023-11-11 09:21:03,402 - mmseg - INFO - Iter [115650/160000]	lr: 1.779e-05, eta: 8:38:13, time: 0.712, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1841, decode.acc_seg: 92.3429, loss: 0.1841
2023-11-11 09:21:35,648 - mmseg - INFO - Iter [115700/160000]	lr: 1.775e-05, eta: 8:37:37, time: 0.646, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1909, decode.acc_seg: 92.0146, loss: 0.1909
2023-11-11 09:22:09,826 - mmseg - INFO - Iter [115750/160000]	lr: 1.772e-05, eta: 8:37:02, time: 0.683, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1736, decode.acc_seg: 92.6098, loss: 0.1736
2023-11-11 09:22:44,588 - mmseg - INFO - Iter [115800/160000]	lr: 1.768e-05, eta: 8:36:26, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1825, decode.acc_seg: 91.9875, loss: 0.1825
2023-11-11 09:23:19,927 - mmseg - INFO - Iter [115850/160000]	lr: 1.764e-05, eta: 8:35:52, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1921, decode.acc_seg: 92.2376, loss: 0.1921
2023-11-11 09:23:55,661 - mmseg - INFO - Iter [115900/160000]	lr: 1.760e-05, eta: 8:35:17, time: 0.714, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1799, decode.acc_seg: 92.5019, loss: 0.1799
2023-11-11 09:24:28,117 - mmseg - INFO - Iter [115950/160000]	lr: 1.757e-05, eta: 8:34:41, time: 0.650, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1834, decode.acc_seg: 92.2946, loss: 0.1834
2023-11-11 09:25:03,754 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 09:25:03,754 - mmseg - INFO - Iter [116000/160000]	lr: 1.753e-05, eta: 8:34:06, time: 0.713, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1862, decode.acc_seg: 92.3911, loss: 0.1862
2023-11-11 09:25:38,384 - mmseg - INFO - Iter [116050/160000]	lr: 1.749e-05, eta: 8:33:31, time: 0.692, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1817, decode.acc_seg: 92.3649, loss: 0.1817
2023-11-11 09:26:13,490 - mmseg - INFO - Iter [116100/160000]	lr: 1.745e-05, eta: 8:32:56, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1920, decode.acc_seg: 92.1285, loss: 0.1920
2023-11-11 09:26:48,426 - mmseg - INFO - Iter [116150/160000]	lr: 1.742e-05, eta: 8:32:21, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1858, decode.acc_seg: 92.1360, loss: 0.1858
2023-11-11 09:27:23,770 - mmseg - INFO - Iter [116200/160000]	lr: 1.738e-05, eta: 8:31:46, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1883, decode.acc_seg: 92.0444, loss: 0.1883
2023-11-11 09:27:59,409 - mmseg - INFO - Iter [116250/160000]	lr: 1.734e-05, eta: 8:31:11, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1851, decode.acc_seg: 92.2570, loss: 0.1851
2023-11-11 09:28:34,005 - mmseg - INFO - Iter [116300/160000]	lr: 1.730e-05, eta: 8:30:36, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1767, decode.acc_seg: 92.6947, loss: 0.1767
2023-11-11 09:29:05,432 - mmseg - INFO - Iter [116350/160000]	lr: 1.727e-05, eta: 8:29:59, time: 0.628, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1928, decode.acc_seg: 91.7256, loss: 0.1928
2023-11-11 09:29:40,620 - mmseg - INFO - Iter [116400/160000]	lr: 1.723e-05, eta: 8:29:24, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1877, decode.acc_seg: 92.1044, loss: 0.1877
2023-11-11 09:30:16,521 - mmseg - INFO - Iter [116450/160000]	lr: 1.719e-05, eta: 8:28:49, time: 0.718, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1807, decode.acc_seg: 92.4083, loss: 0.1807
2023-11-11 09:30:49,344 - mmseg - INFO - Iter [116500/160000]	lr: 1.716e-05, eta: 8:28:14, time: 0.657, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1738, decode.acc_seg: 92.6655, loss: 0.1738
2023-11-11 09:31:22,457 - mmseg - INFO - Iter [116550/160000]	lr: 1.712e-05, eta: 8:27:38, time: 0.662, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1817, decode.acc_seg: 92.5067, loss: 0.1817
2023-11-11 09:31:54,536 - mmseg - INFO - Iter [116600/160000]	lr: 1.708e-05, eta: 8:27:02, time: 0.642, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1866, decode.acc_seg: 92.2340, loss: 0.1866
2023-11-11 09:32:27,061 - mmseg - INFO - Iter [116650/160000]	lr: 1.705e-05, eta: 8:26:26, time: 0.650, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1889, decode.acc_seg: 91.8868, loss: 0.1889
2023-11-11 09:32:59,325 - mmseg - INFO - Iter [116700/160000]	lr: 1.701e-05, eta: 8:25:49, time: 0.645, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1838, decode.acc_seg: 92.3006, loss: 0.1838
2023-11-11 09:33:33,749 - mmseg - INFO - Iter [116750/160000]	lr: 1.697e-05, eta: 8:25:14, time: 0.688, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1784, decode.acc_seg: 92.4794, loss: 0.1784
2023-11-11 09:34:09,556 - mmseg - INFO - Iter [116800/160000]	lr: 1.694e-05, eta: 8:24:39, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1788, decode.acc_seg: 92.2418, loss: 0.1788
2023-11-11 09:34:44,844 - mmseg - INFO - Iter [116850/160000]	lr: 1.690e-05, eta: 8:24:04, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1778, decode.acc_seg: 92.5537, loss: 0.1778
2023-11-11 09:35:19,936 - mmseg - INFO - Iter [116900/160000]	lr: 1.686e-05, eta: 8:23:29, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1784, decode.acc_seg: 92.5816, loss: 0.1784
2023-11-11 09:35:54,930 - mmseg - INFO - Iter [116950/160000]	lr: 1.682e-05, eta: 8:22:54, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1859, decode.acc_seg: 92.1689, loss: 0.1859
2023-11-11 09:36:29,663 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 09:36:29,663 - mmseg - INFO - Iter [117000/160000]	lr: 1.679e-05, eta: 8:22:19, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1910, decode.acc_seg: 91.9969, loss: 0.1910
2023-11-11 09:37:04,805 - mmseg - INFO - Iter [117050/160000]	lr: 1.675e-05, eta: 8:21:44, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1814, decode.acc_seg: 92.4999, loss: 0.1814
2023-11-11 09:37:38,615 - mmseg - INFO - Iter [117100/160000]	lr: 1.671e-05, eta: 8:21:09, time: 0.677, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1851, decode.acc_seg: 92.2694, loss: 0.1851
2023-11-11 09:38:10,079 - mmseg - INFO - Iter [117150/160000]	lr: 1.668e-05, eta: 8:20:32, time: 0.629, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1788, decode.acc_seg: 92.4093, loss: 0.1788
2023-11-11 09:38:42,342 - mmseg - INFO - Iter [117200/160000]	lr: 1.664e-05, eta: 8:19:56, time: 0.646, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1819, decode.acc_seg: 92.3784, loss: 0.1819
2023-11-11 09:39:13,546 - mmseg - INFO - Iter [117250/160000]	lr: 1.661e-05, eta: 8:19:20, time: 0.624, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1869, decode.acc_seg: 92.1290, loss: 0.1869
2023-11-11 09:39:47,336 - mmseg - INFO - Iter [117300/160000]	lr: 1.657e-05, eta: 8:18:44, time: 0.675, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1756, decode.acc_seg: 92.3768, loss: 0.1756
2023-11-11 09:40:21,735 - mmseg - INFO - Iter [117350/160000]	lr: 1.653e-05, eta: 8:18:09, time: 0.688, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1869, decode.acc_seg: 92.1993, loss: 0.1869
2023-11-11 09:40:53,801 - mmseg - INFO - Iter [117400/160000]	lr: 1.650e-05, eta: 8:17:33, time: 0.641, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1896, decode.acc_seg: 92.1041, loss: 0.1896
2023-11-11 09:41:27,279 - mmseg - INFO - Iter [117450/160000]	lr: 1.646e-05, eta: 8:16:57, time: 0.671, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1855, decode.acc_seg: 92.3377, loss: 0.1855
2023-11-11 09:41:59,348 - mmseg - INFO - Iter [117500/160000]	lr: 1.642e-05, eta: 8:16:21, time: 0.640, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1865, decode.acc_seg: 92.3155, loss: 0.1865
2023-11-11 09:42:34,755 - mmseg - INFO - Iter [117550/160000]	lr: 1.639e-05, eta: 8:15:46, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1888, decode.acc_seg: 92.0627, loss: 0.1888
2023-11-11 09:43:10,064 - mmseg - INFO - Iter [117600/160000]	lr: 1.635e-05, eta: 8:15:11, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1768, decode.acc_seg: 92.5258, loss: 0.1768
2023-11-11 09:43:45,409 - mmseg - INFO - Iter [117650/160000]	lr: 1.631e-05, eta: 8:14:37, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1743, decode.acc_seg: 92.5700, loss: 0.1743
2023-11-11 09:44:18,486 - mmseg - INFO - Iter [117700/160000]	lr: 1.628e-05, eta: 8:14:01, time: 0.661, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1848, decode.acc_seg: 92.2889, loss: 0.1848
2023-11-11 09:44:51,450 - mmseg - INFO - Iter [117750/160000]	lr: 1.624e-05, eta: 8:13:25, time: 0.659, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1822, decode.acc_seg: 92.5497, loss: 0.1822
2023-11-11 09:45:25,650 - mmseg - INFO - Iter [117800/160000]	lr: 1.621e-05, eta: 8:12:50, time: 0.684, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1807, decode.acc_seg: 92.2951, loss: 0.1807
2023-11-11 09:46:00,225 - mmseg - INFO - Iter [117850/160000]	lr: 1.617e-05, eta: 8:12:14, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1812, decode.acc_seg: 92.3824, loss: 0.1812
2023-11-11 09:46:35,402 - mmseg - INFO - Iter [117900/160000]	lr: 1.613e-05, eta: 8:11:39, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1878, decode.acc_seg: 92.4402, loss: 0.1878
2023-11-11 09:47:10,781 - mmseg - INFO - Iter [117950/160000]	lr: 1.610e-05, eta: 8:11:05, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1725, decode.acc_seg: 92.7077, loss: 0.1725
2023-11-11 09:47:44,781 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 09:47:44,782 - mmseg - INFO - Iter [118000/160000]	lr: 1.606e-05, eta: 8:10:29, time: 0.681, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1812, decode.acc_seg: 92.3705, loss: 0.1812
2023-11-11 09:48:19,469 - mmseg - INFO - Iter [118050/160000]	lr: 1.602e-05, eta: 8:09:54, time: 0.693, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1906, decode.acc_seg: 92.1925, loss: 0.1906
2023-11-11 09:48:54,204 - mmseg - INFO - Iter [118100/160000]	lr: 1.599e-05, eta: 8:09:19, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1846, decode.acc_seg: 92.1838, loss: 0.1846
2023-11-11 09:49:25,787 - mmseg - INFO - Iter [118150/160000]	lr: 1.595e-05, eta: 8:08:43, time: 0.632, data_time: 0.009, memory: 23129, decode.loss_ce: 0.2001, decode.acc_seg: 91.7108, loss: 0.2001
2023-11-11 09:49:57,202 - mmseg - INFO - Iter [118200/160000]	lr: 1.592e-05, eta: 8:08:06, time: 0.628, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1781, decode.acc_seg: 92.5824, loss: 0.1781
2023-11-11 09:50:29,169 - mmseg - INFO - Iter [118250/160000]	lr: 1.588e-05, eta: 8:07:30, time: 0.639, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1784, decode.acc_seg: 92.5193, loss: 0.1784
2023-11-11 09:51:02,218 - mmseg - INFO - Iter [118300/160000]	lr: 1.584e-05, eta: 8:06:54, time: 0.660, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1725, decode.acc_seg: 92.6572, loss: 0.1725
2023-11-11 09:51:37,065 - mmseg - INFO - Iter [118350/160000]	lr: 1.581e-05, eta: 8:06:19, time: 0.697, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1842, decode.acc_seg: 92.3154, loss: 0.1842
2023-11-11 09:52:11,948 - mmseg - INFO - Iter [118400/160000]	lr: 1.577e-05, eta: 8:05:44, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1732, decode.acc_seg: 92.6726, loss: 0.1732
2023-11-11 09:52:43,836 - mmseg - INFO - Iter [118450/160000]	lr: 1.574e-05, eta: 8:05:08, time: 0.637, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1811, decode.acc_seg: 92.5498, loss: 0.1811
2023-11-11 09:53:18,603 - mmseg - INFO - Iter [118500/160000]	lr: 1.570e-05, eta: 8:04:33, time: 0.694, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1893, decode.acc_seg: 92.2778, loss: 0.1893
2023-11-11 09:53:52,504 - mmseg - INFO - Iter [118550/160000]	lr: 1.567e-05, eta: 8:03:58, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1752, decode.acc_seg: 92.6646, loss: 0.1752
2023-11-11 09:54:27,099 - mmseg - INFO - Iter [118600/160000]	lr: 1.563e-05, eta: 8:03:22, time: 0.691, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1806, decode.acc_seg: 92.4529, loss: 0.1806
2023-11-11 09:55:02,503 - mmseg - INFO - Iter [118650/160000]	lr: 1.559e-05, eta: 8:02:47, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1653, decode.acc_seg: 93.0665, loss: 0.1653
2023-11-11 09:55:37,959 - mmseg - INFO - Iter [118700/160000]	lr: 1.556e-05, eta: 8:02:13, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1818, decode.acc_seg: 92.3194, loss: 0.1818
2023-11-11 09:56:12,711 - mmseg - INFO - Iter [118750/160000]	lr: 1.552e-05, eta: 8:01:37, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1813, decode.acc_seg: 92.4846, loss: 0.1813
2023-11-11 09:56:48,007 - mmseg - INFO - Iter [118800/160000]	lr: 1.549e-05, eta: 8:01:03, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1801, decode.acc_seg: 92.5072, loss: 0.1801
2023-11-11 09:57:23,159 - mmseg - INFO - Iter [118850/160000]	lr: 1.545e-05, eta: 8:00:28, time: 0.704, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1800, decode.acc_seg: 92.4708, loss: 0.1800
2023-11-11 09:57:58,229 - mmseg - INFO - Iter [118900/160000]	lr: 1.542e-05, eta: 7:59:53, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1758, decode.acc_seg: 92.4892, loss: 0.1758
2023-11-11 09:58:33,382 - mmseg - INFO - Iter [118950/160000]	lr: 1.538e-05, eta: 7:59:18, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1754, decode.acc_seg: 92.6075, loss: 0.1754
2023-11-11 09:59:08,831 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 09:59:08,831 - mmseg - INFO - Iter [119000/160000]	lr: 1.535e-05, eta: 7:58:43, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1895, decode.acc_seg: 92.2547, loss: 0.1895
2023-11-11 09:59:41,681 - mmseg - INFO - Iter [119050/160000]	lr: 1.531e-05, eta: 7:58:07, time: 0.657, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1777, decode.acc_seg: 92.5279, loss: 0.1777
2023-11-11 10:00:17,579 - mmseg - INFO - Iter [119100/160000]	lr: 1.528e-05, eta: 7:57:32, time: 0.718, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1899, decode.acc_seg: 92.0327, loss: 0.1899
2023-11-11 10:00:50,872 - mmseg - INFO - Iter [119150/160000]	lr: 1.524e-05, eta: 7:56:57, time: 0.667, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1772, decode.acc_seg: 92.4659, loss: 0.1772
2023-11-11 10:01:25,264 - mmseg - INFO - Iter [119200/160000]	lr: 1.521e-05, eta: 7:56:21, time: 0.687, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1796, decode.acc_seg: 92.5313, loss: 0.1796
2023-11-11 10:01:58,985 - mmseg - INFO - Iter [119250/160000]	lr: 1.517e-05, eta: 7:55:46, time: 0.676, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1810, decode.acc_seg: 92.4978, loss: 0.1810
2023-11-11 10:02:31,780 - mmseg - INFO - Iter [119300/160000]	lr: 1.513e-05, eta: 7:55:10, time: 0.656, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1789, decode.acc_seg: 92.3140, loss: 0.1789
2023-11-11 10:03:05,130 - mmseg - INFO - Iter [119350/160000]	lr: 1.510e-05, eta: 7:54:34, time: 0.667, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1708, decode.acc_seg: 92.6986, loss: 0.1708
2023-11-11 10:03:38,574 - mmseg - INFO - Iter [119400/160000]	lr: 1.506e-05, eta: 7:53:59, time: 0.669, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1871, decode.acc_seg: 92.1967, loss: 0.1871
2023-11-11 10:04:13,126 - mmseg - INFO - Iter [119450/160000]	lr: 1.503e-05, eta: 7:53:24, time: 0.690, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1810, decode.acc_seg: 92.4596, loss: 0.1810
2023-11-11 10:04:47,490 - mmseg - INFO - Iter [119500/160000]	lr: 1.499e-05, eta: 7:52:48, time: 0.688, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1718, decode.acc_seg: 92.6331, loss: 0.1718
2023-11-11 10:05:20,448 - mmseg - INFO - Iter [119550/160000]	lr: 1.496e-05, eta: 7:52:13, time: 0.658, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1828, decode.acc_seg: 92.2167, loss: 0.1828
2023-11-11 10:05:54,208 - mmseg - INFO - Iter [119600/160000]	lr: 1.492e-05, eta: 7:51:37, time: 0.675, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1726, decode.acc_seg: 92.7791, loss: 0.1726
2023-11-11 10:06:27,833 - mmseg - INFO - Iter [119650/160000]	lr: 1.489e-05, eta: 7:51:02, time: 0.672, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1900, decode.acc_seg: 92.0953, loss: 0.1900
2023-11-11 10:07:01,766 - mmseg - INFO - Iter [119700/160000]	lr: 1.485e-05, eta: 7:50:26, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1665, decode.acc_seg: 92.8206, loss: 0.1665
2023-11-11 10:07:34,465 - mmseg - INFO - Iter [119750/160000]	lr: 1.482e-05, eta: 7:49:51, time: 0.654, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1845, decode.acc_seg: 92.1364, loss: 0.1845
2023-11-11 10:08:08,027 - mmseg - INFO - Iter [119800/160000]	lr: 1.478e-05, eta: 7:49:15, time: 0.672, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1696, decode.acc_seg: 92.5208, loss: 0.1696
2023-11-11 10:08:43,562 - mmseg - INFO - Iter [119850/160000]	lr: 1.475e-05, eta: 7:48:40, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1871, decode.acc_seg: 92.1693, loss: 0.1871
2023-11-11 10:09:18,401 - mmseg - INFO - Iter [119900/160000]	lr: 1.471e-05, eta: 7:48:05, time: 0.696, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1818, decode.acc_seg: 92.4403, loss: 0.1818
2023-11-11 10:09:53,992 - mmseg - INFO - Iter [119950/160000]	lr: 1.468e-05, eta: 7:47:30, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1829, decode.acc_seg: 92.1849, loss: 0.1829
2023-11-11 10:10:26,508 - mmseg - INFO - Saving checkpoint at 120000 iterations
2023-11-11 10:10:31,465 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 10:10:31,465 - mmseg - INFO - Iter [120000/160000]	lr: 1.465e-05, eta: 7:46:56, time: 0.751, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1862, decode.acc_seg: 92.2420, loss: 0.1862
2023-11-11 10:12:01,789 - mmseg - INFO - per class results:
2023-11-11 10:12:01,802 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 77.41 | 88.74 |
|       building      | 81.84 | 91.55 |
|         sky         | 93.96 |  97.2 |
|        floor        | 82.79 | 90.08 |
|         tree        | 73.67 | 88.52 |
|       ceiling       | 83.36 |  91.5 |
|         road        |  81.8 | 89.05 |
|         bed         | 89.52 | 95.99 |
|      windowpane     | 61.83 | 78.74 |
|        grass        | 67.67 | 83.93 |
|       cabinet       | 59.79 | 74.51 |
|       sidewalk      | 64.15 | 79.89 |
|        person       | 81.07 | 93.07 |
|        earth        | 37.22 | 52.42 |
|         door        | 49.65 | 63.14 |
|        table        | 62.01 | 76.92 |
|       mountain      | 52.53 | 68.16 |
|        plant        | 51.57 |  61.4 |
|       curtain       | 74.14 | 85.66 |
|        chair        | 57.37 |  71.0 |
|         car         | 82.48 | 92.34 |
|        water        |  55.6 | 70.69 |
|       painting      | 71.93 | 86.81 |
|         sofa        | 67.07 | 83.68 |
|        shelf        | 42.49 | 58.91 |
|        house        | 45.15 | 62.84 |
|         sea         |  59.7 | 82.92 |
|        mirror       | 67.42 | 73.86 |
|         rug         |  64.6 | 77.29 |
|        field        | 27.73 | 40.52 |
|       armchair      | 43.65 | 60.87 |
|         seat        | 63.28 | 80.64 |
|        fence        | 36.52 | 47.63 |
|         desk        | 50.96 | 70.27 |
|         rock        | 37.37 |  52.3 |
|       wardrobe      | 49.47 | 70.03 |
|         lamp        | 65.29 | 77.66 |
|       bathtub       | 80.55 | 85.84 |
|       railing       | 31.12 | 43.46 |
|       cushion       | 58.76 | 73.33 |
|         base        | 30.98 | 40.13 |
|         box         | 30.25 | 38.31 |
|        column       | 45.29 | 56.41 |
|      signboard      | 39.43 |  52.3 |
|   chest of drawers  | 46.02 | 58.85 |
|       counter       | 36.58 | 44.14 |
|         sand        | 47.84 | 71.31 |
|         sink        |  70.6 | 80.76 |
|      skyscraper     | 47.15 | 58.39 |
|      fireplace      | 76.04 | 88.95 |
|     refrigerator    | 76.56 | 84.61 |
|      grandstand     | 42.54 | 66.19 |
|         path        | 22.38 | 34.65 |
|        stairs       | 31.11 | 40.67 |
|        runway       | 64.59 | 82.96 |
|         case        | 45.91 | 57.33 |
|      pool table     | 93.07 |  96.8 |
|        pillow       | 59.09 | 68.58 |
|     screen door     | 72.25 |  86.4 |
|       stairway      | 28.85 | 36.72 |
|        river        | 13.44 | 25.55 |
|        bridge       | 62.23 | 85.16 |
|       bookcase      | 36.85 | 55.13 |
|        blind        | 41.09 | 47.42 |
|     coffee table    | 55.12 | 83.24 |
|        toilet       | 85.59 | 92.04 |
|        flower       | 36.26 | 52.06 |
|         book        | 45.96 | 67.51 |
|         hill        | 10.43 | 16.45 |
|        bench        | 45.04 | 51.38 |
|      countertop     |  57.8 | 82.43 |
|        stove        | 69.92 | 77.65 |
|         palm        | 46.81 | 74.75 |
|    kitchen island   | 41.47 | 71.83 |
|       computer      |  63.2 |  72.9 |
|     swivel chair    | 47.19 | 66.03 |
|         boat        | 45.45 | 54.31 |
|         bar         | 45.87 | 59.56 |
|    arcade machine   | 46.21 | 49.25 |
|        hovel        | 55.23 | 62.27 |
|         bus         |  86.3 | 95.87 |
|        towel        | 65.35 | 74.64 |
|        light        | 54.68 | 63.68 |
|        truck        | 34.12 | 42.05 |
|        tower        |  6.73 |  8.68 |
|      chandelier     | 67.85 | 80.95 |
|        awning       |  28.4 | 34.89 |
|     streetlight     | 27.75 | 38.92 |
|        booth        | 57.31 | 59.07 |
| television receiver | 66.79 | 79.64 |
|       airplane      |  61.4 | 74.64 |
|      dirt track     |  4.42 | 17.43 |
|       apparel       | 49.13 | 63.84 |
|         pole        | 23.21 | 29.49 |
|         land        |  0.29 |  0.41 |
|      bannister      | 13.31 | 16.62 |
|      escalator      | 41.98 | 56.18 |
|       ottoman       | 48.87 | 62.68 |
|        bottle       | 35.58 | 61.56 |
|        buffet       | 32.79 | 36.49 |
|        poster       | 31.11 | 43.68 |
|        stage        | 15.04 |  19.9 |
|         van         | 40.71 | 58.18 |
|         ship        | 57.44 | 82.76 |
|       fountain      | 23.84 | 24.11 |
|    conveyer belt    | 83.18 | 96.07 |
|        canopy       | 11.26 | 16.61 |
|        washer       | 75.29 | 76.49 |
|      plaything      | 20.63 | 30.42 |
|    swimming pool    | 73.78 | 81.62 |
|        stool        | 37.75 | 52.35 |
|        barrel       | 57.36 | 75.56 |
|        basket       | 35.42 | 46.38 |
|      waterfall      |  65.8 |  91.7 |
|         tent        | 96.17 | 98.29 |
|         bag         | 17.64 | 21.68 |
|       minibike      |  67.2 |  84.5 |
|        cradle       |  81.3 | 97.85 |
|         oven        | 43.45 | 65.01 |
|         ball        |  44.0 | 60.98 |
|         food        | 49.05 | 55.14 |
|         step        | 18.35 | 24.24 |
|         tank        | 53.95 | 62.82 |
|      trade name     | 25.94 |  29.6 |
|      microwave      | 76.08 | 86.98 |
|         pot         | 41.73 | 46.97 |
|        animal       | 53.85 | 55.84 |
|       bicycle       | 56.83 | 78.91 |
|         lake        | 48.91 | 54.33 |
|      dishwasher     | 60.42 | 74.94 |
|        screen       | 64.09 | 89.83 |
|       blanket       | 15.47 | 19.52 |
|      sculpture      | 64.65 | 77.59 |
|         hood        | 61.95 | 72.39 |
|        sconce       |  48.4 | 58.17 |
|         vase        | 40.52 | 55.48 |
|    traffic light    | 27.31 | 39.41 |
|         tray        |  7.32 | 14.81 |
|        ashcan       | 41.86 | 58.25 |
|         fan         | 57.98 | 76.06 |
|         pier        | 20.27 | 21.59 |
|      crt screen     |  2.19 |  7.51 |
|        plate        | 52.15 | 69.26 |
|       monitor       |  4.4  |  4.9  |
|    bulletin board   | 40.57 | 47.14 |
|        shower       |  2.35 |  2.45 |
|       radiator      | 68.35 | 78.47 |
|        glass        | 14.72 | 15.97 |
|        clock        | 35.25 | 42.23 |
|         flag        | 44.07 | 47.39 |
+---------------------+-------+-------+
2023-11-11 10:12:01,803 - mmseg - INFO - Summary:
2023-11-11 10:12:01,803 - mmseg - INFO - 
+-------+------+------+
|  aAcc | mIoU | mAcc |
+-------+------+------+
| 82.95 | 49.5 | 61.3 |
+-------+------+------+
2023-11-11 10:12:01,809 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 10:12:01,810 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8295, mIoU: 0.4950, mAcc: 0.6130, IoU.wall: 0.7741, IoU.building: 0.8184, IoU.sky: 0.9396, IoU.floor: 0.8279, IoU.tree: 0.7367, IoU.ceiling: 0.8336, IoU.road: 0.8180, IoU.bed : 0.8952, IoU.windowpane: 0.6183, IoU.grass: 0.6767, IoU.cabinet: 0.5979, IoU.sidewalk: 0.6415, IoU.person: 0.8107, IoU.earth: 0.3722, IoU.door: 0.4965, IoU.table: 0.6201, IoU.mountain: 0.5253, IoU.plant: 0.5157, IoU.curtain: 0.7414, IoU.chair: 0.5737, IoU.car: 0.8248, IoU.water: 0.5560, IoU.painting: 0.7193, IoU.sofa: 0.6707, IoU.shelf: 0.4249, IoU.house: 0.4515, IoU.sea: 0.5970, IoU.mirror: 0.6742, IoU.rug: 0.6460, IoU.field: 0.2773, IoU.armchair: 0.4365, IoU.seat: 0.6328, IoU.fence: 0.3652, IoU.desk: 0.5096, IoU.rock: 0.3737, IoU.wardrobe: 0.4947, IoU.lamp: 0.6529, IoU.bathtub: 0.8055, IoU.railing: 0.3112, IoU.cushion: 0.5876, IoU.base: 0.3098, IoU.box: 0.3025, IoU.column: 0.4529, IoU.signboard: 0.3943, IoU.chest of drawers: 0.4602, IoU.counter: 0.3658, IoU.sand: 0.4784, IoU.sink: 0.7060, IoU.skyscraper: 0.4715, IoU.fireplace: 0.7604, IoU.refrigerator: 0.7656, IoU.grandstand: 0.4254, IoU.path: 0.2238, IoU.stairs: 0.3111, IoU.runway: 0.6459, IoU.case: 0.4591, IoU.pool table: 0.9307, IoU.pillow: 0.5909, IoU.screen door: 0.7225, IoU.stairway: 0.2885, IoU.river: 0.1344, IoU.bridge: 0.6223, IoU.bookcase: 0.3685, IoU.blind: 0.4109, IoU.coffee table: 0.5512, IoU.toilet: 0.8559, IoU.flower: 0.3626, IoU.book: 0.4596, IoU.hill: 0.1043, IoU.bench: 0.4504, IoU.countertop: 0.5780, IoU.stove: 0.6992, IoU.palm: 0.4681, IoU.kitchen island: 0.4147, IoU.computer: 0.6320, IoU.swivel chair: 0.4719, IoU.boat: 0.4545, IoU.bar: 0.4587, IoU.arcade machine: 0.4621, IoU.hovel: 0.5523, IoU.bus: 0.8630, IoU.towel: 0.6535, IoU.light: 0.5468, IoU.truck: 0.3412, IoU.tower: 0.0673, IoU.chandelier: 0.6785, IoU.awning: 0.2840, IoU.streetlight: 0.2775, IoU.booth: 0.5731, IoU.television receiver: 0.6679, IoU.airplane: 0.6140, IoU.dirt track: 0.0442, IoU.apparel: 0.4913, IoU.pole: 0.2321, IoU.land: 0.0029, IoU.bannister: 0.1331, IoU.escalator: 0.4198, IoU.ottoman: 0.4887, IoU.bottle: 0.3558, IoU.buffet: 0.3279, IoU.poster: 0.3111, IoU.stage: 0.1504, IoU.van: 0.4071, IoU.ship: 0.5744, IoU.fountain: 0.2384, IoU.conveyer belt: 0.8318, IoU.canopy: 0.1126, IoU.washer: 0.7529, IoU.plaything: 0.2063, IoU.swimming pool: 0.7378, IoU.stool: 0.3775, IoU.barrel: 0.5736, IoU.basket: 0.3542, IoU.waterfall: 0.6580, IoU.tent: 0.9617, IoU.bag: 0.1764, IoU.minibike: 0.6720, IoU.cradle: 0.8130, IoU.oven: 0.4345, IoU.ball: 0.4400, IoU.food: 0.4905, IoU.step: 0.1835, IoU.tank: 0.5395, IoU.trade name: 0.2594, IoU.microwave: 0.7608, IoU.pot: 0.4173, IoU.animal: 0.5385, IoU.bicycle: 0.5683, IoU.lake: 0.4891, IoU.dishwasher: 0.6042, IoU.screen: 0.6409, IoU.blanket: 0.1547, IoU.sculpture: 0.6465, IoU.hood: 0.6195, IoU.sconce: 0.4840, IoU.vase: 0.4052, IoU.traffic light: 0.2731, IoU.tray: 0.0732, IoU.ashcan: 0.4186, IoU.fan: 0.5798, IoU.pier: 0.2027, IoU.crt screen: 0.0219, IoU.plate: 0.5215, IoU.monitor: 0.0440, IoU.bulletin board: 0.4057, IoU.shower: 0.0235, IoU.radiator: 0.6835, IoU.glass: 0.1472, IoU.clock: 0.3525, IoU.flag: 0.4407, Acc.wall: 0.8874, Acc.building: 0.9155, Acc.sky: 0.9720, Acc.floor: 0.9008, Acc.tree: 0.8852, Acc.ceiling: 0.9150, Acc.road: 0.8905, Acc.bed : 0.9599, Acc.windowpane: 0.7874, Acc.grass: 0.8393, Acc.cabinet: 0.7451, Acc.sidewalk: 0.7989, Acc.person: 0.9307, Acc.earth: 0.5242, Acc.door: 0.6314, Acc.table: 0.7692, Acc.mountain: 0.6816, Acc.plant: 0.6140, Acc.curtain: 0.8566, Acc.chair: 0.7100, Acc.car: 0.9234, Acc.water: 0.7069, Acc.painting: 0.8681, Acc.sofa: 0.8368, Acc.shelf: 0.5891, Acc.house: 0.6284, Acc.sea: 0.8292, Acc.mirror: 0.7386, Acc.rug: 0.7729, Acc.field: 0.4052, Acc.armchair: 0.6087, Acc.seat: 0.8064, Acc.fence: 0.4763, Acc.desk: 0.7027, Acc.rock: 0.5230, Acc.wardrobe: 0.7003, Acc.lamp: 0.7766, Acc.bathtub: 0.8584, Acc.railing: 0.4346, Acc.cushion: 0.7333, Acc.base: 0.4013, Acc.box: 0.3831, Acc.column: 0.5641, Acc.signboard: 0.5230, Acc.chest of drawers: 0.5885, Acc.counter: 0.4414, Acc.sand: 0.7131, Acc.sink: 0.8076, Acc.skyscraper: 0.5839, Acc.fireplace: 0.8895, Acc.refrigerator: 0.8461, Acc.grandstand: 0.6619, Acc.path: 0.3465, Acc.stairs: 0.4067, Acc.runway: 0.8296, Acc.case: 0.5733, Acc.pool table: 0.9680, Acc.pillow: 0.6858, Acc.screen door: 0.8640, Acc.stairway: 0.3672, Acc.river: 0.2555, Acc.bridge: 0.8516, Acc.bookcase: 0.5513, Acc.blind: 0.4742, Acc.coffee table: 0.8324, Acc.toilet: 0.9204, Acc.flower: 0.5206, Acc.book: 0.6751, Acc.hill: 0.1645, Acc.bench: 0.5138, Acc.countertop: 0.8243, Acc.stove: 0.7765, Acc.palm: 0.7475, Acc.kitchen island: 0.7183, Acc.computer: 0.7290, Acc.swivel chair: 0.6603, Acc.boat: 0.5431, Acc.bar: 0.5956, Acc.arcade machine: 0.4925, Acc.hovel: 0.6227, Acc.bus: 0.9587, Acc.towel: 0.7464, Acc.light: 0.6368, Acc.truck: 0.4205, Acc.tower: 0.0868, Acc.chandelier: 0.8095, Acc.awning: 0.3489, Acc.streetlight: 0.3892, Acc.booth: 0.5907, Acc.television receiver: 0.7964, Acc.airplane: 0.7464, Acc.dirt track: 0.1743, Acc.apparel: 0.6384, Acc.pole: 0.2949, Acc.land: 0.0041, Acc.bannister: 0.1662, Acc.escalator: 0.5618, Acc.ottoman: 0.6268, Acc.bottle: 0.6156, Acc.buffet: 0.3649, Acc.poster: 0.4368, Acc.stage: 0.1990, Acc.van: 0.5818, Acc.ship: 0.8276, Acc.fountain: 0.2411, Acc.conveyer belt: 0.9607, Acc.canopy: 0.1661, Acc.washer: 0.7649, Acc.plaything: 0.3042, Acc.swimming pool: 0.8162, Acc.stool: 0.5235, Acc.barrel: 0.7556, Acc.basket: 0.4638, Acc.waterfall: 0.9170, Acc.tent: 0.9829, Acc.bag: 0.2168, Acc.minibike: 0.8450, Acc.cradle: 0.9785, Acc.oven: 0.6501, Acc.ball: 0.6098, Acc.food: 0.5514, Acc.step: 0.2424, Acc.tank: 0.6282, Acc.trade name: 0.2960, Acc.microwave: 0.8698, Acc.pot: 0.4697, Acc.animal: 0.5584, Acc.bicycle: 0.7891, Acc.lake: 0.5433, Acc.dishwasher: 0.7494, Acc.screen: 0.8983, Acc.blanket: 0.1952, Acc.sculpture: 0.7759, Acc.hood: 0.7239, Acc.sconce: 0.5817, Acc.vase: 0.5548, Acc.traffic light: 0.3941, Acc.tray: 0.1481, Acc.ashcan: 0.5825, Acc.fan: 0.7606, Acc.pier: 0.2159, Acc.crt screen: 0.0751, Acc.plate: 0.6926, Acc.monitor: 0.0490, Acc.bulletin board: 0.4714, Acc.shower: 0.0245, Acc.radiator: 0.7847, Acc.glass: 0.1597, Acc.clock: 0.4223, Acc.flag: 0.4739
2023-11-11 10:12:34,468 - mmseg - INFO - Iter [120050/160000]	lr: 1.461e-05, eta: 7:46:50, time: 2.459, data_time: 1.816, memory: 23129, decode.loss_ce: 0.1843, decode.acc_seg: 92.3755, loss: 0.1843
2023-11-11 10:13:10,554 - mmseg - INFO - Iter [120100/160000]	lr: 1.458e-05, eta: 7:46:16, time: 0.721, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1842, decode.acc_seg: 92.3381, loss: 0.1842
2023-11-11 10:13:45,697 - mmseg - INFO - Iter [120150/160000]	lr: 1.454e-05, eta: 7:45:41, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1917, decode.acc_seg: 91.9528, loss: 0.1917
2023-11-11 10:14:20,833 - mmseg - INFO - Iter [120200/160000]	lr: 1.451e-05, eta: 7:45:06, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1842, decode.acc_seg: 92.2039, loss: 0.1842
2023-11-11 10:14:55,992 - mmseg - INFO - Iter [120250/160000]	lr: 1.447e-05, eta: 7:44:31, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1857, decode.acc_seg: 92.1679, loss: 0.1857
2023-11-11 10:15:30,910 - mmseg - INFO - Iter [120300/160000]	lr: 1.444e-05, eta: 7:43:55, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1672, decode.acc_seg: 92.9706, loss: 0.1672
2023-11-11 10:16:02,758 - mmseg - INFO - Iter [120350/160000]	lr: 1.440e-05, eta: 7:43:19, time: 0.638, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1789, decode.acc_seg: 92.3579, loss: 0.1789
2023-11-11 10:16:35,852 - mmseg - INFO - Iter [120400/160000]	lr: 1.437e-05, eta: 7:42:44, time: 0.661, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1778, decode.acc_seg: 92.4851, loss: 0.1778
2023-11-11 10:17:07,883 - mmseg - INFO - Iter [120450/160000]	lr: 1.433e-05, eta: 7:42:08, time: 0.641, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1771, decode.acc_seg: 92.3983, loss: 0.1771
2023-11-11 10:17:42,168 - mmseg - INFO - Iter [120500/160000]	lr: 1.430e-05, eta: 7:41:32, time: 0.684, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1807, decode.acc_seg: 92.3987, loss: 0.1807
2023-11-11 10:18:17,874 - mmseg - INFO - Iter [120550/160000]	lr: 1.427e-05, eta: 7:40:57, time: 0.715, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1817, decode.acc_seg: 92.4490, loss: 0.1817
2023-11-11 10:18:51,767 - mmseg - INFO - Iter [120600/160000]	lr: 1.423e-05, eta: 7:40:22, time: 0.677, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1798, decode.acc_seg: 92.4864, loss: 0.1798
2023-11-11 10:19:26,751 - mmseg - INFO - Iter [120650/160000]	lr: 1.420e-05, eta: 7:39:47, time: 0.701, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1824, decode.acc_seg: 92.6231, loss: 0.1824
2023-11-11 10:20:02,116 - mmseg - INFO - Iter [120700/160000]	lr: 1.416e-05, eta: 7:39:12, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1895, decode.acc_seg: 92.0886, loss: 0.1895
2023-11-11 10:20:37,602 - mmseg - INFO - Iter [120750/160000]	lr: 1.413e-05, eta: 7:38:37, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1843, decode.acc_seg: 92.2123, loss: 0.1843
2023-11-11 10:21:12,896 - mmseg - INFO - Iter [120800/160000]	lr: 1.409e-05, eta: 7:38:02, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1766, decode.acc_seg: 92.5144, loss: 0.1766
2023-11-11 10:21:47,280 - mmseg - INFO - Iter [120850/160000]	lr: 1.406e-05, eta: 7:37:27, time: 0.689, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1919, decode.acc_seg: 92.0691, loss: 0.1919
2023-11-11 10:22:19,135 - mmseg - INFO - Iter [120900/160000]	lr: 1.403e-05, eta: 7:36:51, time: 0.637, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1761, decode.acc_seg: 92.6183, loss: 0.1761
2023-11-11 10:22:51,681 - mmseg - INFO - Iter [120950/160000]	lr: 1.399e-05, eta: 7:36:15, time: 0.651, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1802, decode.acc_seg: 92.3412, loss: 0.1802
2023-11-11 10:23:26,304 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 10:23:26,304 - mmseg - INFO - Iter [121000/160000]	lr: 1.396e-05, eta: 7:35:40, time: 0.692, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1878, decode.acc_seg: 92.2617, loss: 0.1878
2023-11-11 10:23:57,846 - mmseg - INFO - Iter [121050/160000]	lr: 1.392e-05, eta: 7:35:04, time: 0.630, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1776, decode.acc_seg: 92.5456, loss: 0.1776
2023-11-11 10:24:29,322 - mmseg - INFO - Iter [121100/160000]	lr: 1.389e-05, eta: 7:34:27, time: 0.630, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1777, decode.acc_seg: 92.6701, loss: 0.1777
2023-11-11 10:25:03,437 - mmseg - INFO - Iter [121150/160000]	lr: 1.386e-05, eta: 7:33:52, time: 0.681, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1783, decode.acc_seg: 92.5680, loss: 0.1783
2023-11-11 10:25:39,852 - mmseg - INFO - Iter [121200/160000]	lr: 1.382e-05, eta: 7:33:17, time: 0.729, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1870, decode.acc_seg: 92.2422, loss: 0.1870
2023-11-11 10:26:15,204 - mmseg - INFO - Iter [121250/160000]	lr: 1.379e-05, eta: 7:32:42, time: 0.707, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1764, decode.acc_seg: 92.5456, loss: 0.1764
2023-11-11 10:26:48,729 - mmseg - INFO - Iter [121300/160000]	lr: 1.375e-05, eta: 7:32:07, time: 0.671, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1810, decode.acc_seg: 92.2848, loss: 0.1810
2023-11-11 10:27:21,930 - mmseg - INFO - Iter [121350/160000]	lr: 1.372e-05, eta: 7:31:31, time: 0.664, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1707, decode.acc_seg: 92.5393, loss: 0.1707
2023-11-11 10:27:54,212 - mmseg - INFO - Iter [121400/160000]	lr: 1.369e-05, eta: 7:30:55, time: 0.646, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1698, decode.acc_seg: 92.7663, loss: 0.1698
2023-11-11 10:28:26,100 - mmseg - INFO - Iter [121450/160000]	lr: 1.365e-05, eta: 7:30:19, time: 0.637, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1871, decode.acc_seg: 92.2158, loss: 0.1871
2023-11-11 10:28:58,443 - mmseg - INFO - Iter [121500/160000]	lr: 1.362e-05, eta: 7:29:43, time: 0.647, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1739, decode.acc_seg: 92.5658, loss: 0.1739
2023-11-11 10:29:32,756 - mmseg - INFO - Iter [121550/160000]	lr: 1.359e-05, eta: 7:29:08, time: 0.685, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1831, decode.acc_seg: 92.3677, loss: 0.1831
2023-11-11 10:30:07,769 - mmseg - INFO - Iter [121600/160000]	lr: 1.355e-05, eta: 7:28:33, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1881, decode.acc_seg: 92.2221, loss: 0.1881
2023-11-11 10:30:39,745 - mmseg - INFO - Iter [121650/160000]	lr: 1.352e-05, eta: 7:27:57, time: 0.641, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1660, decode.acc_seg: 93.0352, loss: 0.1660
2023-11-11 10:31:13,419 - mmseg - INFO - Iter [121700/160000]	lr: 1.349e-05, eta: 7:27:22, time: 0.672, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1850, decode.acc_seg: 92.3495, loss: 0.1850
2023-11-11 10:31:47,293 - mmseg - INFO - Iter [121750/160000]	lr: 1.345e-05, eta: 7:26:46, time: 0.677, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1787, decode.acc_seg: 92.6749, loss: 0.1787
2023-11-11 10:32:22,865 - mmseg - INFO - Iter [121800/160000]	lr: 1.342e-05, eta: 7:26:11, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1767, decode.acc_seg: 92.6018, loss: 0.1767
2023-11-11 10:32:58,662 - mmseg - INFO - Iter [121850/160000]	lr: 1.338e-05, eta: 7:25:36, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1720, decode.acc_seg: 92.7194, loss: 0.1720
2023-11-11 10:33:34,676 - mmseg - INFO - Iter [121900/160000]	lr: 1.335e-05, eta: 7:25:02, time: 0.720, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1852, decode.acc_seg: 92.1957, loss: 0.1852
2023-11-11 10:34:10,152 - mmseg - INFO - Iter [121950/160000]	lr: 1.332e-05, eta: 7:24:27, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1766, decode.acc_seg: 92.3327, loss: 0.1766
2023-11-11 10:34:45,272 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 10:34:45,273 - mmseg - INFO - Iter [122000/160000]	lr: 1.328e-05, eta: 7:23:52, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1831, decode.acc_seg: 92.2808, loss: 0.1831
2023-11-11 10:35:20,688 - mmseg - INFO - Iter [122050/160000]	lr: 1.325e-05, eta: 7:23:17, time: 0.708, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1828, decode.acc_seg: 92.2754, loss: 0.1828
2023-11-11 10:35:56,356 - mmseg - INFO - Iter [122100/160000]	lr: 1.322e-05, eta: 7:22:42, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1734, decode.acc_seg: 92.5126, loss: 0.1734
2023-11-11 10:36:31,783 - mmseg - INFO - Iter [122150/160000]	lr: 1.318e-05, eta: 7:22:07, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1746, decode.acc_seg: 92.5866, loss: 0.1746
2023-11-11 10:37:07,389 - mmseg - INFO - Iter [122200/160000]	lr: 1.315e-05, eta: 7:21:32, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1759, decode.acc_seg: 92.4128, loss: 0.1759
2023-11-11 10:37:43,064 - mmseg - INFO - Iter [122250/160000]	lr: 1.312e-05, eta: 7:20:57, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1786, decode.acc_seg: 92.4134, loss: 0.1786
2023-11-11 10:38:15,542 - mmseg - INFO - Iter [122300/160000]	lr: 1.309e-05, eta: 7:20:22, time: 0.650, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1849, decode.acc_seg: 92.4140, loss: 0.1849
2023-11-11 10:38:48,980 - mmseg - INFO - Iter [122350/160000]	lr: 1.305e-05, eta: 7:19:46, time: 0.668, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1819, decode.acc_seg: 92.4685, loss: 0.1819
2023-11-11 10:39:24,240 - mmseg - INFO - Iter [122400/160000]	lr: 1.302e-05, eta: 7:19:11, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1803, decode.acc_seg: 92.4515, loss: 0.1803
2023-11-11 10:39:59,009 - mmseg - INFO - Iter [122450/160000]	lr: 1.299e-05, eta: 7:18:36, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1821, decode.acc_seg: 92.4347, loss: 0.1821
2023-11-11 10:40:32,288 - mmseg - INFO - Iter [122500/160000]	lr: 1.295e-05, eta: 7:18:00, time: 0.666, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1793, decode.acc_seg: 92.4315, loss: 0.1793
2023-11-11 10:41:05,831 - mmseg - INFO - Iter [122550/160000]	lr: 1.292e-05, eta: 7:17:25, time: 0.670, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1651, decode.acc_seg: 92.9031, loss: 0.1651
2023-11-11 10:41:40,655 - mmseg - INFO - Iter [122600/160000]	lr: 1.289e-05, eta: 7:16:50, time: 0.698, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1902, decode.acc_seg: 92.3083, loss: 0.1902
2023-11-11 10:42:14,735 - mmseg - INFO - Iter [122650/160000]	lr: 1.285e-05, eta: 7:16:14, time: 0.680, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1911, decode.acc_seg: 91.8887, loss: 0.1911
2023-11-11 10:42:51,779 - mmseg - INFO - Iter [122700/160000]	lr: 1.282e-05, eta: 7:15:40, time: 0.741, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1820, decode.acc_seg: 92.2678, loss: 0.1820
2023-11-11 10:43:24,027 - mmseg - INFO - Iter [122750/160000]	lr: 1.279e-05, eta: 7:15:04, time: 0.646, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1864, decode.acc_seg: 92.3221, loss: 0.1864
2023-11-11 10:43:56,562 - mmseg - INFO - Iter [122800/160000]	lr: 1.276e-05, eta: 7:14:28, time: 0.651, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1848, decode.acc_seg: 92.1316, loss: 0.1848
2023-11-11 10:44:30,840 - mmseg - INFO - Iter [122850/160000]	lr: 1.272e-05, eta: 7:13:53, time: 0.684, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1826, decode.acc_seg: 92.1719, loss: 0.1826
2023-11-11 10:45:05,942 - mmseg - INFO - Iter [122900/160000]	lr: 1.269e-05, eta: 7:13:18, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1796, decode.acc_seg: 92.4459, loss: 0.1796
2023-11-11 10:45:40,242 - mmseg - INFO - Iter [122950/160000]	lr: 1.266e-05, eta: 7:12:43, time: 0.687, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1727, decode.acc_seg: 92.3851, loss: 0.1727
2023-11-11 10:46:11,789 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 10:46:11,789 - mmseg - INFO - Iter [123000/160000]	lr: 1.263e-05, eta: 7:12:07, time: 0.631, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1734, decode.acc_seg: 92.6914, loss: 0.1734
2023-11-11 10:46:45,350 - mmseg - INFO - Iter [123050/160000]	lr: 1.259e-05, eta: 7:11:31, time: 0.671, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1851, decode.acc_seg: 92.3560, loss: 0.1851
2023-11-11 10:47:19,918 - mmseg - INFO - Iter [123100/160000]	lr: 1.256e-05, eta: 7:10:56, time: 0.691, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1726, decode.acc_seg: 92.6624, loss: 0.1726
2023-11-11 10:47:55,592 - mmseg - INFO - Iter [123150/160000]	lr: 1.253e-05, eta: 7:10:21, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1825, decode.acc_seg: 92.2902, loss: 0.1825
2023-11-11 10:48:31,064 - mmseg - INFO - Iter [123200/160000]	lr: 1.250e-05, eta: 7:09:46, time: 0.710, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1835, decode.acc_seg: 92.2594, loss: 0.1835
2023-11-11 10:49:06,190 - mmseg - INFO - Iter [123250/160000]	lr: 1.246e-05, eta: 7:09:11, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1749, decode.acc_seg: 92.6320, loss: 0.1749
2023-11-11 10:49:41,199 - mmseg - INFO - Iter [123300/160000]	lr: 1.243e-05, eta: 7:08:36, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1798, decode.acc_seg: 92.6124, loss: 0.1798
2023-11-11 10:50:16,347 - mmseg - INFO - Iter [123350/160000]	lr: 1.240e-05, eta: 7:08:01, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1863, decode.acc_seg: 92.1094, loss: 0.1863
2023-11-11 10:50:51,525 - mmseg - INFO - Iter [123400/160000]	lr: 1.237e-05, eta: 7:07:26, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1781, decode.acc_seg: 92.4058, loss: 0.1781
2023-11-11 10:51:26,465 - mmseg - INFO - Iter [123450/160000]	lr: 1.233e-05, eta: 7:06:51, time: 0.699, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1813, decode.acc_seg: 92.2574, loss: 0.1813
2023-11-11 10:52:01,458 - mmseg - INFO - Iter [123500/160000]	lr: 1.230e-05, eta: 7:06:16, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1864, decode.acc_seg: 92.3765, loss: 0.1864
2023-11-11 10:52:34,432 - mmseg - INFO - Iter [123550/160000]	lr: 1.227e-05, eta: 7:05:40, time: 0.660, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1815, decode.acc_seg: 92.4212, loss: 0.1815
2023-11-11 10:53:09,578 - mmseg - INFO - Iter [123600/160000]	lr: 1.224e-05, eta: 7:05:05, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1785, decode.acc_seg: 92.4497, loss: 0.1785
2023-11-11 10:53:44,609 - mmseg - INFO - Iter [123650/160000]	lr: 1.220e-05, eta: 7:04:30, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1750, decode.acc_seg: 92.5816, loss: 0.1750
2023-11-11 10:54:19,622 - mmseg - INFO - Iter [123700/160000]	lr: 1.217e-05, eta: 7:03:55, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1757, decode.acc_seg: 92.6347, loss: 0.1757
2023-11-11 10:54:54,136 - mmseg - INFO - Iter [123750/160000]	lr: 1.214e-05, eta: 7:03:20, time: 0.691, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1792, decode.acc_seg: 92.2187, loss: 0.1792
2023-11-11 10:55:27,383 - mmseg - INFO - Iter [123800/160000]	lr: 1.211e-05, eta: 7:02:45, time: 0.664, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1775, decode.acc_seg: 92.4141, loss: 0.1775
2023-11-11 10:55:59,557 - mmseg - INFO - Iter [123850/160000]	lr: 1.208e-05, eta: 7:02:09, time: 0.645, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1732, decode.acc_seg: 92.6691, loss: 0.1732
2023-11-11 10:56:34,121 - mmseg - INFO - Iter [123900/160000]	lr: 1.204e-05, eta: 7:01:34, time: 0.690, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1783, decode.acc_seg: 92.4636, loss: 0.1783
2023-11-11 10:57:08,239 - mmseg - INFO - Iter [123950/160000]	lr: 1.201e-05, eta: 7:00:58, time: 0.682, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1845, decode.acc_seg: 92.3037, loss: 0.1845
2023-11-11 10:57:43,706 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 10:57:43,707 - mmseg - INFO - Iter [124000/160000]	lr: 1.198e-05, eta: 7:00:23, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1749, decode.acc_seg: 92.6378, loss: 0.1749
2023-11-11 10:58:17,702 - mmseg - INFO - Iter [124050/160000]	lr: 1.195e-05, eta: 6:59:48, time: 0.680, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1727, decode.acc_seg: 92.9764, loss: 0.1727
2023-11-11 10:58:52,724 - mmseg - INFO - Iter [124100/160000]	lr: 1.192e-05, eta: 6:59:13, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1780, decode.acc_seg: 92.4096, loss: 0.1780
2023-11-11 10:59:27,659 - mmseg - INFO - Iter [124150/160000]	lr: 1.188e-05, eta: 6:58:38, time: 0.699, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1777, decode.acc_seg: 92.4914, loss: 0.1777
2023-11-11 11:00:02,738 - mmseg - INFO - Iter [124200/160000]	lr: 1.185e-05, eta: 6:58:03, time: 0.701, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1829, decode.acc_seg: 92.0038, loss: 0.1829
2023-11-11 11:00:36,294 - mmseg - INFO - Iter [124250/160000]	lr: 1.182e-05, eta: 6:57:27, time: 0.671, data_time: 0.012, memory: 23129, decode.loss_ce: 0.1864, decode.acc_seg: 92.0289, loss: 0.1864
2023-11-11 11:01:11,463 - mmseg - INFO - Iter [124300/160000]	lr: 1.179e-05, eta: 6:56:52, time: 0.704, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1877, decode.acc_seg: 92.1088, loss: 0.1877
2023-11-11 11:01:47,070 - mmseg - INFO - Iter [124350/160000]	lr: 1.176e-05, eta: 6:56:18, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1780, decode.acc_seg: 92.4680, loss: 0.1780
2023-11-11 11:02:22,794 - mmseg - INFO - Iter [124400/160000]	lr: 1.173e-05, eta: 6:55:43, time: 0.715, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1754, decode.acc_seg: 92.5633, loss: 0.1754
2023-11-11 11:02:57,898 - mmseg - INFO - Iter [124450/160000]	lr: 1.169e-05, eta: 6:55:08, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1857, decode.acc_seg: 92.0858, loss: 0.1857
2023-11-11 11:03:33,073 - mmseg - INFO - Iter [124500/160000]	lr: 1.166e-05, eta: 6:54:33, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1779, decode.acc_seg: 92.3704, loss: 0.1779
2023-11-11 11:04:08,063 - mmseg - INFO - Iter [124550/160000]	lr: 1.163e-05, eta: 6:53:58, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1716, decode.acc_seg: 92.7175, loss: 0.1716
2023-11-11 11:04:41,961 - mmseg - INFO - Iter [124600/160000]	lr: 1.160e-05, eta: 6:53:22, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1908, decode.acc_seg: 92.2044, loss: 0.1908
2023-11-11 11:05:15,109 - mmseg - INFO - Iter [124650/160000]	lr: 1.157e-05, eta: 6:52:47, time: 0.663, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1768, decode.acc_seg: 92.6656, loss: 0.1768
2023-11-11 11:05:49,069 - mmseg - INFO - Iter [124700/160000]	lr: 1.154e-05, eta: 6:52:11, time: 0.679, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1791, decode.acc_seg: 92.5609, loss: 0.1791
2023-11-11 11:06:22,958 - mmseg - INFO - Iter [124750/160000]	lr: 1.151e-05, eta: 6:51:36, time: 0.677, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1676, decode.acc_seg: 92.6959, loss: 0.1676
2023-11-11 11:06:57,046 - mmseg - INFO - Iter [124800/160000]	lr: 1.147e-05, eta: 6:51:01, time: 0.683, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1747, decode.acc_seg: 92.7348, loss: 0.1747
2023-11-11 11:07:29,599 - mmseg - INFO - Iter [124850/160000]	lr: 1.144e-05, eta: 6:50:25, time: 0.650, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1767, decode.acc_seg: 92.4788, loss: 0.1767
2023-11-11 11:08:05,604 - mmseg - INFO - Iter [124900/160000]	lr: 1.141e-05, eta: 6:49:50, time: 0.720, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1815, decode.acc_seg: 92.4180, loss: 0.1815
2023-11-11 11:08:39,676 - mmseg - INFO - Iter [124950/160000]	lr: 1.138e-05, eta: 6:49:15, time: 0.683, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1804, decode.acc_seg: 92.3329, loss: 0.1804
2023-11-11 11:09:12,583 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 11:09:12,583 - mmseg - INFO - Iter [125000/160000]	lr: 1.135e-05, eta: 6:48:39, time: 0.658, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1703, decode.acc_seg: 92.6357, loss: 0.1703
2023-11-11 11:09:45,923 - mmseg - INFO - Iter [125050/160000]	lr: 1.132e-05, eta: 6:48:04, time: 0.666, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1838, decode.acc_seg: 92.0616, loss: 0.1838
2023-11-11 11:10:20,799 - mmseg - INFO - Iter [125100/160000]	lr: 1.129e-05, eta: 6:47:29, time: 0.697, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1646, decode.acc_seg: 92.8592, loss: 0.1646
2023-11-11 11:10:56,325 - mmseg - INFO - Iter [125150/160000]	lr: 1.126e-05, eta: 6:46:54, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1759, decode.acc_seg: 92.7451, loss: 0.1759
2023-11-11 11:11:31,774 - mmseg - INFO - Iter [125200/160000]	lr: 1.123e-05, eta: 6:46:19, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1800, decode.acc_seg: 92.6047, loss: 0.1800
2023-11-11 11:12:07,457 - mmseg - INFO - Iter [125250/160000]	lr: 1.119e-05, eta: 6:45:44, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1749, decode.acc_seg: 92.3819, loss: 0.1749
2023-11-11 11:12:39,356 - mmseg - INFO - Iter [125300/160000]	lr: 1.116e-05, eta: 6:45:08, time: 0.639, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1769, decode.acc_seg: 92.5702, loss: 0.1769
2023-11-11 11:13:11,013 - mmseg - INFO - Iter [125350/160000]	lr: 1.113e-05, eta: 6:44:32, time: 0.633, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1709, decode.acc_seg: 92.7409, loss: 0.1709
2023-11-11 11:13:44,755 - mmseg - INFO - Iter [125400/160000]	lr: 1.110e-05, eta: 6:43:57, time: 0.675, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1651, decode.acc_seg: 92.8874, loss: 0.1651
2023-11-11 11:14:18,652 - mmseg - INFO - Iter [125450/160000]	lr: 1.107e-05, eta: 6:43:22, time: 0.677, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1701, decode.acc_seg: 92.5775, loss: 0.1701
2023-11-11 11:14:53,909 - mmseg - INFO - Iter [125500/160000]	lr: 1.104e-05, eta: 6:42:47, time: 0.705, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1720, decode.acc_seg: 92.7896, loss: 0.1720
2023-11-11 11:15:26,681 - mmseg - INFO - Iter [125550/160000]	lr: 1.101e-05, eta: 6:42:11, time: 0.655, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1861, decode.acc_seg: 92.1424, loss: 0.1861
2023-11-11 11:16:00,856 - mmseg - INFO - Iter [125600/160000]	lr: 1.098e-05, eta: 6:41:36, time: 0.685, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1815, decode.acc_seg: 92.4394, loss: 0.1815
2023-11-11 11:16:36,366 - mmseg - INFO - Iter [125650/160000]	lr: 1.095e-05, eta: 6:41:01, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1911, decode.acc_seg: 92.0399, loss: 0.1911
2023-11-11 11:17:12,198 - mmseg - INFO - Iter [125700/160000]	lr: 1.092e-05, eta: 6:40:26, time: 0.717, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1675, decode.acc_seg: 92.8308, loss: 0.1675
2023-11-11 11:17:47,514 - mmseg - INFO - Iter [125750/160000]	lr: 1.089e-05, eta: 6:39:51, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1672, decode.acc_seg: 92.9486, loss: 0.1672
2023-11-11 11:18:21,570 - mmseg - INFO - Iter [125800/160000]	lr: 1.086e-05, eta: 6:39:16, time: 0.682, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1689, decode.acc_seg: 92.8838, loss: 0.1689
2023-11-11 11:18:53,158 - mmseg - INFO - Iter [125850/160000]	lr: 1.083e-05, eta: 6:38:40, time: 0.632, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1739, decode.acc_seg: 92.6934, loss: 0.1739
2023-11-11 11:19:25,677 - mmseg - INFO - Iter [125900/160000]	lr: 1.080e-05, eta: 6:38:04, time: 0.649, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1737, decode.acc_seg: 92.4556, loss: 0.1737
2023-11-11 11:20:00,449 - mmseg - INFO - Iter [125950/160000]	lr: 1.077e-05, eta: 6:37:29, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1722, decode.acc_seg: 92.6673, loss: 0.1722
2023-11-11 11:20:35,136 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 11:20:35,137 - mmseg - INFO - Iter [126000/160000]	lr: 1.073e-05, eta: 6:36:54, time: 0.694, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1670, decode.acc_seg: 92.8005, loss: 0.1670
2023-11-11 11:21:10,024 - mmseg - INFO - Iter [126050/160000]	lr: 1.070e-05, eta: 6:36:19, time: 0.698, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1658, decode.acc_seg: 92.9093, loss: 0.1658
2023-11-11 11:21:44,470 - mmseg - INFO - Iter [126100/160000]	lr: 1.067e-05, eta: 6:35:44, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1742, decode.acc_seg: 92.6071, loss: 0.1742
2023-11-11 11:22:16,348 - mmseg - INFO - Iter [126150/160000]	lr: 1.064e-05, eta: 6:35:08, time: 0.638, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1802, decode.acc_seg: 92.2903, loss: 0.1802
2023-11-11 11:22:51,479 - mmseg - INFO - Iter [126200/160000]	lr: 1.061e-05, eta: 6:34:33, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1668, decode.acc_seg: 92.9527, loss: 0.1668
2023-11-11 11:23:23,004 - mmseg - INFO - Iter [126250/160000]	lr: 1.058e-05, eta: 6:33:57, time: 0.631, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1759, decode.acc_seg: 92.4613, loss: 0.1759
2023-11-11 11:23:54,838 - mmseg - INFO - Iter [126300/160000]	lr: 1.055e-05, eta: 6:33:21, time: 0.638, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1740, decode.acc_seg: 92.5642, loss: 0.1740
2023-11-11 11:24:28,385 - mmseg - INFO - Iter [126350/160000]	lr: 1.052e-05, eta: 6:32:46, time: 0.671, data_time: 0.053, memory: 23129, decode.loss_ce: 0.1845, decode.acc_seg: 92.4132, loss: 0.1845
2023-11-11 11:25:03,305 - mmseg - INFO - Iter [126400/160000]	lr: 1.049e-05, eta: 6:32:11, time: 0.697, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1841, decode.acc_seg: 92.4383, loss: 0.1841
2023-11-11 11:25:38,599 - mmseg - INFO - Iter [126450/160000]	lr: 1.046e-05, eta: 6:31:36, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1875, decode.acc_seg: 92.3245, loss: 0.1875
2023-11-11 11:26:11,844 - mmseg - INFO - Iter [126500/160000]	lr: 1.043e-05, eta: 6:31:00, time: 0.666, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1669, decode.acc_seg: 92.9232, loss: 0.1669
2023-11-11 11:26:45,014 - mmseg - INFO - Iter [126550/160000]	lr: 1.040e-05, eta: 6:30:25, time: 0.662, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1664, decode.acc_seg: 92.8112, loss: 0.1664
2023-11-11 11:27:20,385 - mmseg - INFO - Iter [126600/160000]	lr: 1.037e-05, eta: 6:29:50, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1718, decode.acc_seg: 92.7181, loss: 0.1718
2023-11-11 11:27:55,990 - mmseg - INFO - Iter [126650/160000]	lr: 1.034e-05, eta: 6:29:15, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1769, decode.acc_seg: 92.6495, loss: 0.1769
2023-11-11 11:28:31,493 - mmseg - INFO - Iter [126700/160000]	lr: 1.031e-05, eta: 6:28:40, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1651, decode.acc_seg: 92.8534, loss: 0.1651
2023-11-11 11:29:06,682 - mmseg - INFO - Iter [126750/160000]	lr: 1.028e-05, eta: 6:28:05, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1747, decode.acc_seg: 92.5321, loss: 0.1747
2023-11-11 11:29:38,350 - mmseg - INFO - Iter [126800/160000]	lr: 1.025e-05, eta: 6:27:29, time: 0.635, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1747, decode.acc_seg: 92.6233, loss: 0.1747
2023-11-11 11:30:13,579 - mmseg - INFO - Iter [126850/160000]	lr: 1.022e-05, eta: 6:26:54, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1795, decode.acc_seg: 92.4778, loss: 0.1795
2023-11-11 11:30:48,914 - mmseg - INFO - Iter [126900/160000]	lr: 1.019e-05, eta: 6:26:19, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1708, decode.acc_seg: 92.6970, loss: 0.1708
2023-11-11 11:31:24,418 - mmseg - INFO - Iter [126950/160000]	lr: 1.016e-05, eta: 6:25:44, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1776, decode.acc_seg: 92.4574, loss: 0.1776
2023-11-11 11:31:56,641 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 11:31:56,641 - mmseg - INFO - Iter [127000/160000]	lr: 1.013e-05, eta: 6:25:09, time: 0.646, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1648, decode.acc_seg: 92.8697, loss: 0.1648
2023-11-11 11:32:28,442 - mmseg - INFO - Iter [127050/160000]	lr: 1.010e-05, eta: 6:24:33, time: 0.636, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1760, decode.acc_seg: 92.5343, loss: 0.1760
2023-11-11 11:33:00,314 - mmseg - INFO - Iter [127100/160000]	lr: 1.008e-05, eta: 6:23:57, time: 0.637, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1701, decode.acc_seg: 92.7289, loss: 0.1701
2023-11-11 11:33:34,356 - mmseg - INFO - Iter [127150/160000]	lr: 1.005e-05, eta: 6:23:22, time: 0.680, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1755, decode.acc_seg: 92.7253, loss: 0.1755
2023-11-11 11:34:07,766 - mmseg - INFO - Iter [127200/160000]	lr: 1.002e-05, eta: 6:22:46, time: 0.668, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1751, decode.acc_seg: 92.6376, loss: 0.1751
2023-11-11 11:34:42,561 - mmseg - INFO - Iter [127250/160000]	lr: 9.987e-06, eta: 6:22:11, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1741, decode.acc_seg: 92.7140, loss: 0.1741
2023-11-11 11:35:15,715 - mmseg - INFO - Iter [127300/160000]	lr: 9.957e-06, eta: 6:21:36, time: 0.662, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1789, decode.acc_seg: 92.4903, loss: 0.1789
2023-11-11 11:35:51,000 - mmseg - INFO - Iter [127350/160000]	lr: 9.928e-06, eta: 6:21:01, time: 0.706, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1758, decode.acc_seg: 92.7205, loss: 0.1758
2023-11-11 11:36:25,957 - mmseg - INFO - Iter [127400/160000]	lr: 9.899e-06, eta: 6:20:26, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1704, decode.acc_seg: 92.8260, loss: 0.1704
2023-11-11 11:37:01,074 - mmseg - INFO - Iter [127450/160000]	lr: 9.869e-06, eta: 6:19:51, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1769, decode.acc_seg: 92.5706, loss: 0.1769
2023-11-11 11:37:36,186 - mmseg - INFO - Iter [127500/160000]	lr: 9.840e-06, eta: 6:19:16, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1816, decode.acc_seg: 92.3295, loss: 0.1816
2023-11-11 11:38:11,088 - mmseg - INFO - Iter [127550/160000]	lr: 9.811e-06, eta: 6:18:41, time: 0.697, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1703, decode.acc_seg: 92.7531, loss: 0.1703
2023-11-11 11:38:46,270 - mmseg - INFO - Iter [127600/160000]	lr: 9.782e-06, eta: 6:18:06, time: 0.704, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1710, decode.acc_seg: 92.7166, loss: 0.1710
2023-11-11 11:39:21,653 - mmseg - INFO - Iter [127650/160000]	lr: 9.753e-06, eta: 6:17:31, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1674, decode.acc_seg: 92.8540, loss: 0.1674
2023-11-11 11:39:54,981 - mmseg - INFO - Iter [127700/160000]	lr: 9.724e-06, eta: 6:16:55, time: 0.668, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1820, decode.acc_seg: 92.3888, loss: 0.1820
2023-11-11 11:40:29,385 - mmseg - INFO - Iter [127750/160000]	lr: 9.694e-06, eta: 6:16:20, time: 0.687, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1729, decode.acc_seg: 92.8044, loss: 0.1729
2023-11-11 11:41:04,751 - mmseg - INFO - Iter [127800/160000]	lr: 9.665e-06, eta: 6:15:45, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1688, decode.acc_seg: 92.8753, loss: 0.1688
2023-11-11 11:41:40,318 - mmseg - INFO - Iter [127850/160000]	lr: 9.636e-06, eta: 6:15:10, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1716, decode.acc_seg: 92.6094, loss: 0.1716
2023-11-11 11:42:14,438 - mmseg - INFO - Iter [127900/160000]	lr: 9.608e-06, eta: 6:14:35, time: 0.683, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1766, decode.acc_seg: 92.4993, loss: 0.1766
2023-11-11 11:42:47,626 - mmseg - INFO - Iter [127950/160000]	lr: 9.579e-06, eta: 6:14:00, time: 0.664, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1681, decode.acc_seg: 92.8899, loss: 0.1681
2023-11-11 11:43:21,612 - mmseg - INFO - Saving checkpoint at 128000 iterations
2023-11-11 11:43:28,569 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 11:43:28,570 - mmseg - INFO - Iter [128000/160000]	lr: 9.550e-06, eta: 6:13:26, time: 0.819, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1668, decode.acc_seg: 92.8735, loss: 0.1668
2023-11-11 11:44:57,867 - mmseg - INFO - per class results:
2023-11-11 11:44:57,880 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 78.03 | 88.12 |
|       building      | 81.97 | 91.22 |
|         sky         | 94.27 | 97.66 |
|        floor        | 82.81 | 90.08 |
|         tree        | 74.48 |  88.1 |
|       ceiling       | 83.81 | 92.14 |
|         road        |  82.1 | 89.84 |
|         bed         | 89.68 | 96.48 |
|      windowpane     | 61.73 | 78.71 |
|        grass        |  65.5 | 82.59 |
|       cabinet       | 60.44 | 73.61 |
|       sidewalk      | 65.13 |  80.2 |
|        person       | 81.62 | 93.52 |
|        earth        | 37.59 | 52.44 |
|         door        | 50.02 | 63.11 |
|        table        | 62.56 | 77.19 |
|       mountain      | 56.18 | 70.81 |
|        plant        | 52.65 | 63.02 |
|       curtain       | 73.97 | 86.33 |
|        chair        | 58.34 |  72.2 |
|         car         | 83.63 | 92.03 |
|        water        | 56.25 | 70.82 |
|       painting      | 72.16 | 87.11 |
|         sofa        | 65.77 | 82.73 |
|        shelf        | 42.17 | 62.89 |
|        house        | 47.62 | 70.06 |
|         sea         | 57.26 | 76.79 |
|        mirror       | 68.35 | 76.21 |
|         rug         | 63.77 |  77.1 |
|        field        | 26.86 | 42.67 |
|       armchair      | 43.69 | 61.33 |
|         seat        | 62.93 | 81.26 |
|        fence        | 41.82 | 57.47 |
|         desk        |  50.5 | 72.43 |
|         rock        | 38.78 | 60.49 |
|       wardrobe      |  47.2 | 66.69 |
|         lamp        | 65.82 | 77.48 |
|       bathtub       | 82.78 | 88.76 |
|       railing       | 32.71 | 48.07 |
|       cushion       | 58.47 | 70.92 |
|         base        | 28.58 | 35.51 |
|         box         | 30.06 | 37.78 |
|        column       | 45.68 |  55.2 |
|      signboard      | 40.49 | 51.86 |
|   chest of drawers  | 46.15 | 59.97 |
|       counter       | 32.67 | 39.15 |
|         sand        |  49.6 | 71.98 |
|         sink        | 72.58 | 81.07 |
|      skyscraper     | 46.68 | 57.61 |
|      fireplace      | 76.05 | 90.09 |
|     refrigerator    | 77.81 | 86.52 |
|      grandstand     | 44.55 | 67.26 |
|         path        | 25.52 | 37.18 |
|        stairs       | 30.76 | 41.22 |
|        runway       | 64.42 | 82.29 |
|         case        | 38.36 |  46.2 |
|      pool table     | 93.05 | 96.31 |
|        pillow       | 60.36 | 72.12 |
|     screen door     | 65.63 |  82.6 |
|       stairway      | 29.88 | 37.95 |
|        river        | 10.51 | 23.03 |
|        bridge       | 66.36 | 80.62 |
|       bookcase      | 36.89 | 55.16 |
|        blind        | 42.42 | 47.66 |
|     coffee table    | 52.07 | 82.36 |
|        toilet       | 86.67 | 91.36 |
|        flower       | 38.91 | 52.95 |
|         book        | 46.34 | 65.64 |
|         hill        | 11.31 | 19.15 |
|        bench        | 46.67 | 52.86 |
|      countertop     | 55.65 | 85.34 |
|        stove        | 72.38 | 79.85 |
|         palm        | 49.62 | 69.52 |
|    kitchen island   |  43.2 | 76.76 |
|       computer      | 62.49 | 71.18 |
|     swivel chair    | 49.44 | 69.35 |
|         boat        | 46.97 | 55.07 |
|         bar         | 48.06 |  67.8 |
|    arcade machine   | 75.01 | 80.44 |
|        hovel        | 60.83 | 71.15 |
|         bus         | 85.94 | 96.37 |
|        towel        | 67.11 | 77.36 |
|        light        | 55.62 | 67.04 |
|        truck        | 36.57 | 45.44 |
|        tower        |  6.79 |  8.63 |
|      chandelier     | 66.89 | 83.31 |
|        awning       | 26.19 | 35.77 |
|     streetlight     | 27.79 | 37.89 |
|        booth        | 62.67 | 66.06 |
| television receiver | 66.08 | 79.92 |
|       airplane      |  55.1 | 66.94 |
|      dirt track     |  5.55 |  8.08 |
|       apparel       | 54.15 | 72.95 |
|         pole        | 22.57 | 29.12 |
|         land        |  0.06 |  0.13 |
|      bannister      | 13.13 | 16.78 |
|      escalator      | 44.93 | 59.51 |
|       ottoman       | 47.02 | 61.29 |
|        bottle       | 35.89 | 63.19 |
|        buffet       |  48.8 | 55.91 |
|        poster       | 32.19 | 40.86 |
|        stage        | 16.94 | 23.18 |
|         van         | 41.73 | 58.95 |
|         ship        | 52.11 | 75.18 |
|       fountain      | 42.13 | 42.56 |
|    conveyer belt    | 84.18 | 95.58 |
|        canopy       | 15.41 | 23.57 |
|        washer       | 75.36 | 76.53 |
|      plaything      | 20.36 | 32.15 |
|    swimming pool    | 72.43 | 81.78 |
|        stool        | 39.45 | 51.45 |
|        barrel       | 60.15 | 73.34 |
|        basket       | 35.63 | 47.51 |
|      waterfall      | 62.83 | 92.17 |
|         tent        | 95.55 | 98.05 |
|         bag         | 15.44 | 20.19 |
|       minibike      | 62.95 | 77.48 |
|        cradle       | 80.68 | 97.77 |
|         oven        | 45.18 |  68.0 |
|         ball        | 38.13 | 50.63 |
|         food        | 47.15 | 51.72 |
|         step        | 13.19 | 17.38 |
|         tank        | 47.42 | 53.67 |
|      trade name     | 30.23 | 35.58 |
|      microwave      | 79.32 | 88.59 |
|         pot         | 42.49 | 48.26 |
|        animal       | 54.74 |  57.2 |
|       bicycle       | 58.27 | 79.03 |
|         lake        | 43.07 | 60.89 |
|      dishwasher     | 62.67 | 77.74 |
|        screen       | 61.43 | 91.01 |
|       blanket       | 15.29 | 18.95 |
|      sculpture      | 64.53 | 81.17 |
|         hood        | 60.41 | 75.24 |
|        sconce       | 51.38 | 64.14 |
|         vase        |  39.8 | 55.73 |
|    traffic light    | 29.24 | 44.37 |
|         tray        |  8.73 | 17.54 |
|        ashcan       | 41.13 | 57.01 |
|         fan         | 60.86 | 76.31 |
|         pier        | 21.49 | 24.44 |
|      crt screen     |  2.8  | 10.41 |
|        plate        | 46.55 | 62.58 |
|       monitor       | 13.38 | 14.92 |
|    bulletin board   | 54.51 | 62.01 |
|        shower       |  3.37 |  7.54 |
|       radiator      | 68.89 | 79.47 |
|        glass        | 14.28 | 15.42 |
|        clock        | 33.57 | 43.69 |
|         flag        | 46.03 | 51.12 |
+---------------------+-------+-------+
2023-11-11 11:44:57,880 - mmseg - INFO - Summary:
2023-11-11 11:44:57,881 - mmseg - INFO - 
+-------+-------+------+
|  aAcc |  mIoU | mAcc |
+-------+-------+------+
| 83.15 | 50.24 | 62.4 |
+-------+-------+------+
2023-11-11 11:44:58,008 - mmseg - INFO - The previous best checkpoint /data/Next-ViT/segmentation/work_dirs/fpn_512_debi_base_160k/best_mIoU_iter_112000.pth was removed
2023-11-11 11:45:00,808 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_128000.pth.
2023-11-11 11:45:00,808 - mmseg - INFO - Best mIoU is 0.5024 at 128000 iter.
2023-11-11 11:45:00,822 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 11:45:00,823 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8315, mIoU: 0.5024, mAcc: 0.6240, IoU.wall: 0.7803, IoU.building: 0.8197, IoU.sky: 0.9427, IoU.floor: 0.8281, IoU.tree: 0.7448, IoU.ceiling: 0.8381, IoU.road: 0.8210, IoU.bed : 0.8968, IoU.windowpane: 0.6173, IoU.grass: 0.6550, IoU.cabinet: 0.6044, IoU.sidewalk: 0.6513, IoU.person: 0.8162, IoU.earth: 0.3759, IoU.door: 0.5002, IoU.table: 0.6256, IoU.mountain: 0.5618, IoU.plant: 0.5265, IoU.curtain: 0.7397, IoU.chair: 0.5834, IoU.car: 0.8363, IoU.water: 0.5625, IoU.painting: 0.7216, IoU.sofa: 0.6577, IoU.shelf: 0.4217, IoU.house: 0.4762, IoU.sea: 0.5726, IoU.mirror: 0.6835, IoU.rug: 0.6377, IoU.field: 0.2686, IoU.armchair: 0.4369, IoU.seat: 0.6293, IoU.fence: 0.4182, IoU.desk: 0.5050, IoU.rock: 0.3878, IoU.wardrobe: 0.4720, IoU.lamp: 0.6582, IoU.bathtub: 0.8278, IoU.railing: 0.3271, IoU.cushion: 0.5847, IoU.base: 0.2858, IoU.box: 0.3006, IoU.column: 0.4568, IoU.signboard: 0.4049, IoU.chest of drawers: 0.4615, IoU.counter: 0.3267, IoU.sand: 0.4960, IoU.sink: 0.7258, IoU.skyscraper: 0.4668, IoU.fireplace: 0.7605, IoU.refrigerator: 0.7781, IoU.grandstand: 0.4455, IoU.path: 0.2552, IoU.stairs: 0.3076, IoU.runway: 0.6442, IoU.case: 0.3836, IoU.pool table: 0.9305, IoU.pillow: 0.6036, IoU.screen door: 0.6563, IoU.stairway: 0.2988, IoU.river: 0.1051, IoU.bridge: 0.6636, IoU.bookcase: 0.3689, IoU.blind: 0.4242, IoU.coffee table: 0.5207, IoU.toilet: 0.8667, IoU.flower: 0.3891, IoU.book: 0.4634, IoU.hill: 0.1131, IoU.bench: 0.4667, IoU.countertop: 0.5565, IoU.stove: 0.7238, IoU.palm: 0.4962, IoU.kitchen island: 0.4320, IoU.computer: 0.6249, IoU.swivel chair: 0.4944, IoU.boat: 0.4697, IoU.bar: 0.4806, IoU.arcade machine: 0.7501, IoU.hovel: 0.6083, IoU.bus: 0.8594, IoU.towel: 0.6711, IoU.light: 0.5562, IoU.truck: 0.3657, IoU.tower: 0.0679, IoU.chandelier: 0.6689, IoU.awning: 0.2619, IoU.streetlight: 0.2779, IoU.booth: 0.6267, IoU.television receiver: 0.6608, IoU.airplane: 0.5510, IoU.dirt track: 0.0555, IoU.apparel: 0.5415, IoU.pole: 0.2257, IoU.land: 0.0006, IoU.bannister: 0.1313, IoU.escalator: 0.4493, IoU.ottoman: 0.4702, IoU.bottle: 0.3589, IoU.buffet: 0.4880, IoU.poster: 0.3219, IoU.stage: 0.1694, IoU.van: 0.4173, IoU.ship: 0.5211, IoU.fountain: 0.4213, IoU.conveyer belt: 0.8418, IoU.canopy: 0.1541, IoU.washer: 0.7536, IoU.plaything: 0.2036, IoU.swimming pool: 0.7243, IoU.stool: 0.3945, IoU.barrel: 0.6015, IoU.basket: 0.3563, IoU.waterfall: 0.6283, IoU.tent: 0.9555, IoU.bag: 0.1544, IoU.minibike: 0.6295, IoU.cradle: 0.8068, IoU.oven: 0.4518, IoU.ball: 0.3813, IoU.food: 0.4715, IoU.step: 0.1319, IoU.tank: 0.4742, IoU.trade name: 0.3023, IoU.microwave: 0.7932, IoU.pot: 0.4249, IoU.animal: 0.5474, IoU.bicycle: 0.5827, IoU.lake: 0.4307, IoU.dishwasher: 0.6267, IoU.screen: 0.6143, IoU.blanket: 0.1529, IoU.sculpture: 0.6453, IoU.hood: 0.6041, IoU.sconce: 0.5138, IoU.vase: 0.3980, IoU.traffic light: 0.2924, IoU.tray: 0.0873, IoU.ashcan: 0.4113, IoU.fan: 0.6086, IoU.pier: 0.2149, IoU.crt screen: 0.0280, IoU.plate: 0.4655, IoU.monitor: 0.1338, IoU.bulletin board: 0.5451, IoU.shower: 0.0337, IoU.radiator: 0.6889, IoU.glass: 0.1428, IoU.clock: 0.3357, IoU.flag: 0.4603, Acc.wall: 0.8812, Acc.building: 0.9122, Acc.sky: 0.9766, Acc.floor: 0.9008, Acc.tree: 0.8810, Acc.ceiling: 0.9214, Acc.road: 0.8984, Acc.bed : 0.9648, Acc.windowpane: 0.7871, Acc.grass: 0.8259, Acc.cabinet: 0.7361, Acc.sidewalk: 0.8020, Acc.person: 0.9352, Acc.earth: 0.5244, Acc.door: 0.6311, Acc.table: 0.7719, Acc.mountain: 0.7081, Acc.plant: 0.6302, Acc.curtain: 0.8633, Acc.chair: 0.7220, Acc.car: 0.9203, Acc.water: 0.7082, Acc.painting: 0.8711, Acc.sofa: 0.8273, Acc.shelf: 0.6289, Acc.house: 0.7006, Acc.sea: 0.7679, Acc.mirror: 0.7621, Acc.rug: 0.7710, Acc.field: 0.4267, Acc.armchair: 0.6133, Acc.seat: 0.8126, Acc.fence: 0.5747, Acc.desk: 0.7243, Acc.rock: 0.6049, Acc.wardrobe: 0.6669, Acc.lamp: 0.7748, Acc.bathtub: 0.8876, Acc.railing: 0.4807, Acc.cushion: 0.7092, Acc.base: 0.3551, Acc.box: 0.3778, Acc.column: 0.5520, Acc.signboard: 0.5186, Acc.chest of drawers: 0.5997, Acc.counter: 0.3915, Acc.sand: 0.7198, Acc.sink: 0.8107, Acc.skyscraper: 0.5761, Acc.fireplace: 0.9009, Acc.refrigerator: 0.8652, Acc.grandstand: 0.6726, Acc.path: 0.3718, Acc.stairs: 0.4122, Acc.runway: 0.8229, Acc.case: 0.4620, Acc.pool table: 0.9631, Acc.pillow: 0.7212, Acc.screen door: 0.8260, Acc.stairway: 0.3795, Acc.river: 0.2303, Acc.bridge: 0.8062, Acc.bookcase: 0.5516, Acc.blind: 0.4766, Acc.coffee table: 0.8236, Acc.toilet: 0.9136, Acc.flower: 0.5295, Acc.book: 0.6564, Acc.hill: 0.1915, Acc.bench: 0.5286, Acc.countertop: 0.8534, Acc.stove: 0.7985, Acc.palm: 0.6952, Acc.kitchen island: 0.7676, Acc.computer: 0.7118, Acc.swivel chair: 0.6935, Acc.boat: 0.5507, Acc.bar: 0.6780, Acc.arcade machine: 0.8044, Acc.hovel: 0.7115, Acc.bus: 0.9637, Acc.towel: 0.7736, Acc.light: 0.6704, Acc.truck: 0.4544, Acc.tower: 0.0863, Acc.chandelier: 0.8331, Acc.awning: 0.3577, Acc.streetlight: 0.3789, Acc.booth: 0.6606, Acc.television receiver: 0.7992, Acc.airplane: 0.6694, Acc.dirt track: 0.0808, Acc.apparel: 0.7295, Acc.pole: 0.2912, Acc.land: 0.0013, Acc.bannister: 0.1678, Acc.escalator: 0.5951, Acc.ottoman: 0.6129, Acc.bottle: 0.6319, Acc.buffet: 0.5591, Acc.poster: 0.4086, Acc.stage: 0.2318, Acc.van: 0.5895, Acc.ship: 0.7518, Acc.fountain: 0.4256, Acc.conveyer belt: 0.9558, Acc.canopy: 0.2357, Acc.washer: 0.7653, Acc.plaything: 0.3215, Acc.swimming pool: 0.8178, Acc.stool: 0.5145, Acc.barrel: 0.7334, Acc.basket: 0.4751, Acc.waterfall: 0.9217, Acc.tent: 0.9805, Acc.bag: 0.2019, Acc.minibike: 0.7748, Acc.cradle: 0.9777, Acc.oven: 0.6800, Acc.ball: 0.5063, Acc.food: 0.5172, Acc.step: 0.1738, Acc.tank: 0.5367, Acc.trade name: 0.3558, Acc.microwave: 0.8859, Acc.pot: 0.4826, Acc.animal: 0.5720, Acc.bicycle: 0.7903, Acc.lake: 0.6089, Acc.dishwasher: 0.7774, Acc.screen: 0.9101, Acc.blanket: 0.1895, Acc.sculpture: 0.8117, Acc.hood: 0.7524, Acc.sconce: 0.6414, Acc.vase: 0.5573, Acc.traffic light: 0.4437, Acc.tray: 0.1754, Acc.ashcan: 0.5701, Acc.fan: 0.7631, Acc.pier: 0.2444, Acc.crt screen: 0.1041, Acc.plate: 0.6258, Acc.monitor: 0.1492, Acc.bulletin board: 0.6201, Acc.shower: 0.0754, Acc.radiator: 0.7947, Acc.glass: 0.1542, Acc.clock: 0.4369, Acc.flag: 0.5112
2023-11-11 11:45:32,557 - mmseg - INFO - Iter [128050/160000]	lr: 9.521e-06, eta: 6:13:13, time: 2.480, data_time: 1.854, memory: 23129, decode.loss_ce: 0.1820, decode.acc_seg: 92.5129, loss: 0.1820
2023-11-11 11:46:04,224 - mmseg - INFO - Iter [128100/160000]	lr: 9.492e-06, eta: 6:12:38, time: 0.632, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1666, decode.acc_seg: 92.9733, loss: 0.1666
2023-11-11 11:46:39,404 - mmseg - INFO - Iter [128150/160000]	lr: 9.463e-06, eta: 6:12:03, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1813, decode.acc_seg: 92.3528, loss: 0.1813
2023-11-11 11:47:14,687 - mmseg - INFO - Iter [128200/160000]	lr: 9.435e-06, eta: 6:11:28, time: 0.706, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1667, decode.acc_seg: 92.7657, loss: 0.1667
2023-11-11 11:47:49,727 - mmseg - INFO - Iter [128250/160000]	lr: 9.406e-06, eta: 6:10:52, time: 0.700, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1757, decode.acc_seg: 92.6134, loss: 0.1757
2023-11-11 11:48:25,303 - mmseg - INFO - Iter [128300/160000]	lr: 9.377e-06, eta: 6:10:18, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1672, decode.acc_seg: 92.9920, loss: 0.1672
2023-11-11 11:48:58,963 - mmseg - INFO - Iter [128350/160000]	lr: 9.349e-06, eta: 6:09:42, time: 0.673, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1684, decode.acc_seg: 92.8941, loss: 0.1684
2023-11-11 11:49:34,238 - mmseg - INFO - Iter [128400/160000]	lr: 9.320e-06, eta: 6:09:07, time: 0.706, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1740, decode.acc_seg: 92.6157, loss: 0.1740
2023-11-11 11:50:09,385 - mmseg - INFO - Iter [128450/160000]	lr: 9.292e-06, eta: 6:08:32, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1736, decode.acc_seg: 92.7000, loss: 0.1736
2023-11-11 11:50:44,730 - mmseg - INFO - Iter [128500/160000]	lr: 9.263e-06, eta: 6:07:57, time: 0.707, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1799, decode.acc_seg: 92.5418, loss: 0.1799
2023-11-11 11:51:20,170 - mmseg - INFO - Iter [128550/160000]	lr: 9.235e-06, eta: 6:07:22, time: 0.709, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1742, decode.acc_seg: 92.6533, loss: 0.1742
2023-11-11 11:51:55,275 - mmseg - INFO - Iter [128600/160000]	lr: 9.206e-06, eta: 6:06:47, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1663, decode.acc_seg: 92.8217, loss: 0.1663
2023-11-11 11:52:29,949 - mmseg - INFO - Iter [128650/160000]	lr: 9.178e-06, eta: 6:06:12, time: 0.692, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1779, decode.acc_seg: 92.7071, loss: 0.1779
2023-11-11 11:53:02,352 - mmseg - INFO - Iter [128700/160000]	lr: 9.150e-06, eta: 6:05:36, time: 0.649, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1700, decode.acc_seg: 92.7374, loss: 0.1700
2023-11-11 11:53:34,149 - mmseg - INFO - Iter [128750/160000]	lr: 9.121e-06, eta: 6:05:01, time: 0.636, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1775, decode.acc_seg: 92.5366, loss: 0.1775
2023-11-11 11:54:06,222 - mmseg - INFO - Iter [128800/160000]	lr: 9.093e-06, eta: 6:04:25, time: 0.639, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1688, decode.acc_seg: 92.8579, loss: 0.1688
2023-11-11 11:54:41,727 - mmseg - INFO - Iter [128850/160000]	lr: 9.065e-06, eta: 6:03:50, time: 0.711, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1789, decode.acc_seg: 92.5285, loss: 0.1789
2023-11-11 11:55:16,973 - mmseg - INFO - Iter [128900/160000]	lr: 9.037e-06, eta: 6:03:15, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1684, decode.acc_seg: 92.8201, loss: 0.1684
2023-11-11 11:55:52,343 - mmseg - INFO - Iter [128950/160000]	lr: 9.009e-06, eta: 6:02:40, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1734, decode.acc_seg: 92.7798, loss: 0.1734
2023-11-11 11:56:27,709 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 11:56:27,709 - mmseg - INFO - Iter [129000/160000]	lr: 8.980e-06, eta: 6:02:05, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1765, decode.acc_seg: 92.5665, loss: 0.1765
2023-11-11 11:57:03,186 - mmseg - INFO - Iter [129050/160000]	lr: 8.952e-06, eta: 6:01:30, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1699, decode.acc_seg: 92.8172, loss: 0.1699
2023-11-11 11:57:38,974 - mmseg - INFO - Iter [129100/160000]	lr: 8.924e-06, eta: 6:00:55, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1768, decode.acc_seg: 92.6044, loss: 0.1768
2023-11-11 11:58:14,565 - mmseg - INFO - Iter [129150/160000]	lr: 8.896e-06, eta: 6:00:20, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1823, decode.acc_seg: 92.4851, loss: 0.1823
2023-11-11 11:58:47,722 - mmseg - INFO - Iter [129200/160000]	lr: 8.869e-06, eta: 5:59:45, time: 0.664, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1691, decode.acc_seg: 92.8391, loss: 0.1691
2023-11-11 11:59:20,438 - mmseg - INFO - Iter [129250/160000]	lr: 8.841e-06, eta: 5:59:09, time: 0.653, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1682, decode.acc_seg: 92.7985, loss: 0.1682
2023-11-11 11:59:53,461 - mmseg - INFO - Iter [129300/160000]	lr: 8.813e-06, eta: 5:58:34, time: 0.660, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1803, decode.acc_seg: 92.3846, loss: 0.1803
2023-11-11 12:00:28,689 - mmseg - INFO - Iter [129350/160000]	lr: 8.785e-06, eta: 5:57:59, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1696, decode.acc_seg: 92.7516, loss: 0.1696
2023-11-11 12:01:02,088 - mmseg - INFO - Iter [129400/160000]	lr: 8.757e-06, eta: 5:57:23, time: 0.669, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1719, decode.acc_seg: 92.8380, loss: 0.1719
2023-11-11 12:01:35,941 - mmseg - INFO - Iter [129450/160000]	lr: 8.729e-06, eta: 5:56:48, time: 0.676, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1751, decode.acc_seg: 92.5163, loss: 0.1751
2023-11-11 12:02:10,193 - mmseg - INFO - Iter [129500/160000]	lr: 8.702e-06, eta: 5:56:13, time: 0.685, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1714, decode.acc_seg: 92.5777, loss: 0.1714
2023-11-11 12:02:45,759 - mmseg - INFO - Iter [129550/160000]	lr: 8.674e-06, eta: 5:55:38, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1668, decode.acc_seg: 92.9492, loss: 0.1668
2023-11-11 12:03:21,044 - mmseg - INFO - Iter [129600/160000]	lr: 8.647e-06, eta: 5:55:03, time: 0.706, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1801, decode.acc_seg: 92.4225, loss: 0.1801
2023-11-11 12:03:56,723 - mmseg - INFO - Iter [129650/160000]	lr: 8.619e-06, eta: 5:54:28, time: 0.714, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1686, decode.acc_seg: 92.9711, loss: 0.1686
2023-11-11 12:04:31,851 - mmseg - INFO - Iter [129700/160000]	lr: 8.591e-06, eta: 5:53:53, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1813, decode.acc_seg: 92.5173, loss: 0.1813
2023-11-11 12:05:07,120 - mmseg - INFO - Iter [129750/160000]	lr: 8.564e-06, eta: 5:53:18, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1700, decode.acc_seg: 92.9598, loss: 0.1700
2023-11-11 12:05:42,372 - mmseg - INFO - Iter [129800/160000]	lr: 8.536e-06, eta: 5:52:43, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1810, decode.acc_seg: 92.5401, loss: 0.1810
2023-11-11 12:06:17,777 - mmseg - INFO - Iter [129850/160000]	lr: 8.509e-06, eta: 5:52:08, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1713, decode.acc_seg: 92.8243, loss: 0.1713
2023-11-11 12:06:53,245 - mmseg - INFO - Iter [129900/160000]	lr: 8.482e-06, eta: 5:51:33, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1826, decode.acc_seg: 92.3403, loss: 0.1826
2023-11-11 12:07:28,867 - mmseg - INFO - Iter [129950/160000]	lr: 8.454e-06, eta: 5:50:58, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1904, decode.acc_seg: 92.0120, loss: 0.1904
2023-11-11 12:08:04,006 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 12:08:04,007 - mmseg - INFO - Iter [130000/160000]	lr: 8.427e-06, eta: 5:50:23, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1642, decode.acc_seg: 93.1420, loss: 0.1642
2023-11-11 12:08:35,969 - mmseg - INFO - Iter [130050/160000]	lr: 8.400e-06, eta: 5:49:47, time: 0.638, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1729, decode.acc_seg: 92.7332, loss: 0.1729
2023-11-11 12:09:11,375 - mmseg - INFO - Iter [130100/160000]	lr: 8.373e-06, eta: 5:49:12, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1714, decode.acc_seg: 92.6449, loss: 0.1714
2023-11-11 12:09:45,692 - mmseg - INFO - Iter [130150/160000]	lr: 8.345e-06, eta: 5:48:37, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1731, decode.acc_seg: 92.7207, loss: 0.1731
2023-11-11 12:10:20,759 - mmseg - INFO - Iter [130200/160000]	lr: 8.318e-06, eta: 5:48:02, time: 0.701, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1759, decode.acc_seg: 92.5344, loss: 0.1759
2023-11-11 12:10:53,830 - mmseg - INFO - Iter [130250/160000]	lr: 8.291e-06, eta: 5:47:27, time: 0.661, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1750, decode.acc_seg: 92.4975, loss: 0.1750
2023-11-11 12:11:29,064 - mmseg - INFO - Iter [130300/160000]	lr: 8.264e-06, eta: 5:46:52, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1695, decode.acc_seg: 92.9547, loss: 0.1695
2023-11-11 12:12:05,555 - mmseg - INFO - Iter [130350/160000]	lr: 8.237e-06, eta: 5:46:17, time: 0.731, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1658, decode.acc_seg: 93.0920, loss: 0.1658
2023-11-11 12:12:37,255 - mmseg - INFO - Iter [130400/160000]	lr: 8.210e-06, eta: 5:45:41, time: 0.634, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1704, decode.acc_seg: 92.5974, loss: 0.1704
2023-11-11 12:13:09,269 - mmseg - INFO - Iter [130450/160000]	lr: 8.183e-06, eta: 5:45:05, time: 0.640, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1708, decode.acc_seg: 92.7996, loss: 0.1708
2023-11-11 12:13:42,814 - mmseg - INFO - Iter [130500/160000]	lr: 8.156e-06, eta: 5:44:30, time: 0.671, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1804, decode.acc_seg: 92.4153, loss: 0.1804
2023-11-11 12:14:18,078 - mmseg - INFO - Iter [130550/160000]	lr: 8.129e-06, eta: 5:43:55, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1891, decode.acc_seg: 92.5838, loss: 0.1891
2023-11-11 12:14:52,487 - mmseg - INFO - Iter [130600/160000]	lr: 8.103e-06, eta: 5:43:20, time: 0.690, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1777, decode.acc_seg: 92.4039, loss: 0.1777
2023-11-11 12:15:27,644 - mmseg - INFO - Iter [130650/160000]	lr: 8.076e-06, eta: 5:42:45, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1706, decode.acc_seg: 92.7022, loss: 0.1706
2023-11-11 12:16:02,741 - mmseg - INFO - Iter [130700/160000]	lr: 8.049e-06, eta: 5:42:10, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1805, decode.acc_seg: 92.5812, loss: 0.1805
2023-11-11 12:16:35,664 - mmseg - INFO - Iter [130750/160000]	lr: 8.023e-06, eta: 5:41:34, time: 0.660, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1755, decode.acc_seg: 92.6218, loss: 0.1755
2023-11-11 12:17:10,334 - mmseg - INFO - Iter [130800/160000]	lr: 7.996e-06, eta: 5:40:59, time: 0.692, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1738, decode.acc_seg: 92.6652, loss: 0.1738
2023-11-11 12:17:43,926 - mmseg - INFO - Iter [130850/160000]	lr: 7.969e-06, eta: 5:40:24, time: 0.673, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1733, decode.acc_seg: 92.6450, loss: 0.1733
2023-11-11 12:18:15,995 - mmseg - INFO - Iter [130900/160000]	lr: 7.943e-06, eta: 5:39:48, time: 0.641, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1692, decode.acc_seg: 92.8303, loss: 0.1692
2023-11-11 12:18:47,558 - mmseg - INFO - Iter [130950/160000]	lr: 7.916e-06, eta: 5:39:12, time: 0.632, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1758, decode.acc_seg: 92.6048, loss: 0.1758
2023-11-11 12:19:22,807 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 12:19:22,808 - mmseg - INFO - Iter [131000/160000]	lr: 7.890e-06, eta: 5:38:37, time: 0.704, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1744, decode.acc_seg: 92.5802, loss: 0.1744
2023-11-11 12:19:57,785 - mmseg - INFO - Iter [131050/160000]	lr: 7.863e-06, eta: 5:38:02, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1637, decode.acc_seg: 93.0156, loss: 0.1637
2023-11-11 12:20:32,711 - mmseg - INFO - Iter [131100/160000]	lr: 7.837e-06, eta: 5:37:27, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1684, decode.acc_seg: 92.7789, loss: 0.1684
2023-11-11 12:21:07,723 - mmseg - INFO - Iter [131150/160000]	lr: 7.810e-06, eta: 5:36:52, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1858, decode.acc_seg: 92.2324, loss: 0.1858
2023-11-11 12:21:43,737 - mmseg - INFO - Iter [131200/160000]	lr: 7.784e-06, eta: 5:36:18, time: 0.720, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1660, decode.acc_seg: 93.0166, loss: 0.1660
2023-11-11 12:22:20,745 - mmseg - INFO - Iter [131250/160000]	lr: 7.758e-06, eta: 5:35:43, time: 0.741, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1660, decode.acc_seg: 92.8670, loss: 0.1660
2023-11-11 12:22:56,019 - mmseg - INFO - Iter [131300/160000]	lr: 7.732e-06, eta: 5:35:08, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1791, decode.acc_seg: 92.4317, loss: 0.1791
2023-11-11 12:23:31,096 - mmseg - INFO - Iter [131350/160000]	lr: 7.705e-06, eta: 5:34:33, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1656, decode.acc_seg: 93.1242, loss: 0.1656
2023-11-11 12:24:06,355 - mmseg - INFO - Iter [131400/160000]	lr: 7.679e-06, eta: 5:33:58, time: 0.706, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1741, decode.acc_seg: 93.0428, loss: 0.1741
2023-11-11 12:24:41,290 - mmseg - INFO - Iter [131450/160000]	lr: 7.653e-06, eta: 5:33:23, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1618, decode.acc_seg: 93.2167, loss: 0.1618
2023-11-11 12:25:15,503 - mmseg - INFO - Iter [131500/160000]	lr: 7.627e-06, eta: 5:32:48, time: 0.684, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1683, decode.acc_seg: 92.9077, loss: 0.1683
2023-11-11 12:25:48,759 - mmseg - INFO - Iter [131550/160000]	lr: 7.601e-06, eta: 5:32:12, time: 0.665, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1690, decode.acc_seg: 92.8034, loss: 0.1690
2023-11-11 12:26:23,731 - mmseg - INFO - Iter [131600/160000]	lr: 7.575e-06, eta: 5:31:37, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1709, decode.acc_seg: 92.8193, loss: 0.1709
2023-11-11 12:26:59,425 - mmseg - INFO - Iter [131650/160000]	lr: 7.549e-06, eta: 5:31:02, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1648, decode.acc_seg: 92.8689, loss: 0.1648
2023-11-11 12:27:34,320 - mmseg - INFO - Iter [131700/160000]	lr: 7.523e-06, eta: 5:30:27, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1643, decode.acc_seg: 92.9370, loss: 0.1643
2023-11-11 12:28:10,299 - mmseg - INFO - Iter [131750/160000]	lr: 7.497e-06, eta: 5:29:52, time: 0.719, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1694, decode.acc_seg: 92.6604, loss: 0.1694
2023-11-11 12:28:45,106 - mmseg - INFO - Iter [131800/160000]	lr: 7.471e-06, eta: 5:29:17, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1752, decode.acc_seg: 92.6577, loss: 0.1752
2023-11-11 12:29:20,828 - mmseg - INFO - Iter [131850/160000]	lr: 7.446e-06, eta: 5:28:42, time: 0.715, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1685, decode.acc_seg: 92.7528, loss: 0.1685
2023-11-11 12:29:56,101 - mmseg - INFO - Iter [131900/160000]	lr: 7.420e-06, eta: 5:28:07, time: 0.706, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1619, decode.acc_seg: 93.0016, loss: 0.1619
2023-11-11 12:30:30,967 - mmseg - INFO - Iter [131950/160000]	lr: 7.394e-06, eta: 5:27:32, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1683, decode.acc_seg: 92.7580, loss: 0.1683
2023-11-11 12:31:05,974 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 12:31:05,974 - mmseg - INFO - Iter [132000/160000]	lr: 7.369e-06, eta: 5:26:57, time: 0.699, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1770, decode.acc_seg: 92.3658, loss: 0.1770
2023-11-11 12:31:39,468 - mmseg - INFO - Iter [132050/160000]	lr: 7.343e-06, eta: 5:26:22, time: 0.670, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1746, decode.acc_seg: 92.5597, loss: 0.1746
2023-11-11 12:32:14,907 - mmseg - INFO - Iter [132100/160000]	lr: 7.317e-06, eta: 5:25:47, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1726, decode.acc_seg: 92.6173, loss: 0.1726
2023-11-11 12:32:50,160 - mmseg - INFO - Iter [132150/160000]	lr: 7.292e-06, eta: 5:25:12, time: 0.706, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1763, decode.acc_seg: 92.5889, loss: 0.1763
2023-11-11 12:33:23,053 - mmseg - INFO - Iter [132200/160000]	lr: 7.266e-06, eta: 5:24:37, time: 0.658, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1680, decode.acc_seg: 92.8463, loss: 0.1680
2023-11-11 12:33:57,404 - mmseg - INFO - Iter [132250/160000]	lr: 7.241e-06, eta: 5:24:01, time: 0.686, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1692, decode.acc_seg: 92.7863, loss: 0.1692
2023-11-11 12:34:32,879 - mmseg - INFO - Iter [132300/160000]	lr: 7.215e-06, eta: 5:23:27, time: 0.710, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1915, decode.acc_seg: 92.0045, loss: 0.1915
2023-11-11 12:35:07,573 - mmseg - INFO - Iter [132350/160000]	lr: 7.190e-06, eta: 5:22:51, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1684, decode.acc_seg: 92.7563, loss: 0.1684
2023-11-11 12:35:42,388 - mmseg - INFO - Iter [132400/160000]	lr: 7.165e-06, eta: 5:22:16, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1745, decode.acc_seg: 92.6813, loss: 0.1745
2023-11-11 12:36:16,333 - mmseg - INFO - Iter [132450/160000]	lr: 7.139e-06, eta: 5:21:41, time: 0.680, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1699, decode.acc_seg: 92.8162, loss: 0.1699
2023-11-11 12:36:50,000 - mmseg - INFO - Iter [132500/160000]	lr: 7.114e-06, eta: 5:21:06, time: 0.673, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1684, decode.acc_seg: 92.7130, loss: 0.1684
2023-11-11 12:37:23,399 - mmseg - INFO - Iter [132550/160000]	lr: 7.089e-06, eta: 5:20:30, time: 0.667, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1682, decode.acc_seg: 92.8940, loss: 0.1682
2023-11-11 12:37:57,013 - mmseg - INFO - Iter [132600/160000]	lr: 7.064e-06, eta: 5:19:55, time: 0.672, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1679, decode.acc_seg: 92.8580, loss: 0.1679
2023-11-11 12:38:32,301 - mmseg - INFO - Iter [132650/160000]	lr: 7.039e-06, eta: 5:19:20, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1728, decode.acc_seg: 92.8611, loss: 0.1728
2023-11-11 12:39:07,259 - mmseg - INFO - Iter [132700/160000]	lr: 7.013e-06, eta: 5:18:45, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1640, decode.acc_seg: 92.9124, loss: 0.1640
2023-11-11 12:39:40,603 - mmseg - INFO - Iter [132750/160000]	lr: 6.988e-06, eta: 5:18:10, time: 0.668, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1700, decode.acc_seg: 92.8006, loss: 0.1700
2023-11-11 12:40:12,449 - mmseg - INFO - Iter [132800/160000]	lr: 6.963e-06, eta: 5:17:34, time: 0.637, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1728, decode.acc_seg: 92.6526, loss: 0.1728
2023-11-11 12:40:45,848 - mmseg - INFO - Iter [132850/160000]	lr: 6.938e-06, eta: 5:16:59, time: 0.667, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1695, decode.acc_seg: 92.7188, loss: 0.1695
2023-11-11 12:41:19,209 - mmseg - INFO - Iter [132900/160000]	lr: 6.914e-06, eta: 5:16:23, time: 0.668, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1787, decode.acc_seg: 92.5405, loss: 0.1787
2023-11-11 12:41:54,596 - mmseg - INFO - Iter [132950/160000]	lr: 6.889e-06, eta: 5:15:48, time: 0.707, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1745, decode.acc_seg: 92.7018, loss: 0.1745
2023-11-11 12:42:30,156 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 12:42:30,157 - mmseg - INFO - Iter [133000/160000]	lr: 6.864e-06, eta: 5:15:13, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1777, decode.acc_seg: 92.4127, loss: 0.1777
2023-11-11 12:43:06,325 - mmseg - INFO - Iter [133050/160000]	lr: 6.839e-06, eta: 5:14:39, time: 0.724, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1707, decode.acc_seg: 92.8174, loss: 0.1707
2023-11-11 12:43:41,589 - mmseg - INFO - Iter [133100/160000]	lr: 6.814e-06, eta: 5:14:04, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1665, decode.acc_seg: 92.9392, loss: 0.1665
2023-11-11 12:44:15,745 - mmseg - INFO - Iter [133150/160000]	lr: 6.789e-06, eta: 5:13:28, time: 0.683, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1647, decode.acc_seg: 92.9681, loss: 0.1647
2023-11-11 12:44:51,074 - mmseg - INFO - Iter [133200/160000]	lr: 6.765e-06, eta: 5:12:53, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1740, decode.acc_seg: 92.9072, loss: 0.1740
2023-11-11 12:45:26,427 - mmseg - INFO - Iter [133250/160000]	lr: 6.740e-06, eta: 5:12:19, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1739, decode.acc_seg: 92.9060, loss: 0.1739
2023-11-11 12:46:01,913 - mmseg - INFO - Iter [133300/160000]	lr: 6.716e-06, eta: 5:11:44, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1758, decode.acc_seg: 92.5896, loss: 0.1758
2023-11-11 12:46:35,981 - mmseg - INFO - Iter [133350/160000]	lr: 6.691e-06, eta: 5:11:08, time: 0.682, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1694, decode.acc_seg: 92.8760, loss: 0.1694
2023-11-11 12:47:08,697 - mmseg - INFO - Iter [133400/160000]	lr: 6.667e-06, eta: 5:10:33, time: 0.654, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1724, decode.acc_seg: 92.6254, loss: 0.1724
2023-11-11 12:47:42,810 - mmseg - INFO - Iter [133450/160000]	lr: 6.642e-06, eta: 5:09:58, time: 0.681, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1689, decode.acc_seg: 92.8524, loss: 0.1689
2023-11-11 12:48:15,644 - mmseg - INFO - Iter [133500/160000]	lr: 6.618e-06, eta: 5:09:22, time: 0.657, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1725, decode.acc_seg: 92.3790, loss: 0.1725
2023-11-11 12:48:47,727 - mmseg - INFO - Iter [133550/160000]	lr: 6.593e-06, eta: 5:08:47, time: 0.642, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1623, decode.acc_seg: 93.1225, loss: 0.1623
2023-11-11 12:49:21,301 - mmseg - INFO - Iter [133600/160000]	lr: 6.569e-06, eta: 5:08:11, time: 0.670, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1688, decode.acc_seg: 92.7363, loss: 0.1688
2023-11-11 12:49:56,926 - mmseg - INFO - Iter [133650/160000]	lr: 6.545e-06, eta: 5:07:36, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1658, decode.acc_seg: 93.0499, loss: 0.1658
2023-11-11 12:50:32,048 - mmseg - INFO - Iter [133700/160000]	lr: 6.520e-06, eta: 5:07:01, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1767, decode.acc_seg: 92.6919, loss: 0.1767
2023-11-11 12:51:07,315 - mmseg - INFO - Iter [133750/160000]	lr: 6.496e-06, eta: 5:06:26, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1679, decode.acc_seg: 92.9761, loss: 0.1679
2023-11-11 12:51:42,434 - mmseg - INFO - Iter [133800/160000]	lr: 6.472e-06, eta: 5:05:51, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1690, decode.acc_seg: 92.9025, loss: 0.1690
2023-11-11 12:52:18,011 - mmseg - INFO - Iter [133850/160000]	lr: 6.448e-06, eta: 5:05:16, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1775, decode.acc_seg: 92.4520, loss: 0.1775
2023-11-11 12:52:53,287 - mmseg - INFO - Iter [133900/160000]	lr: 6.424e-06, eta: 5:04:42, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1641, decode.acc_seg: 93.0954, loss: 0.1641
2023-11-11 12:53:28,691 - mmseg - INFO - Iter [133950/160000]	lr: 6.400e-06, eta: 5:04:07, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1746, decode.acc_seg: 92.7308, loss: 0.1746
2023-11-11 12:54:02,755 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 12:54:02,756 - mmseg - INFO - Iter [134000/160000]	lr: 6.376e-06, eta: 5:03:31, time: 0.681, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1622, decode.acc_seg: 93.2697, loss: 0.1622
2023-11-11 12:54:37,841 - mmseg - INFO - Iter [134050/160000]	lr: 6.352e-06, eta: 5:02:56, time: 0.702, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1671, decode.acc_seg: 92.7501, loss: 0.1671
2023-11-11 12:55:10,003 - mmseg - INFO - Iter [134100/160000]	lr: 6.328e-06, eta: 5:02:21, time: 0.644, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1695, decode.acc_seg: 92.9331, loss: 0.1695
2023-11-11 12:55:41,217 - mmseg - INFO - Iter [134150/160000]	lr: 6.304e-06, eta: 5:01:45, time: 0.624, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1726, decode.acc_seg: 92.6360, loss: 0.1726
2023-11-11 12:56:14,653 - mmseg - INFO - Iter [134200/160000]	lr: 6.280e-06, eta: 5:01:10, time: 0.668, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1634, decode.acc_seg: 92.9432, loss: 0.1634
2023-11-11 12:56:49,121 - mmseg - INFO - Iter [134250/160000]	lr: 6.256e-06, eta: 5:00:35, time: 0.690, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1760, decode.acc_seg: 92.4103, loss: 0.1760
2023-11-11 12:57:22,528 - mmseg - INFO - Iter [134300/160000]	lr: 6.233e-06, eta: 4:59:59, time: 0.668, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1739, decode.acc_seg: 92.5476, loss: 0.1739
2023-11-11 12:57:55,626 - mmseg - INFO - Iter [134350/160000]	lr: 6.209e-06, eta: 4:59:24, time: 0.661, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1664, decode.acc_seg: 92.8825, loss: 0.1664
2023-11-11 12:58:30,484 - mmseg - INFO - Iter [134400/160000]	lr: 6.185e-06, eta: 4:58:49, time: 0.697, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1800, decode.acc_seg: 92.2953, loss: 0.1800
2023-11-11 12:59:05,552 - mmseg - INFO - Iter [134450/160000]	lr: 6.162e-06, eta: 4:58:14, time: 0.701, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1690, decode.acc_seg: 92.6450, loss: 0.1690
2023-11-11 12:59:40,477 - mmseg - INFO - Iter [134500/160000]	lr: 6.138e-06, eta: 4:57:39, time: 0.699, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1659, decode.acc_seg: 92.9700, loss: 0.1659
2023-11-11 13:00:15,584 - mmseg - INFO - Iter [134550/160000]	lr: 6.114e-06, eta: 4:57:04, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1757, decode.acc_seg: 92.4998, loss: 0.1757
2023-11-11 13:00:50,496 - mmseg - INFO - Iter [134600/160000]	lr: 6.091e-06, eta: 4:56:29, time: 0.699, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1760, decode.acc_seg: 92.3958, loss: 0.1760
2023-11-11 13:01:25,444 - mmseg - INFO - Iter [134650/160000]	lr: 6.067e-06, eta: 4:55:54, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1745, decode.acc_seg: 92.7749, loss: 0.1745
2023-11-11 13:02:00,376 - mmseg - INFO - Iter [134700/160000]	lr: 6.044e-06, eta: 4:55:19, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1621, decode.acc_seg: 93.2449, loss: 0.1621
2023-11-11 13:02:35,743 - mmseg - INFO - Iter [134750/160000]	lr: 6.021e-06, eta: 4:54:44, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1712, decode.acc_seg: 92.7811, loss: 0.1712
2023-11-11 13:03:10,135 - mmseg - INFO - Iter [134800/160000]	lr: 5.997e-06, eta: 4:54:09, time: 0.689, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1718, decode.acc_seg: 92.4313, loss: 0.1718
2023-11-11 13:03:45,129 - mmseg - INFO - Iter [134850/160000]	lr: 5.974e-06, eta: 4:53:34, time: 0.699, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1662, decode.acc_seg: 92.9083, loss: 0.1662
2023-11-11 13:04:20,892 - mmseg - INFO - Iter [134900/160000]	lr: 5.951e-06, eta: 4:52:59, time: 0.715, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1793, decode.acc_seg: 92.3543, loss: 0.1793
2023-11-11 13:04:56,512 - mmseg - INFO - Iter [134950/160000]	lr: 5.928e-06, eta: 4:52:24, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1789, decode.acc_seg: 92.2389, loss: 0.1789
2023-11-11 13:05:32,369 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 13:05:32,370 - mmseg - INFO - Iter [135000/160000]	lr: 5.904e-06, eta: 4:51:49, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1650, decode.acc_seg: 92.9230, loss: 0.1650
2023-11-11 13:06:06,752 - mmseg - INFO - Iter [135050/160000]	lr: 5.881e-06, eta: 4:51:14, time: 0.689, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1650, decode.acc_seg: 92.9134, loss: 0.1650
2023-11-11 13:06:39,320 - mmseg - INFO - Iter [135100/160000]	lr: 5.858e-06, eta: 4:50:38, time: 0.651, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1712, decode.acc_seg: 92.9078, loss: 0.1712
2023-11-11 13:07:10,900 - mmseg - INFO - Iter [135150/160000]	lr: 5.835e-06, eta: 4:50:03, time: 0.632, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1747, decode.acc_seg: 92.7587, loss: 0.1747
2023-11-11 13:07:45,528 - mmseg - INFO - Iter [135200/160000]	lr: 5.812e-06, eta: 4:49:28, time: 0.692, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1637, decode.acc_seg: 92.8756, loss: 0.1637
2023-11-11 13:08:19,972 - mmseg - INFO - Iter [135250/160000]	lr: 5.789e-06, eta: 4:48:52, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1741, decode.acc_seg: 92.8396, loss: 0.1741
2023-11-11 13:08:51,451 - mmseg - INFO - Iter [135300/160000]	lr: 5.766e-06, eta: 4:48:17, time: 0.629, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1674, decode.acc_seg: 92.8043, loss: 0.1674
2023-11-11 13:09:22,919 - mmseg - INFO - Iter [135350/160000]	lr: 5.743e-06, eta: 4:47:41, time: 0.629, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1683, decode.acc_seg: 92.7174, loss: 0.1683
2023-11-11 13:09:54,456 - mmseg - INFO - Iter [135400/160000]	lr: 5.721e-06, eta: 4:47:05, time: 0.631, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1629, decode.acc_seg: 93.0900, loss: 0.1629
2023-11-11 13:10:27,314 - mmseg - INFO - Iter [135450/160000]	lr: 5.698e-06, eta: 4:46:30, time: 0.656, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1646, decode.acc_seg: 92.9475, loss: 0.1646
2023-11-11 13:11:03,123 - mmseg - INFO - Iter [135500/160000]	lr: 5.675e-06, eta: 4:45:55, time: 0.716, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1771, decode.acc_seg: 92.4342, loss: 0.1771
2023-11-11 13:11:37,384 - mmseg - INFO - Iter [135550/160000]	lr: 5.652e-06, eta: 4:45:20, time: 0.687, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1703, decode.acc_seg: 92.9110, loss: 0.1703
2023-11-11 13:12:12,247 - mmseg - INFO - Iter [135600/160000]	lr: 5.630e-06, eta: 4:44:45, time: 0.696, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1699, decode.acc_seg: 92.9921, loss: 0.1699
2023-11-11 13:12:47,832 - mmseg - INFO - Iter [135650/160000]	lr: 5.607e-06, eta: 4:44:10, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1675, decode.acc_seg: 92.8873, loss: 0.1675
2023-11-11 13:13:23,181 - mmseg - INFO - Iter [135700/160000]	lr: 5.585e-06, eta: 4:43:35, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1547, decode.acc_seg: 93.3347, loss: 0.1547
2023-11-11 13:13:57,702 - mmseg - INFO - Iter [135750/160000]	lr: 5.562e-06, eta: 4:43:00, time: 0.691, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1709, decode.acc_seg: 92.7626, loss: 0.1709
2023-11-11 13:14:29,274 - mmseg - INFO - Iter [135800/160000]	lr: 5.540e-06, eta: 4:42:24, time: 0.631, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1690, decode.acc_seg: 92.9677, loss: 0.1690
2023-11-11 13:15:00,798 - mmseg - INFO - Iter [135850/160000]	lr: 5.517e-06, eta: 4:41:49, time: 0.630, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1753, decode.acc_seg: 92.5086, loss: 0.1753
2023-11-11 13:15:33,323 - mmseg - INFO - Iter [135900/160000]	lr: 5.495e-06, eta: 4:41:13, time: 0.650, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1791, decode.acc_seg: 92.3401, loss: 0.1791
2023-11-11 13:16:07,649 - mmseg - INFO - Iter [135950/160000]	lr: 5.472e-06, eta: 4:40:38, time: 0.685, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1643, decode.acc_seg: 93.0645, loss: 0.1643
2023-11-11 13:16:43,163 - mmseg - INFO - Saving checkpoint at 136000 iterations
2023-11-11 13:16:48,017 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 13:16:48,017 - mmseg - INFO - Iter [136000/160000]	lr: 5.450e-06, eta: 4:40:04, time: 0.809, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1691, decode.acc_seg: 92.9480, loss: 0.1691
2023-11-11 13:18:16,389 - mmseg - INFO - per class results:
2023-11-11 13:18:16,403 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 77.97 |  88.8 |
|       building      | 81.86 | 91.36 |
|         sky         | 94.51 |  97.6 |
|        floor        | 83.05 | 90.24 |
|         tree        |  75.2 | 88.03 |
|       ceiling       | 84.04 | 91.93 |
|         road        | 82.42 | 88.98 |
|         bed         | 89.89 | 95.99 |
|      windowpane     | 62.04 | 78.02 |
|        grass        | 65.74 | 82.82 |
|       cabinet       | 60.66 | 73.98 |
|       sidewalk      | 64.73 | 81.79 |
|        person       | 81.22 | 93.65 |
|        earth        | 39.15 |  55.0 |
|         door        | 48.87 | 60.93 |
|        table        | 62.75 | 77.48 |
|       mountain      | 54.28 | 68.86 |
|        plant        | 54.46 | 64.39 |
|       curtain       | 74.24 | 87.29 |
|        chair        | 58.21 | 72.63 |
|         car         | 83.64 | 92.58 |
|        water        | 59.99 |  76.9 |
|       painting      | 71.94 | 87.26 |
|         sofa        | 67.06 | 84.82 |
|        shelf        | 42.21 | 59.78 |
|        house        | 44.25 | 62.56 |
|         sea         | 59.65 | 74.92 |
|        mirror       | 68.12 | 75.51 |
|         rug         | 64.44 | 76.72 |
|        field        | 27.08 | 42.17 |
|       armchair      | 44.07 | 61.05 |
|         seat        | 63.96 | 81.87 |
|        fence        | 42.72 | 56.92 |
|         desk        |  50.9 | 73.74 |
|         rock        | 37.18 | 57.88 |
|       wardrobe      | 49.84 | 69.53 |
|         lamp        | 66.03 | 77.54 |
|       bathtub       | 81.03 | 85.84 |
|       railing       | 31.34 | 44.02 |
|       cushion       | 58.36 | 70.95 |
|         base        | 31.44 | 39.95 |
|         box         | 30.51 | 37.32 |
|        column       | 45.75 | 53.93 |
|      signboard      | 39.57 | 52.48 |
|   chest of drawers  | 48.66 | 57.71 |
|       counter       | 37.23 | 43.95 |
|         sand        | 50.77 | 73.64 |
|         sink        | 71.97 | 81.46 |
|      skyscraper     | 48.01 | 60.05 |
|      fireplace      | 75.25 | 90.45 |
|     refrigerator    | 76.86 | 84.55 |
|      grandstand     | 43.18 | 66.05 |
|         path        | 24.23 | 36.63 |
|        stairs       |  29.5 | 40.77 |
|        runway       | 64.57 | 82.72 |
|         case        | 48.59 | 60.23 |
|      pool table     | 93.07 | 96.66 |
|        pillow       | 60.82 | 72.53 |
|     screen door     | 69.96 | 80.45 |
|       stairway      | 29.47 | 37.27 |
|        river        | 11.36 |  24.1 |
|        bridge       | 67.94 | 84.82 |
|       bookcase      | 36.23 | 56.28 |
|        blind        | 44.29 | 49.56 |
|     coffee table    | 52.78 | 82.54 |
|        toilet       | 86.61 | 91.26 |
|        flower       | 37.16 | 53.21 |
|         book        | 44.71 | 69.57 |
|         hill        | 11.59 | 19.25 |
|        bench        | 45.51 | 52.88 |
|      countertop     | 60.09 | 84.06 |
|        stove        | 73.14 | 81.36 |
|         palm        | 47.82 | 71.16 |
|    kitchen island   | 37.98 | 65.81 |
|       computer      | 64.09 | 73.67 |
|     swivel chair    | 49.87 | 66.53 |
|         boat        | 46.54 | 55.28 |
|         bar         |  51.9 | 69.31 |
|    arcade machine   | 64.99 | 69.12 |
|        hovel        | 58.98 | 67.71 |
|         bus         | 86.23 | 96.07 |
|        towel        | 68.52 |  79.6 |
|        light        | 55.23 | 65.14 |
|        truck        | 31.04 | 38.65 |
|        tower        |  5.58 |  8.8  |
|      chandelier     | 66.91 | 83.25 |
|        awning       | 27.33 | 35.34 |
|     streetlight     | 27.52 | 36.16 |
|        booth        | 58.77 | 61.11 |
| television receiver | 66.11 | 80.21 |
|       airplane      | 57.68 |  69.3 |
|      dirt track     | 10.03 | 17.14 |
|       apparel       | 49.44 | 70.22 |
|         pole        |  25.6 | 34.61 |
|         land        |  0.0  |  0.0  |
|      bannister      | 13.68 | 17.66 |
|      escalator      | 49.98 | 68.94 |
|       ottoman       | 48.07 | 61.57 |
|        bottle       | 35.67 | 61.92 |
|        buffet       | 40.15 | 45.56 |
|        poster       | 32.13 | 42.91 |
|        stage        | 15.56 | 20.73 |
|         van         | 41.62 | 59.57 |
|         ship        | 56.91 | 82.48 |
|       fountain      | 39.31 | 39.66 |
|    conveyer belt    | 82.98 | 96.76 |
|        canopy       | 12.08 | 18.03 |
|        washer       | 75.29 | 76.17 |
|      plaything      | 21.23 | 31.54 |
|    swimming pool    | 78.57 | 88.56 |
|        stool        | 38.85 | 51.77 |
|        barrel       | 65.71 | 79.22 |
|        basket       | 37.76 | 50.35 |
|      waterfall      | 72.67 | 92.21 |
|         tent        | 96.45 |  98.2 |
|         bag         | 18.55 | 24.08 |
|       minibike      | 64.37 | 78.52 |
|        cradle       | 83.14 | 97.49 |
|         oven        | 47.79 | 66.18 |
|         ball        | 42.24 | 55.29 |
|         food        | 49.42 | 56.17 |
|         step        | 15.56 | 20.29 |
|         tank        | 48.28 | 54.78 |
|      trade name     | 25.82 | 30.66 |
|      microwave      | 79.33 | 90.95 |
|         pot         | 42.54 |  48.0 |
|        animal       | 54.29 |  56.8 |
|       bicycle       | 58.72 | 80.53 |
|         lake        | 51.91 |  61.3 |
|      dishwasher     | 58.34 | 69.33 |
|        screen       | 57.16 | 82.17 |
|       blanket       | 16.13 | 19.34 |
|      sculpture      | 59.73 | 83.44 |
|         hood        | 61.38 | 75.13 |
|        sconce       | 49.08 | 58.72 |
|         vase        | 40.58 |  57.3 |
|    traffic light    | 28.02 | 50.66 |
|         tray        |  9.27 | 18.12 |
|        ashcan       | 41.92 | 57.59 |
|         fan         | 61.38 | 76.38 |
|         pier        | 12.33 | 13.11 |
|      crt screen     |  2.82 | 10.39 |
|        plate        | 42.29 | 55.53 |
|       monitor       | 10.28 | 11.33 |
|    bulletin board   | 55.77 | 64.88 |
|        shower       |  2.16 |  5.03 |
|       radiator      | 68.04 | 74.86 |
|        glass        | 13.78 | 14.71 |
|        clock        | 36.76 | 45.04 |
|         flag        | 45.34 | 49.69 |
+---------------------+-------+-------+
2023-11-11 13:18:16,404 - mmseg - INFO - Summary:
2023-11-11 13:18:16,404 - mmseg - INFO - 
+------+-------+-------+
| aAcc |  mIoU |  mAcc |
+------+-------+-------+
| 83.3 | 50.45 | 62.35 |
+------+-------+-------+
2023-11-11 13:18:16,529 - mmseg - INFO - The previous best checkpoint /data/Next-ViT/segmentation/work_dirs/fpn_512_debi_base_160k/best_mIoU_iter_128000.pth was removed
2023-11-11 13:18:20,378 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_136000.pth.
2023-11-11 13:18:20,379 - mmseg - INFO - Best mIoU is 0.5045 at 136000 iter.
2023-11-11 13:18:20,393 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 13:18:20,394 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8330, mIoU: 0.5045, mAcc: 0.6235, IoU.wall: 0.7797, IoU.building: 0.8186, IoU.sky: 0.9451, IoU.floor: 0.8305, IoU.tree: 0.7520, IoU.ceiling: 0.8404, IoU.road: 0.8242, IoU.bed : 0.8989, IoU.windowpane: 0.6204, IoU.grass: 0.6574, IoU.cabinet: 0.6066, IoU.sidewalk: 0.6473, IoU.person: 0.8122, IoU.earth: 0.3915, IoU.door: 0.4887, IoU.table: 0.6275, IoU.mountain: 0.5428, IoU.plant: 0.5446, IoU.curtain: 0.7424, IoU.chair: 0.5821, IoU.car: 0.8364, IoU.water: 0.5999, IoU.painting: 0.7194, IoU.sofa: 0.6706, IoU.shelf: 0.4221, IoU.house: 0.4425, IoU.sea: 0.5965, IoU.mirror: 0.6812, IoU.rug: 0.6444, IoU.field: 0.2708, IoU.armchair: 0.4407, IoU.seat: 0.6396, IoU.fence: 0.4272, IoU.desk: 0.5090, IoU.rock: 0.3718, IoU.wardrobe: 0.4984, IoU.lamp: 0.6603, IoU.bathtub: 0.8103, IoU.railing: 0.3134, IoU.cushion: 0.5836, IoU.base: 0.3144, IoU.box: 0.3051, IoU.column: 0.4575, IoU.signboard: 0.3957, IoU.chest of drawers: 0.4866, IoU.counter: 0.3723, IoU.sand: 0.5077, IoU.sink: 0.7197, IoU.skyscraper: 0.4801, IoU.fireplace: 0.7525, IoU.refrigerator: 0.7686, IoU.grandstand: 0.4318, IoU.path: 0.2423, IoU.stairs: 0.2950, IoU.runway: 0.6457, IoU.case: 0.4859, IoU.pool table: 0.9307, IoU.pillow: 0.6082, IoU.screen door: 0.6996, IoU.stairway: 0.2947, IoU.river: 0.1136, IoU.bridge: 0.6794, IoU.bookcase: 0.3623, IoU.blind: 0.4429, IoU.coffee table: 0.5278, IoU.toilet: 0.8661, IoU.flower: 0.3716, IoU.book: 0.4471, IoU.hill: 0.1159, IoU.bench: 0.4551, IoU.countertop: 0.6009, IoU.stove: 0.7314, IoU.palm: 0.4782, IoU.kitchen island: 0.3798, IoU.computer: 0.6409, IoU.swivel chair: 0.4987, IoU.boat: 0.4654, IoU.bar: 0.5190, IoU.arcade machine: 0.6499, IoU.hovel: 0.5898, IoU.bus: 0.8623, IoU.towel: 0.6852, IoU.light: 0.5523, IoU.truck: 0.3104, IoU.tower: 0.0558, IoU.chandelier: 0.6691, IoU.awning: 0.2733, IoU.streetlight: 0.2752, IoU.booth: 0.5877, IoU.television receiver: 0.6611, IoU.airplane: 0.5768, IoU.dirt track: 0.1003, IoU.apparel: 0.4944, IoU.pole: 0.2560, IoU.land: 0.0000, IoU.bannister: 0.1368, IoU.escalator: 0.4998, IoU.ottoman: 0.4807, IoU.bottle: 0.3567, IoU.buffet: 0.4015, IoU.poster: 0.3213, IoU.stage: 0.1556, IoU.van: 0.4162, IoU.ship: 0.5691, IoU.fountain: 0.3931, IoU.conveyer belt: 0.8298, IoU.canopy: 0.1208, IoU.washer: 0.7529, IoU.plaything: 0.2123, IoU.swimming pool: 0.7857, IoU.stool: 0.3885, IoU.barrel: 0.6571, IoU.basket: 0.3776, IoU.waterfall: 0.7267, IoU.tent: 0.9645, IoU.bag: 0.1855, IoU.minibike: 0.6437, IoU.cradle: 0.8314, IoU.oven: 0.4779, IoU.ball: 0.4224, IoU.food: 0.4942, IoU.step: 0.1556, IoU.tank: 0.4828, IoU.trade name: 0.2582, IoU.microwave: 0.7933, IoU.pot: 0.4254, IoU.animal: 0.5429, IoU.bicycle: 0.5872, IoU.lake: 0.5191, IoU.dishwasher: 0.5834, IoU.screen: 0.5716, IoU.blanket: 0.1613, IoU.sculpture: 0.5973, IoU.hood: 0.6138, IoU.sconce: 0.4908, IoU.vase: 0.4058, IoU.traffic light: 0.2802, IoU.tray: 0.0927, IoU.ashcan: 0.4192, IoU.fan: 0.6138, IoU.pier: 0.1233, IoU.crt screen: 0.0282, IoU.plate: 0.4229, IoU.monitor: 0.1028, IoU.bulletin board: 0.5577, IoU.shower: 0.0216, IoU.radiator: 0.6804, IoU.glass: 0.1378, IoU.clock: 0.3676, IoU.flag: 0.4534, Acc.wall: 0.8880, Acc.building: 0.9136, Acc.sky: 0.9760, Acc.floor: 0.9024, Acc.tree: 0.8803, Acc.ceiling: 0.9193, Acc.road: 0.8898, Acc.bed : 0.9599, Acc.windowpane: 0.7802, Acc.grass: 0.8282, Acc.cabinet: 0.7398, Acc.sidewalk: 0.8179, Acc.person: 0.9365, Acc.earth: 0.5500, Acc.door: 0.6093, Acc.table: 0.7748, Acc.mountain: 0.6886, Acc.plant: 0.6439, Acc.curtain: 0.8729, Acc.chair: 0.7263, Acc.car: 0.9258, Acc.water: 0.7690, Acc.painting: 0.8726, Acc.sofa: 0.8482, Acc.shelf: 0.5978, Acc.house: 0.6256, Acc.sea: 0.7492, Acc.mirror: 0.7551, Acc.rug: 0.7672, Acc.field: 0.4217, Acc.armchair: 0.6105, Acc.seat: 0.8187, Acc.fence: 0.5692, Acc.desk: 0.7374, Acc.rock: 0.5788, Acc.wardrobe: 0.6953, Acc.lamp: 0.7754, Acc.bathtub: 0.8584, Acc.railing: 0.4402, Acc.cushion: 0.7095, Acc.base: 0.3995, Acc.box: 0.3732, Acc.column: 0.5393, Acc.signboard: 0.5248, Acc.chest of drawers: 0.5771, Acc.counter: 0.4395, Acc.sand: 0.7364, Acc.sink: 0.8146, Acc.skyscraper: 0.6005, Acc.fireplace: 0.9045, Acc.refrigerator: 0.8455, Acc.grandstand: 0.6605, Acc.path: 0.3663, Acc.stairs: 0.4077, Acc.runway: 0.8272, Acc.case: 0.6023, Acc.pool table: 0.9666, Acc.pillow: 0.7253, Acc.screen door: 0.8045, Acc.stairway: 0.3727, Acc.river: 0.2410, Acc.bridge: 0.8482, Acc.bookcase: 0.5628, Acc.blind: 0.4956, Acc.coffee table: 0.8254, Acc.toilet: 0.9126, Acc.flower: 0.5321, Acc.book: 0.6957, Acc.hill: 0.1925, Acc.bench: 0.5288, Acc.countertop: 0.8406, Acc.stove: 0.8136, Acc.palm: 0.7116, Acc.kitchen island: 0.6581, Acc.computer: 0.7367, Acc.swivel chair: 0.6653, Acc.boat: 0.5528, Acc.bar: 0.6931, Acc.arcade machine: 0.6912, Acc.hovel: 0.6771, Acc.bus: 0.9607, Acc.towel: 0.7960, Acc.light: 0.6514, Acc.truck: 0.3865, Acc.tower: 0.0880, Acc.chandelier: 0.8325, Acc.awning: 0.3534, Acc.streetlight: 0.3616, Acc.booth: 0.6111, Acc.television receiver: 0.8021, Acc.airplane: 0.6930, Acc.dirt track: 0.1714, Acc.apparel: 0.7022, Acc.pole: 0.3461, Acc.land: 0.0000, Acc.bannister: 0.1766, Acc.escalator: 0.6894, Acc.ottoman: 0.6157, Acc.bottle: 0.6192, Acc.buffet: 0.4556, Acc.poster: 0.4291, Acc.stage: 0.2073, Acc.van: 0.5957, Acc.ship: 0.8248, Acc.fountain: 0.3966, Acc.conveyer belt: 0.9676, Acc.canopy: 0.1803, Acc.washer: 0.7617, Acc.plaything: 0.3154, Acc.swimming pool: 0.8856, Acc.stool: 0.5177, Acc.barrel: 0.7922, Acc.basket: 0.5035, Acc.waterfall: 0.9221, Acc.tent: 0.9820, Acc.bag: 0.2408, Acc.minibike: 0.7852, Acc.cradle: 0.9749, Acc.oven: 0.6618, Acc.ball: 0.5529, Acc.food: 0.5617, Acc.step: 0.2029, Acc.tank: 0.5478, Acc.trade name: 0.3066, Acc.microwave: 0.9095, Acc.pot: 0.4800, Acc.animal: 0.5680, Acc.bicycle: 0.8053, Acc.lake: 0.6130, Acc.dishwasher: 0.6933, Acc.screen: 0.8217, Acc.blanket: 0.1934, Acc.sculpture: 0.8344, Acc.hood: 0.7513, Acc.sconce: 0.5872, Acc.vase: 0.5730, Acc.traffic light: 0.5066, Acc.tray: 0.1812, Acc.ashcan: 0.5759, Acc.fan: 0.7638, Acc.pier: 0.1311, Acc.crt screen: 0.1039, Acc.plate: 0.5553, Acc.monitor: 0.1133, Acc.bulletin board: 0.6488, Acc.shower: 0.0503, Acc.radiator: 0.7486, Acc.glass: 0.1471, Acc.clock: 0.4504, Acc.flag: 0.4969
2023-11-11 13:18:55,724 - mmseg - INFO - Iter [136050/160000]	lr: 5.428e-06, eta: 4:39:45, time: 2.553, data_time: 1.857, memory: 23129, decode.loss_ce: 0.1800, decode.acc_seg: 92.6083, loss: 0.1800
2023-11-11 13:19:27,444 - mmseg - INFO - Iter [136100/160000]	lr: 5.406e-06, eta: 4:39:10, time: 0.635, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1743, decode.acc_seg: 92.6290, loss: 0.1743
2023-11-11 13:19:58,822 - mmseg - INFO - Iter [136150/160000]	lr: 5.383e-06, eta: 4:38:34, time: 0.628, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1644, decode.acc_seg: 93.1259, loss: 0.1644
2023-11-11 13:20:33,293 - mmseg - INFO - Iter [136200/160000]	lr: 5.361e-06, eta: 4:37:59, time: 0.688, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1656, decode.acc_seg: 92.8576, loss: 0.1656
2023-11-11 13:21:06,562 - mmseg - INFO - Iter [136250/160000]	lr: 5.339e-06, eta: 4:37:24, time: 0.666, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1754, decode.acc_seg: 92.5919, loss: 0.1754
2023-11-11 13:21:42,476 - mmseg - INFO - Iter [136300/160000]	lr: 5.317e-06, eta: 4:36:49, time: 0.718, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1744, decode.acc_seg: 92.4876, loss: 0.1744
2023-11-11 13:22:17,991 - mmseg - INFO - Iter [136350/160000]	lr: 5.295e-06, eta: 4:36:14, time: 0.710, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1673, decode.acc_seg: 92.7414, loss: 0.1673
2023-11-11 13:22:53,427 - mmseg - INFO - Iter [136400/160000]	lr: 5.273e-06, eta: 4:35:39, time: 0.710, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1782, decode.acc_seg: 92.5443, loss: 0.1782
2023-11-11 13:23:27,796 - mmseg - INFO - Iter [136450/160000]	lr: 5.251e-06, eta: 4:35:04, time: 0.686, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1722, decode.acc_seg: 92.6512, loss: 0.1722
2023-11-11 13:24:01,951 - mmseg - INFO - Iter [136500/160000]	lr: 5.229e-06, eta: 4:34:28, time: 0.683, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1673, decode.acc_seg: 93.0525, loss: 0.1673
2023-11-11 13:24:33,514 - mmseg - INFO - Iter [136550/160000]	lr: 5.208e-06, eta: 4:33:53, time: 0.632, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1542, decode.acc_seg: 93.2716, loss: 0.1542
2023-11-11 13:25:08,387 - mmseg - INFO - Iter [136600/160000]	lr: 5.186e-06, eta: 4:33:18, time: 0.696, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1705, decode.acc_seg: 92.6629, loss: 0.1705
2023-11-11 13:25:43,836 - mmseg - INFO - Iter [136650/160000]	lr: 5.164e-06, eta: 4:32:43, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1681, decode.acc_seg: 92.7587, loss: 0.1681
2023-11-11 13:26:18,264 - mmseg - INFO - Iter [136700/160000]	lr: 5.142e-06, eta: 4:32:08, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1698, decode.acc_seg: 92.8311, loss: 0.1698
2023-11-11 13:26:51,893 - mmseg - INFO - Iter [136750/160000]	lr: 5.121e-06, eta: 4:31:32, time: 0.672, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1675, decode.acc_seg: 92.7978, loss: 0.1675
2023-11-11 13:27:27,443 - mmseg - INFO - Iter [136800/160000]	lr: 5.099e-06, eta: 4:30:57, time: 0.711, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1788, decode.acc_seg: 92.5232, loss: 0.1788
2023-11-11 13:28:01,686 - mmseg - INFO - Iter [136850/160000]	lr: 5.077e-06, eta: 4:30:22, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1703, decode.acc_seg: 92.7244, loss: 0.1703
2023-11-11 13:28:35,048 - mmseg - INFO - Iter [136900/160000]	lr: 5.056e-06, eta: 4:29:47, time: 0.666, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1733, decode.acc_seg: 92.9354, loss: 0.1733
2023-11-11 13:29:09,345 - mmseg - INFO - Iter [136950/160000]	lr: 5.034e-06, eta: 4:29:12, time: 0.686, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1733, decode.acc_seg: 92.6535, loss: 0.1733
2023-11-11 13:29:44,498 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 13:29:44,498 - mmseg - INFO - Iter [137000/160000]	lr: 5.013e-06, eta: 4:28:37, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1823, decode.acc_seg: 92.4631, loss: 0.1823
2023-11-11 13:30:19,665 - mmseg - INFO - Iter [137050/160000]	lr: 4.992e-06, eta: 4:28:02, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1646, decode.acc_seg: 92.9682, loss: 0.1646
2023-11-11 13:30:54,480 - mmseg - INFO - Iter [137100/160000]	lr: 4.970e-06, eta: 4:27:27, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1554, decode.acc_seg: 93.4020, loss: 0.1554
2023-11-11 13:31:29,371 - mmseg - INFO - Iter [137150/160000]	lr: 4.949e-06, eta: 4:26:52, time: 0.698, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1759, decode.acc_seg: 92.5627, loss: 0.1759
2023-11-11 13:32:04,891 - mmseg - INFO - Iter [137200/160000]	lr: 4.928e-06, eta: 4:26:17, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1745, decode.acc_seg: 92.6405, loss: 0.1745
2023-11-11 13:32:38,666 - mmseg - INFO - Iter [137250/160000]	lr: 4.906e-06, eta: 4:25:41, time: 0.675, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1610, decode.acc_seg: 93.2197, loss: 0.1610
2023-11-11 13:33:14,381 - mmseg - INFO - Iter [137300/160000]	lr: 4.885e-06, eta: 4:25:06, time: 0.714, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1717, decode.acc_seg: 92.9134, loss: 0.1717
2023-11-11 13:33:50,067 - mmseg - INFO - Iter [137350/160000]	lr: 4.864e-06, eta: 4:24:32, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1662, decode.acc_seg: 92.9730, loss: 0.1662
2023-11-11 13:34:25,770 - mmseg - INFO - Iter [137400/160000]	lr: 4.843e-06, eta: 4:23:57, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1683, decode.acc_seg: 92.7601, loss: 0.1683
2023-11-11 13:35:01,260 - mmseg - INFO - Iter [137450/160000]	lr: 4.822e-06, eta: 4:23:22, time: 0.710, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1749, decode.acc_seg: 92.4146, loss: 0.1749
2023-11-11 13:35:36,392 - mmseg - INFO - Iter [137500/160000]	lr: 4.801e-06, eta: 4:22:47, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1754, decode.acc_seg: 92.4796, loss: 0.1754
2023-11-11 13:36:09,787 - mmseg - INFO - Iter [137550/160000]	lr: 4.780e-06, eta: 4:22:11, time: 0.669, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1729, decode.acc_seg: 92.6551, loss: 0.1729
2023-11-11 13:36:41,710 - mmseg - INFO - Iter [137600/160000]	lr: 4.759e-06, eta: 4:21:36, time: 0.638, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1604, decode.acc_seg: 93.1826, loss: 0.1604
2023-11-11 13:37:15,948 - mmseg - INFO - Iter [137650/160000]	lr: 4.738e-06, eta: 4:21:01, time: 0.684, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1653, decode.acc_seg: 92.9637, loss: 0.1653
2023-11-11 13:37:51,992 - mmseg - INFO - Iter [137700/160000]	lr: 4.717e-06, eta: 4:20:26, time: 0.721, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1728, decode.acc_seg: 92.6837, loss: 0.1728
2023-11-11 13:38:25,982 - mmseg - INFO - Iter [137750/160000]	lr: 4.697e-06, eta: 4:19:51, time: 0.680, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1643, decode.acc_seg: 92.9559, loss: 0.1643
2023-11-11 13:39:01,435 - mmseg - INFO - Iter [137800/160000]	lr: 4.676e-06, eta: 4:19:16, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1673, decode.acc_seg: 92.8130, loss: 0.1673
2023-11-11 13:39:37,114 - mmseg - INFO - Iter [137850/160000]	lr: 4.655e-06, eta: 4:18:41, time: 0.714, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1720, decode.acc_seg: 92.7772, loss: 0.1720
2023-11-11 13:40:11,640 - mmseg - INFO - Iter [137900/160000]	lr: 4.634e-06, eta: 4:18:06, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1644, decode.acc_seg: 93.0400, loss: 0.1644
2023-11-11 13:40:44,593 - mmseg - INFO - Iter [137950/160000]	lr: 4.614e-06, eta: 4:17:30, time: 0.659, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1700, decode.acc_seg: 92.6026, loss: 0.1700
2023-11-11 13:41:17,429 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 13:41:17,429 - mmseg - INFO - Iter [138000/160000]	lr: 4.593e-06, eta: 4:16:55, time: 0.657, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1645, decode.acc_seg: 92.9930, loss: 0.1645
2023-11-11 13:41:50,120 - mmseg - INFO - Iter [138050/160000]	lr: 4.573e-06, eta: 4:16:19, time: 0.654, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1665, decode.acc_seg: 92.9765, loss: 0.1665
2023-11-11 13:42:22,589 - mmseg - INFO - Iter [138100/160000]	lr: 4.552e-06, eta: 4:15:44, time: 0.648, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1674, decode.acc_seg: 92.9038, loss: 0.1674
2023-11-11 13:42:57,171 - mmseg - INFO - Iter [138150/160000]	lr: 4.532e-06, eta: 4:15:09, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1661, decode.acc_seg: 93.0648, loss: 0.1661
2023-11-11 13:43:30,305 - mmseg - INFO - Iter [138200/160000]	lr: 4.511e-06, eta: 4:14:33, time: 0.663, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1636, decode.acc_seg: 92.8915, loss: 0.1636
2023-11-11 13:44:04,062 - mmseg - INFO - Iter [138250/160000]	lr: 4.491e-06, eta: 4:13:58, time: 0.674, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1681, decode.acc_seg: 92.9505, loss: 0.1681
2023-11-11 13:44:38,554 - mmseg - INFO - Iter [138300/160000]	lr: 4.471e-06, eta: 4:13:23, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1622, decode.acc_seg: 93.0795, loss: 0.1622
2023-11-11 13:45:10,083 - mmseg - INFO - Iter [138350/160000]	lr: 4.450e-06, eta: 4:12:48, time: 0.631, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1581, decode.acc_seg: 93.0058, loss: 0.1581
2023-11-11 13:45:42,484 - mmseg - INFO - Iter [138400/160000]	lr: 4.430e-06, eta: 4:12:12, time: 0.647, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1755, decode.acc_seg: 92.4886, loss: 0.1755
2023-11-11 13:46:17,989 - mmseg - INFO - Iter [138450/160000]	lr: 4.410e-06, eta: 4:11:37, time: 0.710, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1579, decode.acc_seg: 93.1446, loss: 0.1579
2023-11-11 13:46:53,790 - mmseg - INFO - Iter [138500/160000]	lr: 4.390e-06, eta: 4:11:02, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1700, decode.acc_seg: 92.7560, loss: 0.1700
2023-11-11 13:47:28,805 - mmseg - INFO - Iter [138550/160000]	lr: 4.370e-06, eta: 4:10:27, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1614, decode.acc_seg: 93.2562, loss: 0.1614
2023-11-11 13:48:00,844 - mmseg - INFO - Iter [138600/160000]	lr: 4.350e-06, eta: 4:09:52, time: 0.642, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1724, decode.acc_seg: 92.6305, loss: 0.1724
2023-11-11 13:48:35,557 - mmseg - INFO - Iter [138650/160000]	lr: 4.330e-06, eta: 4:09:17, time: 0.694, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1792, decode.acc_seg: 92.6204, loss: 0.1792
2023-11-11 13:49:11,501 - mmseg - INFO - Iter [138700/160000]	lr: 4.310e-06, eta: 4:08:42, time: 0.718, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1650, decode.acc_seg: 92.9983, loss: 0.1650
2023-11-11 13:49:48,796 - mmseg - INFO - Iter [138750/160000]	lr: 4.290e-06, eta: 4:08:07, time: 0.746, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1669, decode.acc_seg: 93.0301, loss: 0.1669
2023-11-11 13:50:23,353 - mmseg - INFO - Iter [138800/160000]	lr: 4.270e-06, eta: 4:07:32, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1681, decode.acc_seg: 92.7114, loss: 0.1681
2023-11-11 13:50:55,773 - mmseg - INFO - Iter [138850/160000]	lr: 4.250e-06, eta: 4:06:57, time: 0.647, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1636, decode.acc_seg: 93.0815, loss: 0.1636
2023-11-11 13:51:30,486 - mmseg - INFO - Iter [138900/160000]	lr: 4.230e-06, eta: 4:06:21, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1687, decode.acc_seg: 92.8694, loss: 0.1687
2023-11-11 13:52:07,798 - mmseg - INFO - Iter [138950/160000]	lr: 4.211e-06, eta: 4:05:47, time: 0.746, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1769, decode.acc_seg: 92.5784, loss: 0.1769
2023-11-11 13:52:44,356 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 13:52:44,356 - mmseg - INFO - Iter [139000/160000]	lr: 4.191e-06, eta: 4:05:12, time: 0.731, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1665, decode.acc_seg: 92.8824, loss: 0.1665
2023-11-11 13:53:21,260 - mmseg - INFO - Iter [139050/160000]	lr: 4.171e-06, eta: 4:04:37, time: 0.738, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1606, decode.acc_seg: 93.1909, loss: 0.1606
2023-11-11 13:53:57,932 - mmseg - INFO - Iter [139100/160000]	lr: 4.152e-06, eta: 4:04:02, time: 0.733, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1659, decode.acc_seg: 92.9395, loss: 0.1659
2023-11-11 13:54:34,775 - mmseg - INFO - Iter [139150/160000]	lr: 4.132e-06, eta: 4:03:28, time: 0.737, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1689, decode.acc_seg: 92.8076, loss: 0.1689
2023-11-11 13:55:11,574 - mmseg - INFO - Iter [139200/160000]	lr: 4.113e-06, eta: 4:02:53, time: 0.736, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1679, decode.acc_seg: 92.8052, loss: 0.1679
2023-11-11 13:55:44,961 - mmseg - INFO - Iter [139250/160000]	lr: 4.093e-06, eta: 4:02:18, time: 0.668, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1693, decode.acc_seg: 92.9640, loss: 0.1693
2023-11-11 13:56:19,933 - mmseg - INFO - Iter [139300/160000]	lr: 4.074e-06, eta: 4:01:43, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1720, decode.acc_seg: 92.7366, loss: 0.1720
2023-11-11 13:56:55,054 - mmseg - INFO - Iter [139350/160000]	lr: 4.054e-06, eta: 4:01:08, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1709, decode.acc_seg: 92.7835, loss: 0.1709
2023-11-11 13:57:30,316 - mmseg - INFO - Iter [139400/160000]	lr: 4.035e-06, eta: 4:00:33, time: 0.706, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1749, decode.acc_seg: 92.7176, loss: 0.1749
2023-11-11 13:58:05,633 - mmseg - INFO - Iter [139450/160000]	lr: 4.016e-06, eta: 3:59:58, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1696, decode.acc_seg: 92.9516, loss: 0.1696
2023-11-11 13:58:40,603 - mmseg - INFO - Iter [139500/160000]	lr: 3.996e-06, eta: 3:59:23, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1696, decode.acc_seg: 92.7813, loss: 0.1696
2023-11-11 13:59:15,816 - mmseg - INFO - Iter [139550/160000]	lr: 3.977e-06, eta: 3:58:48, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1679, decode.acc_seg: 93.0333, loss: 0.1679
2023-11-11 13:59:51,078 - mmseg - INFO - Iter [139600/160000]	lr: 3.958e-06, eta: 3:58:13, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1748, decode.acc_seg: 92.7588, loss: 0.1748
2023-11-11 14:00:26,523 - mmseg - INFO - Iter [139650/160000]	lr: 3.939e-06, eta: 3:57:38, time: 0.709, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1602, decode.acc_seg: 93.1638, loss: 0.1602
2023-11-11 14:00:58,610 - mmseg - INFO - Iter [139700/160000]	lr: 3.920e-06, eta: 3:57:02, time: 0.643, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1656, decode.acc_seg: 92.9338, loss: 0.1656
2023-11-11 14:01:32,357 - mmseg - INFO - Iter [139750/160000]	lr: 3.901e-06, eta: 3:56:27, time: 0.674, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1789, decode.acc_seg: 92.4236, loss: 0.1789
2023-11-11 14:02:07,520 - mmseg - INFO - Iter [139800/160000]	lr: 3.882e-06, eta: 3:55:52, time: 0.704, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1666, decode.acc_seg: 92.9324, loss: 0.1666
2023-11-11 14:02:42,124 - mmseg - INFO - Iter [139850/160000]	lr: 3.863e-06, eta: 3:55:17, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1657, decode.acc_seg: 93.0458, loss: 0.1657
2023-11-11 14:03:17,045 - mmseg - INFO - Iter [139900/160000]	lr: 3.844e-06, eta: 3:54:42, time: 0.699, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1717, decode.acc_seg: 92.8407, loss: 0.1717
2023-11-11 14:03:51,206 - mmseg - INFO - Iter [139950/160000]	lr: 3.825e-06, eta: 3:54:07, time: 0.683, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1621, decode.acc_seg: 93.0073, loss: 0.1621
2023-11-11 14:04:26,039 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 14:04:26,039 - mmseg - INFO - Iter [140000/160000]	lr: 3.806e-06, eta: 3:53:32, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1751, decode.acc_seg: 92.5767, loss: 0.1751
2023-11-11 14:05:00,707 - mmseg - INFO - Iter [140050/160000]	lr: 3.788e-06, eta: 3:52:56, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1683, decode.acc_seg: 92.6562, loss: 0.1683
2023-11-11 14:05:35,497 - mmseg - INFO - Iter [140100/160000]	lr: 3.769e-06, eta: 3:52:21, time: 0.696, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1793, decode.acc_seg: 92.7166, loss: 0.1793
2023-11-11 14:06:09,161 - mmseg - INFO - Iter [140150/160000]	lr: 3.750e-06, eta: 3:51:46, time: 0.674, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1630, decode.acc_seg: 93.0513, loss: 0.1630
2023-11-11 14:06:43,792 - mmseg - INFO - Iter [140200/160000]	lr: 3.732e-06, eta: 3:51:11, time: 0.692, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1707, decode.acc_seg: 92.7858, loss: 0.1707
2023-11-11 14:07:19,806 - mmseg - INFO - Iter [140250/160000]	lr: 3.713e-06, eta: 3:50:36, time: 0.720, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1710, decode.acc_seg: 92.7314, loss: 0.1710
2023-11-11 14:07:51,300 - mmseg - INFO - Iter [140300/160000]	lr: 3.694e-06, eta: 3:50:01, time: 0.631, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1568, decode.acc_seg: 93.2603, loss: 0.1568
2023-11-11 14:08:24,091 - mmseg - INFO - Iter [140350/160000]	lr: 3.676e-06, eta: 3:49:25, time: 0.656, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1708, decode.acc_seg: 92.8495, loss: 0.1708
2023-11-11 14:08:57,959 - mmseg - INFO - Iter [140400/160000]	lr: 3.658e-06, eta: 3:48:50, time: 0.677, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1782, decode.acc_seg: 92.7040, loss: 0.1782
2023-11-11 14:09:33,251 - mmseg - INFO - Iter [140450/160000]	lr: 3.639e-06, eta: 3:48:15, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1719, decode.acc_seg: 92.7661, loss: 0.1719
2023-11-11 14:10:08,005 - mmseg - INFO - Iter [140500/160000]	lr: 3.621e-06, eta: 3:47:40, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1665, decode.acc_seg: 92.7188, loss: 0.1665
2023-11-11 14:10:42,228 - mmseg - INFO - Iter [140550/160000]	lr: 3.602e-06, eta: 3:47:05, time: 0.685, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1687, decode.acc_seg: 92.9098, loss: 0.1687
2023-11-11 14:11:17,532 - mmseg - INFO - Iter [140600/160000]	lr: 3.584e-06, eta: 3:46:30, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1640, decode.acc_seg: 93.0216, loss: 0.1640
2023-11-11 14:11:52,919 - mmseg - INFO - Iter [140650/160000]	lr: 3.566e-06, eta: 3:45:55, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1644, decode.acc_seg: 93.1062, loss: 0.1644
2023-11-11 14:12:26,128 - mmseg - INFO - Iter [140700/160000]	lr: 3.548e-06, eta: 3:45:20, time: 0.665, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1643, decode.acc_seg: 93.0378, loss: 0.1643
2023-11-11 14:12:58,661 - mmseg - INFO - Iter [140750/160000]	lr: 3.530e-06, eta: 3:44:44, time: 0.650, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1682, decode.acc_seg: 92.8099, loss: 0.1682
2023-11-11 14:13:34,261 - mmseg - INFO - Iter [140800/160000]	lr: 3.512e-06, eta: 3:44:09, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1728, decode.acc_seg: 92.6919, loss: 0.1728
2023-11-11 14:14:09,004 - mmseg - INFO - Iter [140850/160000]	lr: 3.493e-06, eta: 3:43:34, time: 0.693, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1726, decode.acc_seg: 92.4029, loss: 0.1726
2023-11-11 14:14:43,747 - mmseg - INFO - Iter [140900/160000]	lr: 3.475e-06, eta: 3:42:59, time: 0.697, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1658, decode.acc_seg: 92.8064, loss: 0.1658
2023-11-11 14:15:16,119 - mmseg - INFO - Iter [140950/160000]	lr: 3.458e-06, eta: 3:42:24, time: 0.646, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1662, decode.acc_seg: 93.0482, loss: 0.1662
2023-11-11 14:15:50,994 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 14:15:50,994 - mmseg - INFO - Iter [141000/160000]	lr: 3.440e-06, eta: 3:41:49, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1603, decode.acc_seg: 93.0682, loss: 0.1603
2023-11-11 14:16:24,676 - mmseg - INFO - Iter [141050/160000]	lr: 3.422e-06, eta: 3:41:14, time: 0.674, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1782, decode.acc_seg: 92.5746, loss: 0.1782
2023-11-11 14:17:00,376 - mmseg - INFO - Iter [141100/160000]	lr: 3.404e-06, eta: 3:40:39, time: 0.714, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1682, decode.acc_seg: 92.7966, loss: 0.1682
2023-11-11 14:17:33,951 - mmseg - INFO - Iter [141150/160000]	lr: 3.386e-06, eta: 3:40:03, time: 0.671, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1637, decode.acc_seg: 93.0889, loss: 0.1637
2023-11-11 14:18:08,830 - mmseg - INFO - Iter [141200/160000]	lr: 3.368e-06, eta: 3:39:28, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1629, decode.acc_seg: 92.9518, loss: 0.1629
2023-11-11 14:18:44,226 - mmseg - INFO - Iter [141250/160000]	lr: 3.351e-06, eta: 3:38:53, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1664, decode.acc_seg: 92.9011, loss: 0.1664
2023-11-11 14:19:19,685 - mmseg - INFO - Iter [141300/160000]	lr: 3.333e-06, eta: 3:38:18, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1765, decode.acc_seg: 92.5790, loss: 0.1765
2023-11-11 14:19:55,235 - mmseg - INFO - Iter [141350/160000]	lr: 3.315e-06, eta: 3:37:44, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1639, decode.acc_seg: 93.0535, loss: 0.1639
2023-11-11 14:20:30,515 - mmseg - INFO - Iter [141400/160000]	lr: 3.298e-06, eta: 3:37:09, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1667, decode.acc_seg: 92.9236, loss: 0.1667
2023-11-11 14:21:05,718 - mmseg - INFO - Iter [141450/160000]	lr: 3.280e-06, eta: 3:36:34, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1709, decode.acc_seg: 92.8449, loss: 0.1709
2023-11-11 14:21:38,472 - mmseg - INFO - Iter [141500/160000]	lr: 3.263e-06, eta: 3:35:58, time: 0.657, data_time: 0.012, memory: 23129, decode.loss_ce: 0.1755, decode.acc_seg: 92.5353, loss: 0.1755
2023-11-11 14:22:09,986 - mmseg - INFO - Iter [141550/160000]	lr: 3.246e-06, eta: 3:35:23, time: 0.630, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1549, decode.acc_seg: 93.4660, loss: 0.1549
2023-11-11 14:22:42,095 - mmseg - INFO - Iter [141600/160000]	lr: 3.228e-06, eta: 3:34:47, time: 0.642, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1718, decode.acc_seg: 92.8029, loss: 0.1718
2023-11-11 14:23:19,115 - mmseg - INFO - Iter [141650/160000]	lr: 3.211e-06, eta: 3:34:13, time: 0.740, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1619, decode.acc_seg: 93.0674, loss: 0.1619
2023-11-11 14:23:56,479 - mmseg - INFO - Iter [141700/160000]	lr: 3.194e-06, eta: 3:33:38, time: 0.747, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1633, decode.acc_seg: 93.0473, loss: 0.1633
2023-11-11 14:24:33,623 - mmseg - INFO - Iter [141750/160000]	lr: 3.176e-06, eta: 3:33:03, time: 0.743, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1678, decode.acc_seg: 92.7621, loss: 0.1678
2023-11-11 14:25:10,662 - mmseg - INFO - Iter [141800/160000]	lr: 3.159e-06, eta: 3:32:28, time: 0.741, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1621, decode.acc_seg: 93.2091, loss: 0.1621
2023-11-11 14:25:44,325 - mmseg - INFO - Iter [141850/160000]	lr: 3.142e-06, eta: 3:31:53, time: 0.673, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1533, decode.acc_seg: 93.4876, loss: 0.1533
2023-11-11 14:26:15,798 - mmseg - INFO - Iter [141900/160000]	lr: 3.125e-06, eta: 3:31:18, time: 0.630, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1669, decode.acc_seg: 92.8104, loss: 0.1669
2023-11-11 14:26:50,172 - mmseg - INFO - Iter [141950/160000]	lr: 3.108e-06, eta: 3:30:43, time: 0.687, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1714, decode.acc_seg: 92.9005, loss: 0.1714
2023-11-11 14:27:23,666 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 14:27:23,667 - mmseg - INFO - Iter [142000/160000]	lr: 3.091e-06, eta: 3:30:07, time: 0.670, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1732, decode.acc_seg: 92.6520, loss: 0.1732
2023-11-11 14:27:59,310 - mmseg - INFO - Iter [142050/160000]	lr: 3.074e-06, eta: 3:29:32, time: 0.712, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1695, decode.acc_seg: 92.7521, loss: 0.1695
2023-11-11 14:28:34,172 - mmseg - INFO - Iter [142100/160000]	lr: 3.057e-06, eta: 3:28:57, time: 0.698, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1580, decode.acc_seg: 93.1096, loss: 0.1580
2023-11-11 14:29:07,071 - mmseg - INFO - Iter [142150/160000]	lr: 3.040e-06, eta: 3:28:22, time: 0.658, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1606, decode.acc_seg: 93.2125, loss: 0.1606
2023-11-11 14:29:39,787 - mmseg - INFO - Iter [142200/160000]	lr: 3.023e-06, eta: 3:27:47, time: 0.654, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1689, decode.acc_seg: 92.6839, loss: 0.1689
2023-11-11 14:30:13,447 - mmseg - INFO - Iter [142250/160000]	lr: 3.006e-06, eta: 3:27:12, time: 0.673, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1732, decode.acc_seg: 92.6743, loss: 0.1732
2023-11-11 14:30:46,554 - mmseg - INFO - Iter [142300/160000]	lr: 2.990e-06, eta: 3:26:36, time: 0.662, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1599, decode.acc_seg: 93.1505, loss: 0.1599
2023-11-11 14:31:22,174 - mmseg - INFO - Iter [142350/160000]	lr: 2.973e-06, eta: 3:26:01, time: 0.711, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1697, decode.acc_seg: 92.8314, loss: 0.1697
2023-11-11 14:31:57,177 - mmseg - INFO - Iter [142400/160000]	lr: 2.956e-06, eta: 3:25:26, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1615, decode.acc_seg: 93.0603, loss: 0.1615
2023-11-11 14:32:30,415 - mmseg - INFO - Iter [142450/160000]	lr: 2.940e-06, eta: 3:24:51, time: 0.664, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1805, decode.acc_seg: 92.4635, loss: 0.1805
2023-11-11 14:33:02,125 - mmseg - INFO - Iter [142500/160000]	lr: 2.923e-06, eta: 3:24:16, time: 0.634, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1672, decode.acc_seg: 92.8409, loss: 0.1672
2023-11-11 14:33:36,818 - mmseg - INFO - Iter [142550/160000]	lr: 2.907e-06, eta: 3:23:41, time: 0.694, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1677, decode.acc_seg: 92.8335, loss: 0.1677
2023-11-11 14:34:11,318 - mmseg - INFO - Iter [142600/160000]	lr: 2.890e-06, eta: 3:23:06, time: 0.689, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1663, decode.acc_seg: 93.0333, loss: 0.1663
2023-11-11 14:34:46,703 - mmseg - INFO - Iter [142650/160000]	lr: 2.874e-06, eta: 3:22:31, time: 0.708, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1664, decode.acc_seg: 92.9526, loss: 0.1664
2023-11-11 14:35:19,095 - mmseg - INFO - Iter [142700/160000]	lr: 2.857e-06, eta: 3:21:55, time: 0.649, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1672, decode.acc_seg: 92.6343, loss: 0.1672
2023-11-11 14:35:50,543 - mmseg - INFO - Iter [142750/160000]	lr: 2.841e-06, eta: 3:21:20, time: 0.629, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1723, decode.acc_seg: 92.6725, loss: 0.1723
2023-11-11 14:36:25,634 - mmseg - INFO - Iter [142800/160000]	lr: 2.825e-06, eta: 3:20:45, time: 0.701, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1759, decode.acc_seg: 92.4118, loss: 0.1759
2023-11-11 14:36:58,246 - mmseg - INFO - Iter [142850/160000]	lr: 2.808e-06, eta: 3:20:09, time: 0.652, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1776, decode.acc_seg: 92.6761, loss: 0.1776
2023-11-11 14:37:30,922 - mmseg - INFO - Iter [142900/160000]	lr: 2.792e-06, eta: 3:19:34, time: 0.654, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1677, decode.acc_seg: 92.7394, loss: 0.1677
2023-11-11 14:38:06,167 - mmseg - INFO - Iter [142950/160000]	lr: 2.776e-06, eta: 3:18:59, time: 0.704, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1648, decode.acc_seg: 92.9669, loss: 0.1648
2023-11-11 14:38:41,467 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 14:38:41,467 - mmseg - INFO - Iter [143000/160000]	lr: 2.760e-06, eta: 3:18:24, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1689, decode.acc_seg: 92.7840, loss: 0.1689
2023-11-11 14:39:14,098 - mmseg - INFO - Iter [143050/160000]	lr: 2.744e-06, eta: 3:17:49, time: 0.653, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1672, decode.acc_seg: 93.0101, loss: 0.1672
2023-11-11 14:39:46,047 - mmseg - INFO - Iter [143100/160000]	lr: 2.728e-06, eta: 3:17:14, time: 0.638, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1594, decode.acc_seg: 93.1825, loss: 0.1594
2023-11-11 14:40:21,140 - mmseg - INFO - Iter [143150/160000]	lr: 2.712e-06, eta: 3:16:39, time: 0.702, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1642, decode.acc_seg: 92.9778, loss: 0.1642
2023-11-11 14:40:56,188 - mmseg - INFO - Iter [143200/160000]	lr: 2.696e-06, eta: 3:16:04, time: 0.701, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1635, decode.acc_seg: 92.9951, loss: 0.1635
2023-11-11 14:41:28,071 - mmseg - INFO - Iter [143250/160000]	lr: 2.680e-06, eta: 3:15:28, time: 0.639, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1720, decode.acc_seg: 92.5847, loss: 0.1720
2023-11-11 14:42:01,268 - mmseg - INFO - Iter [143300/160000]	lr: 2.664e-06, eta: 3:14:53, time: 0.663, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1751, decode.acc_seg: 92.5144, loss: 0.1751
2023-11-11 14:42:36,847 - mmseg - INFO - Iter [143350/160000]	lr: 2.649e-06, eta: 3:14:18, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1671, decode.acc_seg: 92.9878, loss: 0.1671
2023-11-11 14:43:12,383 - mmseg - INFO - Iter [143400/160000]	lr: 2.633e-06, eta: 3:13:43, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1619, decode.acc_seg: 93.2182, loss: 0.1619
2023-11-11 14:43:45,142 - mmseg - INFO - Iter [143450/160000]	lr: 2.617e-06, eta: 3:13:08, time: 0.656, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1617, decode.acc_seg: 93.0284, loss: 0.1617
2023-11-11 14:44:18,580 - mmseg - INFO - Iter [143500/160000]	lr: 2.601e-06, eta: 3:12:33, time: 0.668, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1601, decode.acc_seg: 92.9656, loss: 0.1601
2023-11-11 14:44:53,836 - mmseg - INFO - Iter [143550/160000]	lr: 2.586e-06, eta: 3:11:58, time: 0.705, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1574, decode.acc_seg: 93.3579, loss: 0.1574
2023-11-11 14:45:29,111 - mmseg - INFO - Iter [143600/160000]	lr: 2.570e-06, eta: 3:11:23, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1652, decode.acc_seg: 93.0939, loss: 0.1652
2023-11-11 14:46:04,316 - mmseg - INFO - Iter [143650/160000]	lr: 2.555e-06, eta: 3:10:48, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1747, decode.acc_seg: 92.7210, loss: 0.1747
2023-11-11 14:46:39,187 - mmseg - INFO - Iter [143700/160000]	lr: 2.539e-06, eta: 3:10:13, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1749, decode.acc_seg: 92.6122, loss: 0.1749
2023-11-11 14:47:12,955 - mmseg - INFO - Iter [143750/160000]	lr: 2.524e-06, eta: 3:09:37, time: 0.676, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1622, decode.acc_seg: 93.2534, loss: 0.1622
2023-11-11 14:47:44,810 - mmseg - INFO - Iter [143800/160000]	lr: 2.509e-06, eta: 3:09:02, time: 0.637, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1661, decode.acc_seg: 92.8466, loss: 0.1661
2023-11-11 14:48:17,760 - mmseg - INFO - Iter [143850/160000]	lr: 2.493e-06, eta: 3:08:27, time: 0.659, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1741, decode.acc_seg: 92.8422, loss: 0.1741
2023-11-11 14:48:49,112 - mmseg - INFO - Iter [143900/160000]	lr: 2.478e-06, eta: 3:07:51, time: 0.627, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1663, decode.acc_seg: 92.7653, loss: 0.1663
2023-11-11 14:49:20,530 - mmseg - INFO - Iter [143950/160000]	lr: 2.463e-06, eta: 3:07:16, time: 0.628, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1778, decode.acc_seg: 92.4369, loss: 0.1778
2023-11-11 14:49:54,588 - mmseg - INFO - Saving checkpoint at 144000 iterations
2023-11-11 14:49:59,435 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 14:49:59,435 - mmseg - INFO - Iter [144000/160000]	lr: 2.447e-06, eta: 3:06:41, time: 0.778, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1708, decode.acc_seg: 92.7522, loss: 0.1708
2023-11-11 14:51:27,552 - mmseg - INFO - per class results:
2023-11-11 14:51:27,566 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 77.95 | 88.16 |
|       building      | 81.71 | 91.53 |
|         sky         | 94.57 | 97.66 |
|        floor        | 82.92 | 90.19 |
|         tree        | 74.85 | 88.57 |
|       ceiling       | 83.68 | 92.24 |
|         road        | 82.73 | 89.41 |
|         bed         |  89.6 | 96.21 |
|      windowpane     | 61.95 | 78.71 |
|        grass        | 66.38 | 82.79 |
|       cabinet       | 61.04 | 74.43 |
|       sidewalk      | 65.39 | 81.84 |
|        person       | 81.48 | 93.91 |
|        earth        | 39.54 | 54.96 |
|         door        | 50.39 | 64.06 |
|        table        | 63.28 | 78.13 |
|       mountain      | 56.33 | 70.43 |
|        plant        | 52.59 | 61.95 |
|       curtain       | 74.14 | 86.63 |
|        chair        | 57.87 | 71.74 |
|         car         | 84.03 | 92.16 |
|        water        | 60.46 | 78.17 |
|       painting      | 71.16 | 88.06 |
|         sofa        | 67.44 | 83.76 |
|        shelf        | 43.02 | 60.16 |
|        house        | 45.54 |  62.2 |
|         sea         | 60.81 | 76.65 |
|        mirror       | 69.15 | 76.67 |
|         rug         | 63.95 | 75.55 |
|        field        |  31.1 | 49.34 |
|       armchair      | 44.17 | 61.53 |
|         seat        | 62.89 | 80.45 |
|        fence        | 42.26 | 57.09 |
|         desk        | 51.01 |  72.1 |
|         rock        | 35.95 | 57.04 |
|       wardrobe      | 49.44 | 68.34 |
|         lamp        | 65.72 | 76.46 |
|       bathtub       | 81.63 | 87.15 |
|       railing       | 31.01 | 43.58 |
|       cushion       |  59.4 | 72.78 |
|         base        | 31.34 | 40.18 |
|         box         | 30.48 | 37.98 |
|        column       | 46.58 | 55.55 |
|      signboard      | 40.28 | 53.94 |
|   chest of drawers  | 47.12 | 58.32 |
|       counter       | 38.78 | 46.33 |
|         sand        | 51.96 | 74.49 |
|         sink        | 71.81 | 80.71 |
|      skyscraper     | 47.71 | 59.41 |
|      fireplace      | 75.22 | 89.28 |
|     refrigerator    | 78.44 | 84.96 |
|      grandstand     | 38.27 | 68.24 |
|         path        | 24.74 | 38.46 |
|        stairs       | 30.16 | 40.71 |
|        runway       | 64.51 | 82.74 |
|         case        | 47.71 | 58.22 |
|      pool table     | 93.15 | 96.63 |
|        pillow       | 59.55 | 69.65 |
|     screen door     | 68.87 | 83.04 |
|       stairway      | 29.85 | 37.39 |
|        river        | 11.24 | 21.59 |
|        bridge       | 70.05 | 78.34 |
|       bookcase      | 37.25 | 56.55 |
|        blind        | 43.37 | 48.21 |
|     coffee table    | 53.19 | 82.86 |
|        toilet       | 86.02 |  91.1 |
|        flower       | 36.71 | 53.03 |
|         book        | 46.17 | 67.61 |
|         hill        | 11.26 | 19.42 |
|        bench        | 46.16 | 53.23 |
|      countertop     | 59.71 | 83.57 |
|        stove        | 73.53 | 82.27 |
|         palm        | 48.41 | 70.48 |
|    kitchen island   | 41.56 | 73.92 |
|       computer      | 64.11 | 73.52 |
|     swivel chair    | 48.35 | 68.45 |
|         boat        | 45.79 | 54.96 |
|         bar         | 50.37 | 64.28 |
|    arcade machine   |  77.3 | 83.32 |
|        hovel        | 56.54 | 64.21 |
|         bus         | 86.12 | 96.32 |
|        towel        | 65.84 | 77.87 |
|        light        | 55.49 | 65.92 |
|        truck        | 32.88 | 41.19 |
|        tower        |  6.12 |  8.71 |
|      chandelier     | 66.88 | 83.52 |
|        awning       | 27.81 | 36.55 |
|     streetlight     | 28.37 | 37.03 |
|        booth        | 61.95 | 65.05 |
| television receiver | 66.38 | 81.73 |
|       airplane      | 61.34 | 74.11 |
|      dirt track     | 10.97 | 17.51 |
|       apparel       | 48.56 | 70.55 |
|         pole        | 26.45 |  35.4 |
|         land        |  0.01 |  0.01 |
|      bannister      | 13.49 | 17.58 |
|      escalator      | 39.65 | 51.23 |
|       ottoman       | 46.44 | 62.26 |
|        bottle       | 36.16 | 62.94 |
|        buffet       | 44.15 | 50.48 |
|        poster       | 33.42 | 43.97 |
|        stage        | 16.03 | 21.66 |
|         van         | 43.41 | 62.52 |
|         ship        | 55.54 | 80.52 |
|       fountain      |  43.9 | 44.41 |
|    conveyer belt    | 79.35 | 96.13 |
|        canopy       |  17.2 | 26.47 |
|        washer       | 75.35 | 76.19 |
|      plaything      | 20.93 | 32.29 |
|    swimming pool    | 78.97 | 89.12 |
|        stool        | 38.91 | 53.67 |
|        barrel       | 62.42 | 75.95 |
|        basket       | 36.48 | 47.87 |
|      waterfall      | 64.92 | 92.35 |
|         tent        | 96.23 | 98.37 |
|         bag         |  18.5 | 23.75 |
|       minibike      | 69.69 | 84.16 |
|        cradle       |  81.1 | 97.37 |
|         oven        | 42.27 | 65.95 |
|         ball        | 42.05 | 60.14 |
|         food        | 50.19 | 55.83 |
|         step        | 13.87 | 17.92 |
|         tank        | 48.63 | 55.49 |
|      trade name     |  27.0 | 31.03 |
|      microwave      |  74.9 | 85.02 |
|         pot         | 42.74 | 48.37 |
|        animal       | 54.06 | 56.49 |
|       bicycle       | 58.48 | 80.75 |
|         lake        | 55.26 | 60.05 |
|      dishwasher     | 60.01 | 73.32 |
|        screen       | 60.14 | 85.77 |
|       blanket       | 13.55 | 16.32 |
|      sculpture      | 64.04 | 83.02 |
|         hood        | 62.08 | 74.99 |
|        sconce       | 48.97 |  59.8 |
|         vase        | 38.62 | 53.06 |
|    traffic light    | 28.63 | 47.38 |
|         tray        |  9.52 | 18.77 |
|        ashcan       | 41.35 | 55.66 |
|         fan         | 60.95 |  76.9 |
|         pier        | 11.69 | 12.36 |
|      crt screen     |  2.83 |  10.7 |
|        plate        | 42.34 | 54.93 |
|       monitor       |  7.28 |  8.05 |
|    bulletin board   | 51.95 | 61.83 |
|        shower       |  2.63 |  5.45 |
|       radiator      | 66.28 | 73.13 |
|        glass        | 14.19 | 15.12 |
|        clock        | 35.77 | 44.38 |
|         flag        | 45.75 | 49.61 |
+---------------------+-------+-------+
2023-11-11 14:51:27,566 - mmseg - INFO - Summary:
2023-11-11 14:51:27,567 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 83.36 | 50.52 | 62.54 |
+-------+-------+-------+
2023-11-11 14:51:27,701 - mmseg - INFO - The previous best checkpoint /data/Next-ViT/segmentation/work_dirs/fpn_512_debi_base_160k/best_mIoU_iter_136000.pth was removed
2023-11-11 14:51:30,574 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_144000.pth.
2023-11-11 14:51:30,575 - mmseg - INFO - Best mIoU is 0.5052 at 144000 iter.
2023-11-11 14:51:30,616 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 14:51:30,616 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8336, mIoU: 0.5052, mAcc: 0.6254, IoU.wall: 0.7795, IoU.building: 0.8171, IoU.sky: 0.9457, IoU.floor: 0.8292, IoU.tree: 0.7485, IoU.ceiling: 0.8368, IoU.road: 0.8273, IoU.bed : 0.8960, IoU.windowpane: 0.6195, IoU.grass: 0.6638, IoU.cabinet: 0.6104, IoU.sidewalk: 0.6539, IoU.person: 0.8148, IoU.earth: 0.3954, IoU.door: 0.5039, IoU.table: 0.6328, IoU.mountain: 0.5633, IoU.plant: 0.5259, IoU.curtain: 0.7414, IoU.chair: 0.5787, IoU.car: 0.8403, IoU.water: 0.6046, IoU.painting: 0.7116, IoU.sofa: 0.6744, IoU.shelf: 0.4302, IoU.house: 0.4554, IoU.sea: 0.6081, IoU.mirror: 0.6915, IoU.rug: 0.6395, IoU.field: 0.3110, IoU.armchair: 0.4417, IoU.seat: 0.6289, IoU.fence: 0.4226, IoU.desk: 0.5101, IoU.rock: 0.3595, IoU.wardrobe: 0.4944, IoU.lamp: 0.6572, IoU.bathtub: 0.8163, IoU.railing: 0.3101, IoU.cushion: 0.5940, IoU.base: 0.3134, IoU.box: 0.3048, IoU.column: 0.4658, IoU.signboard: 0.4028, IoU.chest of drawers: 0.4712, IoU.counter: 0.3878, IoU.sand: 0.5196, IoU.sink: 0.7181, IoU.skyscraper: 0.4771, IoU.fireplace: 0.7522, IoU.refrigerator: 0.7844, IoU.grandstand: 0.3827, IoU.path: 0.2474, IoU.stairs: 0.3016, IoU.runway: 0.6451, IoU.case: 0.4771, IoU.pool table: 0.9315, IoU.pillow: 0.5955, IoU.screen door: 0.6887, IoU.stairway: 0.2985, IoU.river: 0.1124, IoU.bridge: 0.7005, IoU.bookcase: 0.3725, IoU.blind: 0.4337, IoU.coffee table: 0.5319, IoU.toilet: 0.8602, IoU.flower: 0.3671, IoU.book: 0.4617, IoU.hill: 0.1126, IoU.bench: 0.4616, IoU.countertop: 0.5971, IoU.stove: 0.7353, IoU.palm: 0.4841, IoU.kitchen island: 0.4156, IoU.computer: 0.6411, IoU.swivel chair: 0.4835, IoU.boat: 0.4579, IoU.bar: 0.5037, IoU.arcade machine: 0.7730, IoU.hovel: 0.5654, IoU.bus: 0.8612, IoU.towel: 0.6584, IoU.light: 0.5549, IoU.truck: 0.3288, IoU.tower: 0.0612, IoU.chandelier: 0.6688, IoU.awning: 0.2781, IoU.streetlight: 0.2837, IoU.booth: 0.6195, IoU.television receiver: 0.6638, IoU.airplane: 0.6134, IoU.dirt track: 0.1097, IoU.apparel: 0.4856, IoU.pole: 0.2645, IoU.land: 0.0001, IoU.bannister: 0.1349, IoU.escalator: 0.3965, IoU.ottoman: 0.4644, IoU.bottle: 0.3616, IoU.buffet: 0.4415, IoU.poster: 0.3342, IoU.stage: 0.1603, IoU.van: 0.4341, IoU.ship: 0.5554, IoU.fountain: 0.4390, IoU.conveyer belt: 0.7935, IoU.canopy: 0.1720, IoU.washer: 0.7535, IoU.plaything: 0.2093, IoU.swimming pool: 0.7897, IoU.stool: 0.3891, IoU.barrel: 0.6242, IoU.basket: 0.3648, IoU.waterfall: 0.6492, IoU.tent: 0.9623, IoU.bag: 0.1850, IoU.minibike: 0.6969, IoU.cradle: 0.8110, IoU.oven: 0.4227, IoU.ball: 0.4205, IoU.food: 0.5019, IoU.step: 0.1387, IoU.tank: 0.4863, IoU.trade name: 0.2700, IoU.microwave: 0.7490, IoU.pot: 0.4274, IoU.animal: 0.5406, IoU.bicycle: 0.5848, IoU.lake: 0.5526, IoU.dishwasher: 0.6001, IoU.screen: 0.6014, IoU.blanket: 0.1355, IoU.sculpture: 0.6404, IoU.hood: 0.6208, IoU.sconce: 0.4897, IoU.vase: 0.3862, IoU.traffic light: 0.2863, IoU.tray: 0.0952, IoU.ashcan: 0.4135, IoU.fan: 0.6095, IoU.pier: 0.1169, IoU.crt screen: 0.0283, IoU.plate: 0.4234, IoU.monitor: 0.0728, IoU.bulletin board: 0.5195, IoU.shower: 0.0263, IoU.radiator: 0.6628, IoU.glass: 0.1419, IoU.clock: 0.3577, IoU.flag: 0.4575, Acc.wall: 0.8816, Acc.building: 0.9153, Acc.sky: 0.9766, Acc.floor: 0.9019, Acc.tree: 0.8857, Acc.ceiling: 0.9224, Acc.road: 0.8941, Acc.bed : 0.9621, Acc.windowpane: 0.7871, Acc.grass: 0.8279, Acc.cabinet: 0.7443, Acc.sidewalk: 0.8184, Acc.person: 0.9391, Acc.earth: 0.5496, Acc.door: 0.6406, Acc.table: 0.7813, Acc.mountain: 0.7043, Acc.plant: 0.6195, Acc.curtain: 0.8663, Acc.chair: 0.7174, Acc.car: 0.9216, Acc.water: 0.7817, Acc.painting: 0.8806, Acc.sofa: 0.8376, Acc.shelf: 0.6016, Acc.house: 0.6220, Acc.sea: 0.7665, Acc.mirror: 0.7667, Acc.rug: 0.7555, Acc.field: 0.4934, Acc.armchair: 0.6153, Acc.seat: 0.8045, Acc.fence: 0.5709, Acc.desk: 0.7210, Acc.rock: 0.5704, Acc.wardrobe: 0.6834, Acc.lamp: 0.7646, Acc.bathtub: 0.8715, Acc.railing: 0.4358, Acc.cushion: 0.7278, Acc.base: 0.4018, Acc.box: 0.3798, Acc.column: 0.5555, Acc.signboard: 0.5394, Acc.chest of drawers: 0.5832, Acc.counter: 0.4633, Acc.sand: 0.7449, Acc.sink: 0.8071, Acc.skyscraper: 0.5941, Acc.fireplace: 0.8928, Acc.refrigerator: 0.8496, Acc.grandstand: 0.6824, Acc.path: 0.3846, Acc.stairs: 0.4071, Acc.runway: 0.8274, Acc.case: 0.5822, Acc.pool table: 0.9663, Acc.pillow: 0.6965, Acc.screen door: 0.8304, Acc.stairway: 0.3739, Acc.river: 0.2159, Acc.bridge: 0.7834, Acc.bookcase: 0.5655, Acc.blind: 0.4821, Acc.coffee table: 0.8286, Acc.toilet: 0.9110, Acc.flower: 0.5303, Acc.book: 0.6761, Acc.hill: 0.1942, Acc.bench: 0.5323, Acc.countertop: 0.8357, Acc.stove: 0.8227, Acc.palm: 0.7048, Acc.kitchen island: 0.7392, Acc.computer: 0.7352, Acc.swivel chair: 0.6845, Acc.boat: 0.5496, Acc.bar: 0.6428, Acc.arcade machine: 0.8332, Acc.hovel: 0.6421, Acc.bus: 0.9632, Acc.towel: 0.7787, Acc.light: 0.6592, Acc.truck: 0.4119, Acc.tower: 0.0871, Acc.chandelier: 0.8352, Acc.awning: 0.3655, Acc.streetlight: 0.3703, Acc.booth: 0.6505, Acc.television receiver: 0.8173, Acc.airplane: 0.7411, Acc.dirt track: 0.1751, Acc.apparel: 0.7055, Acc.pole: 0.3540, Acc.land: 0.0001, Acc.bannister: 0.1758, Acc.escalator: 0.5123, Acc.ottoman: 0.6226, Acc.bottle: 0.6294, Acc.buffet: 0.5048, Acc.poster: 0.4397, Acc.stage: 0.2166, Acc.van: 0.6252, Acc.ship: 0.8052, Acc.fountain: 0.4441, Acc.conveyer belt: 0.9613, Acc.canopy: 0.2647, Acc.washer: 0.7619, Acc.plaything: 0.3229, Acc.swimming pool: 0.8912, Acc.stool: 0.5367, Acc.barrel: 0.7595, Acc.basket: 0.4787, Acc.waterfall: 0.9235, Acc.tent: 0.9837, Acc.bag: 0.2375, Acc.minibike: 0.8416, Acc.cradle: 0.9737, Acc.oven: 0.6595, Acc.ball: 0.6014, Acc.food: 0.5583, Acc.step: 0.1792, Acc.tank: 0.5549, Acc.trade name: 0.3103, Acc.microwave: 0.8502, Acc.pot: 0.4837, Acc.animal: 0.5649, Acc.bicycle: 0.8075, Acc.lake: 0.6005, Acc.dishwasher: 0.7332, Acc.screen: 0.8577, Acc.blanket: 0.1632, Acc.sculpture: 0.8302, Acc.hood: 0.7499, Acc.sconce: 0.5980, Acc.vase: 0.5306, Acc.traffic light: 0.4738, Acc.tray: 0.1877, Acc.ashcan: 0.5566, Acc.fan: 0.7690, Acc.pier: 0.1236, Acc.crt screen: 0.1070, Acc.plate: 0.5493, Acc.monitor: 0.0805, Acc.bulletin board: 0.6183, Acc.shower: 0.0545, Acc.radiator: 0.7313, Acc.glass: 0.1512, Acc.clock: 0.4438, Acc.flag: 0.4961
2023-11-11 14:52:03,426 - mmseg - INFO - Iter [144050/160000]	lr: 2.432e-06, eta: 3:06:16, time: 2.480, data_time: 1.833, memory: 23129, decode.loss_ce: 0.1664, decode.acc_seg: 93.1151, loss: 0.1664
2023-11-11 14:52:37,113 - mmseg - INFO - Iter [144100/160000]	lr: 2.417e-06, eta: 3:05:41, time: 0.673, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1664, decode.acc_seg: 93.0257, loss: 0.1664
2023-11-11 14:53:12,297 - mmseg - INFO - Iter [144150/160000]	lr: 2.402e-06, eta: 3:05:06, time: 0.703, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1643, decode.acc_seg: 93.0544, loss: 0.1643
2023-11-11 14:53:47,506 - mmseg - INFO - Iter [144200/160000]	lr: 2.387e-06, eta: 3:04:31, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1691, decode.acc_seg: 92.6489, loss: 0.1691
2023-11-11 14:54:19,604 - mmseg - INFO - Iter [144250/160000]	lr: 2.372e-06, eta: 3:03:56, time: 0.643, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1656, decode.acc_seg: 93.1136, loss: 0.1656
2023-11-11 14:54:51,178 - mmseg - INFO - Iter [144300/160000]	lr: 2.357e-06, eta: 3:03:20, time: 0.632, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1703, decode.acc_seg: 92.8554, loss: 0.1703
2023-11-11 14:55:25,478 - mmseg - INFO - Iter [144350/160000]	lr: 2.342e-06, eta: 3:02:45, time: 0.685, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1640, decode.acc_seg: 92.8162, loss: 0.1640
2023-11-11 14:56:01,013 - mmseg - INFO - Iter [144400/160000]	lr: 2.328e-06, eta: 3:02:10, time: 0.711, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1613, decode.acc_seg: 93.0286, loss: 0.1613
2023-11-11 14:56:36,406 - mmseg - INFO - Iter [144450/160000]	lr: 2.313e-06, eta: 3:01:35, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1582, decode.acc_seg: 93.1997, loss: 0.1582
2023-11-11 14:57:12,002 - mmseg - INFO - Iter [144500/160000]	lr: 2.298e-06, eta: 3:01:00, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1631, decode.acc_seg: 93.0652, loss: 0.1631
2023-11-11 14:57:46,092 - mmseg - INFO - Iter [144550/160000]	lr: 2.283e-06, eta: 3:00:25, time: 0.682, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1631, decode.acc_seg: 92.8857, loss: 0.1631
2023-11-11 14:58:21,501 - mmseg - INFO - Iter [144600/160000]	lr: 2.269e-06, eta: 2:59:50, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1695, decode.acc_seg: 92.8130, loss: 0.1695
2023-11-11 14:58:53,922 - mmseg - INFO - Iter [144650/160000]	lr: 2.254e-06, eta: 2:59:15, time: 0.649, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1567, decode.acc_seg: 93.1696, loss: 0.1567
2023-11-11 14:59:27,315 - mmseg - INFO - Iter [144700/160000]	lr: 2.240e-06, eta: 2:58:40, time: 0.667, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1676, decode.acc_seg: 92.7930, loss: 0.1676
2023-11-11 15:00:01,709 - mmseg - INFO - Iter [144750/160000]	lr: 2.225e-06, eta: 2:58:05, time: 0.689, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1609, decode.acc_seg: 92.9462, loss: 0.1609
2023-11-11 15:00:33,989 - mmseg - INFO - Iter [144800/160000]	lr: 2.211e-06, eta: 2:57:29, time: 0.644, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1595, decode.acc_seg: 93.2163, loss: 0.1595
2023-11-11 15:01:06,840 - mmseg - INFO - Iter [144850/160000]	lr: 2.196e-06, eta: 2:56:54, time: 0.658, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1627, decode.acc_seg: 92.8212, loss: 0.1627
2023-11-11 15:01:38,242 - mmseg - INFO - Iter [144900/160000]	lr: 2.182e-06, eta: 2:56:19, time: 0.628, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1638, decode.acc_seg: 92.9447, loss: 0.1638
2023-11-11 15:02:09,843 - mmseg - INFO - Iter [144950/160000]	lr: 2.168e-06, eta: 2:55:43, time: 0.632, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1716, decode.acc_seg: 92.6551, loss: 0.1716
2023-11-11 15:02:43,496 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 15:02:43,497 - mmseg - INFO - Iter [145000/160000]	lr: 2.153e-06, eta: 2:55:08, time: 0.672, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1702, decode.acc_seg: 92.7909, loss: 0.1702
2023-11-11 15:03:19,205 - mmseg - INFO - Iter [145050/160000]	lr: 2.139e-06, eta: 2:54:33, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1661, decode.acc_seg: 93.0255, loss: 0.1661
2023-11-11 15:03:55,066 - mmseg - INFO - Iter [145100/160000]	lr: 2.125e-06, eta: 2:53:58, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1623, decode.acc_seg: 93.0672, loss: 0.1623
2023-11-11 15:04:30,279 - mmseg - INFO - Iter [145150/160000]	lr: 2.111e-06, eta: 2:53:23, time: 0.704, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1720, decode.acc_seg: 92.6611, loss: 0.1720
2023-11-11 15:05:05,765 - mmseg - INFO - Iter [145200/160000]	lr: 2.097e-06, eta: 2:52:48, time: 0.710, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1681, decode.acc_seg: 92.7872, loss: 0.1681
2023-11-11 15:05:41,086 - mmseg - INFO - Iter [145250/160000]	lr: 2.083e-06, eta: 2:52:13, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1577, decode.acc_seg: 93.3775, loss: 0.1577
2023-11-11 15:06:15,819 - mmseg - INFO - Iter [145300/160000]	lr: 2.069e-06, eta: 2:51:38, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1627, decode.acc_seg: 92.9964, loss: 0.1627
2023-11-11 15:06:50,734 - mmseg - INFO - Iter [145350/160000]	lr: 2.055e-06, eta: 2:51:03, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1693, decode.acc_seg: 92.8127, loss: 0.1693
2023-11-11 15:07:26,386 - mmseg - INFO - Iter [145400/160000]	lr: 2.041e-06, eta: 2:50:28, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1695, decode.acc_seg: 92.9319, loss: 0.1695
2023-11-11 15:08:02,239 - mmseg - INFO - Iter [145450/160000]	lr: 2.027e-06, eta: 2:49:53, time: 0.717, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1738, decode.acc_seg: 92.7367, loss: 0.1738
2023-11-11 15:08:34,342 - mmseg - INFO - Iter [145500/160000]	lr: 2.013e-06, eta: 2:49:18, time: 0.643, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1639, decode.acc_seg: 93.1541, loss: 0.1639
2023-11-11 15:09:09,694 - mmseg - INFO - Iter [145550/160000]	lr: 1.999e-06, eta: 2:48:43, time: 0.706, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1558, decode.acc_seg: 93.3069, loss: 0.1558
2023-11-11 15:09:42,103 - mmseg - INFO - Iter [145600/160000]	lr: 1.986e-06, eta: 2:48:07, time: 0.648, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1701, decode.acc_seg: 92.6623, loss: 0.1701
2023-11-11 15:10:16,743 - mmseg - INFO - Iter [145650/160000]	lr: 1.972e-06, eta: 2:47:32, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1695, decode.acc_seg: 92.8409, loss: 0.1695
2023-11-11 15:10:51,983 - mmseg - INFO - Iter [145700/160000]	lr: 1.958e-06, eta: 2:46:57, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1559, decode.acc_seg: 93.1632, loss: 0.1559
2023-11-11 15:11:26,998 - mmseg - INFO - Iter [145750/160000]	lr: 1.945e-06, eta: 2:46:22, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1709, decode.acc_seg: 92.7296, loss: 0.1709
2023-11-11 15:11:58,281 - mmseg - INFO - Iter [145800/160000]	lr: 1.931e-06, eta: 2:45:47, time: 0.626, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1638, decode.acc_seg: 93.1642, loss: 0.1638
2023-11-11 15:12:33,731 - mmseg - INFO - Iter [145850/160000]	lr: 1.918e-06, eta: 2:45:12, time: 0.708, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1665, decode.acc_seg: 92.9863, loss: 0.1665
2023-11-11 15:13:09,024 - mmseg - INFO - Iter [145900/160000]	lr: 1.904e-06, eta: 2:44:37, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1631, decode.acc_seg: 93.0964, loss: 0.1631
2023-11-11 15:13:44,340 - mmseg - INFO - Iter [145950/160000]	lr: 1.891e-06, eta: 2:44:02, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1619, decode.acc_seg: 93.1470, loss: 0.1619
2023-11-11 15:14:19,271 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 15:14:19,272 - mmseg - INFO - Iter [146000/160000]	lr: 1.878e-06, eta: 2:43:27, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1652, decode.acc_seg: 92.9714, loss: 0.1652
2023-11-11 15:14:54,031 - mmseg - INFO - Iter [146050/160000]	lr: 1.864e-06, eta: 2:42:52, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1660, decode.acc_seg: 92.8263, loss: 0.1660
2023-11-11 15:15:29,091 - mmseg - INFO - Iter [146100/160000]	lr: 1.851e-06, eta: 2:42:17, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1643, decode.acc_seg: 93.0955, loss: 0.1643
2023-11-11 15:16:03,720 - mmseg - INFO - Iter [146150/160000]	lr: 1.838e-06, eta: 2:41:42, time: 0.692, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1705, decode.acc_seg: 92.7093, loss: 0.1705
2023-11-11 15:16:38,534 - mmseg - INFO - Iter [146200/160000]	lr: 1.825e-06, eta: 2:41:07, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1677, decode.acc_seg: 92.8981, loss: 0.1677
2023-11-11 15:17:13,286 - mmseg - INFO - Iter [146250/160000]	lr: 1.811e-06, eta: 2:40:32, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1621, decode.acc_seg: 92.9740, loss: 0.1621
2023-11-11 15:17:48,593 - mmseg - INFO - Iter [146300/160000]	lr: 1.798e-06, eta: 2:39:57, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1635, decode.acc_seg: 93.0663, loss: 0.1635
2023-11-11 15:18:24,511 - mmseg - INFO - Iter [146350/160000]	lr: 1.785e-06, eta: 2:39:22, time: 0.718, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1705, decode.acc_seg: 92.8700, loss: 0.1705
2023-11-11 15:19:00,299 - mmseg - INFO - Iter [146400/160000]	lr: 1.772e-06, eta: 2:38:47, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1685, decode.acc_seg: 93.0074, loss: 0.1685
2023-11-11 15:19:35,895 - mmseg - INFO - Iter [146450/160000]	lr: 1.759e-06, eta: 2:38:12, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1767, decode.acc_seg: 92.6157, loss: 0.1767
2023-11-11 15:20:11,078 - mmseg - INFO - Iter [146500/160000]	lr: 1.747e-06, eta: 2:37:37, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1630, decode.acc_seg: 93.1266, loss: 0.1630
2023-11-11 15:20:45,885 - mmseg - INFO - Iter [146550/160000]	lr: 1.734e-06, eta: 2:37:02, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1727, decode.acc_seg: 92.6366, loss: 0.1727
2023-11-11 15:21:17,241 - mmseg - INFO - Iter [146600/160000]	lr: 1.721e-06, eta: 2:36:26, time: 0.627, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1588, decode.acc_seg: 93.3934, loss: 0.1588
2023-11-11 15:21:51,373 - mmseg - INFO - Iter [146650/160000]	lr: 1.708e-06, eta: 2:35:51, time: 0.681, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1627, decode.acc_seg: 93.1119, loss: 0.1627
2023-11-11 15:22:25,613 - mmseg - INFO - Iter [146700/160000]	lr: 1.696e-06, eta: 2:35:16, time: 0.685, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1699, decode.acc_seg: 92.6924, loss: 0.1699
2023-11-11 15:23:00,330 - mmseg - INFO - Iter [146750/160000]	lr: 1.683e-06, eta: 2:34:41, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1789, decode.acc_seg: 92.3563, loss: 0.1789
2023-11-11 15:23:35,147 - mmseg - INFO - Iter [146800/160000]	lr: 1.670e-06, eta: 2:34:06, time: 0.696, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1739, decode.acc_seg: 92.6437, loss: 0.1739
2023-11-11 15:24:10,247 - mmseg - INFO - Iter [146850/160000]	lr: 1.658e-06, eta: 2:33:31, time: 0.702, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1620, decode.acc_seg: 93.0588, loss: 0.1620
2023-11-11 15:24:45,333 - mmseg - INFO - Iter [146900/160000]	lr: 1.645e-06, eta: 2:32:56, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1719, decode.acc_seg: 92.6767, loss: 0.1719
2023-11-11 15:25:20,034 - mmseg - INFO - Iter [146950/160000]	lr: 1.633e-06, eta: 2:32:21, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1679, decode.acc_seg: 92.8152, loss: 0.1679
2023-11-11 15:25:55,035 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 15:25:55,035 - mmseg - INFO - Iter [147000/160000]	lr: 1.620e-06, eta: 2:31:46, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1651, decode.acc_seg: 92.9632, loss: 0.1651
2023-11-11 15:26:28,691 - mmseg - INFO - Iter [147050/160000]	lr: 1.608e-06, eta: 2:31:11, time: 0.674, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1588, decode.acc_seg: 93.2016, loss: 0.1588
2023-11-11 15:27:00,606 - mmseg - INFO - Iter [147100/160000]	lr: 1.596e-06, eta: 2:30:36, time: 0.638, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1684, decode.acc_seg: 92.9641, loss: 0.1684
2023-11-11 15:27:32,245 - mmseg - INFO - Iter [147150/160000]	lr: 1.583e-06, eta: 2:30:00, time: 0.633, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1749, decode.acc_seg: 92.5146, loss: 0.1749
2023-11-11 15:28:04,699 - mmseg - INFO - Iter [147200/160000]	lr: 1.571e-06, eta: 2:29:25, time: 0.649, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1692, decode.acc_seg: 92.7704, loss: 0.1692
2023-11-11 15:28:37,973 - mmseg - INFO - Iter [147250/160000]	lr: 1.559e-06, eta: 2:28:50, time: 0.666, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1697, decode.acc_seg: 92.8424, loss: 0.1697
2023-11-11 15:29:09,435 - mmseg - INFO - Iter [147300/160000]	lr: 1.547e-06, eta: 2:28:14, time: 0.628, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1749, decode.acc_seg: 92.6981, loss: 0.1749
2023-11-11 15:29:44,920 - mmseg - INFO - Iter [147350/160000]	lr: 1.535e-06, eta: 2:27:39, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1653, decode.acc_seg: 93.0390, loss: 0.1653
2023-11-11 15:30:20,297 - mmseg - INFO - Iter [147400/160000]	lr: 1.523e-06, eta: 2:27:05, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1632, decode.acc_seg: 93.0956, loss: 0.1632
2023-11-11 15:30:53,179 - mmseg - INFO - Iter [147450/160000]	lr: 1.511e-06, eta: 2:26:29, time: 0.658, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1679, decode.acc_seg: 92.8411, loss: 0.1679
2023-11-11 15:31:24,804 - mmseg - INFO - Iter [147500/160000]	lr: 1.499e-06, eta: 2:25:54, time: 0.632, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1661, decode.acc_seg: 92.8592, loss: 0.1661
2023-11-11 15:32:00,614 - mmseg - INFO - Iter [147550/160000]	lr: 1.487e-06, eta: 2:25:19, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1577, decode.acc_seg: 93.1554, loss: 0.1577
2023-11-11 15:32:36,428 - mmseg - INFO - Iter [147600/160000]	lr: 1.475e-06, eta: 2:24:44, time: 0.716, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1650, decode.acc_seg: 93.0279, loss: 0.1650
2023-11-11 15:33:08,665 - mmseg - INFO - Iter [147650/160000]	lr: 1.463e-06, eta: 2:24:09, time: 0.645, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1692, decode.acc_seg: 92.5861, loss: 0.1692
2023-11-11 15:33:42,390 - mmseg - INFO - Iter [147700/160000]	lr: 1.451e-06, eta: 2:23:34, time: 0.675, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1582, decode.acc_seg: 93.3280, loss: 0.1582
2023-11-11 15:34:16,822 - mmseg - INFO - Iter [147750/160000]	lr: 1.440e-06, eta: 2:22:59, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1686, decode.acc_seg: 92.6740, loss: 0.1686
2023-11-11 15:34:50,678 - mmseg - INFO - Iter [147800/160000]	lr: 1.428e-06, eta: 2:22:24, time: 0.677, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1622, decode.acc_seg: 92.9339, loss: 0.1622
2023-11-11 15:35:23,594 - mmseg - INFO - Iter [147850/160000]	lr: 1.416e-06, eta: 2:21:48, time: 0.658, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1630, decode.acc_seg: 93.1728, loss: 0.1630
2023-11-11 15:35:58,250 - mmseg - INFO - Iter [147900/160000]	lr: 1.405e-06, eta: 2:21:13, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1650, decode.acc_seg: 92.7852, loss: 0.1650
2023-11-11 15:36:33,059 - mmseg - INFO - Iter [147950/160000]	lr: 1.393e-06, eta: 2:20:38, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1680, decode.acc_seg: 92.7538, loss: 0.1680
2023-11-11 15:37:07,648 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 15:37:07,648 - mmseg - INFO - Iter [148000/160000]	lr: 1.382e-06, eta: 2:20:03, time: 0.691, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1654, decode.acc_seg: 93.0221, loss: 0.1654
2023-11-11 15:37:43,442 - mmseg - INFO - Iter [148050/160000]	lr: 1.370e-06, eta: 2:19:28, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1675, decode.acc_seg: 92.8059, loss: 0.1675
2023-11-11 15:38:19,177 - mmseg - INFO - Iter [148100/160000]	lr: 1.359e-06, eta: 2:18:53, time: 0.715, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1713, decode.acc_seg: 92.5414, loss: 0.1713
2023-11-11 15:38:54,370 - mmseg - INFO - Iter [148150/160000]	lr: 1.348e-06, eta: 2:18:18, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1610, decode.acc_seg: 93.0189, loss: 0.1610
2023-11-11 15:39:29,631 - mmseg - INFO - Iter [148200/160000]	lr: 1.336e-06, eta: 2:17:43, time: 0.705, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1674, decode.acc_seg: 92.9776, loss: 0.1674
2023-11-11 15:40:04,815 - mmseg - INFO - Iter [148250/160000]	lr: 1.325e-06, eta: 2:17:08, time: 0.704, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1733, decode.acc_seg: 92.7839, loss: 0.1733
2023-11-11 15:40:39,583 - mmseg - INFO - Iter [148300/160000]	lr: 1.314e-06, eta: 2:16:33, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1692, decode.acc_seg: 92.7577, loss: 0.1692
2023-11-11 15:41:14,414 - mmseg - INFO - Iter [148350/160000]	lr: 1.303e-06, eta: 2:15:58, time: 0.696, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1621, decode.acc_seg: 93.2151, loss: 0.1621
2023-11-11 15:41:47,100 - mmseg - INFO - Iter [148400/160000]	lr: 1.292e-06, eta: 2:15:23, time: 0.655, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1652, decode.acc_seg: 92.8404, loss: 0.1652
2023-11-11 15:42:20,087 - mmseg - INFO - Iter [148450/160000]	lr: 1.280e-06, eta: 2:14:48, time: 0.660, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1658, decode.acc_seg: 93.0214, loss: 0.1658
2023-11-11 15:42:53,493 - mmseg - INFO - Iter [148500/160000]	lr: 1.269e-06, eta: 2:14:13, time: 0.668, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1610, decode.acc_seg: 93.1330, loss: 0.1610
2023-11-11 15:43:27,823 - mmseg - INFO - Iter [148550/160000]	lr: 1.259e-06, eta: 2:13:38, time: 0.686, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1638, decode.acc_seg: 93.0732, loss: 0.1638
2023-11-11 15:44:03,716 - mmseg - INFO - Iter [148600/160000]	lr: 1.248e-06, eta: 2:13:03, time: 0.718, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1738, decode.acc_seg: 92.6451, loss: 0.1738
2023-11-11 15:44:38,350 - mmseg - INFO - Iter [148650/160000]	lr: 1.237e-06, eta: 2:12:28, time: 0.693, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1694, decode.acc_seg: 92.8192, loss: 0.1694
2023-11-11 15:45:13,137 - mmseg - INFO - Iter [148700/160000]	lr: 1.226e-06, eta: 2:11:53, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1697, decode.acc_seg: 92.8133, loss: 0.1697
2023-11-11 15:45:45,953 - mmseg - INFO - Iter [148750/160000]	lr: 1.215e-06, eta: 2:11:17, time: 0.657, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1626, decode.acc_seg: 92.9564, loss: 0.1626
2023-11-11 15:46:19,126 - mmseg - INFO - Iter [148800/160000]	lr: 1.204e-06, eta: 2:10:42, time: 0.662, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1733, decode.acc_seg: 92.8160, loss: 0.1733
2023-11-11 15:46:54,944 - mmseg - INFO - Iter [148850/160000]	lr: 1.194e-06, eta: 2:10:07, time: 0.716, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1658, decode.acc_seg: 92.8624, loss: 0.1658
2023-11-11 15:47:29,563 - mmseg - INFO - Iter [148900/160000]	lr: 1.183e-06, eta: 2:09:32, time: 0.694, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1635, decode.acc_seg: 93.0419, loss: 0.1635
2023-11-11 15:48:01,474 - mmseg - INFO - Iter [148950/160000]	lr: 1.172e-06, eta: 2:08:57, time: 0.638, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1699, decode.acc_seg: 92.7795, loss: 0.1699
2023-11-11 15:48:33,990 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 15:48:33,990 - mmseg - INFO - Iter [149000/160000]	lr: 1.162e-06, eta: 2:08:22, time: 0.650, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1677, decode.acc_seg: 92.9558, loss: 0.1677
2023-11-11 15:49:05,746 - mmseg - INFO - Iter [149050/160000]	lr: 1.151e-06, eta: 2:07:47, time: 0.634, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1628, decode.acc_seg: 93.1065, loss: 0.1628
2023-11-11 15:49:40,990 - mmseg - INFO - Iter [149100/160000]	lr: 1.141e-06, eta: 2:07:12, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1627, decode.acc_seg: 93.0250, loss: 0.1627
2023-11-11 15:50:14,052 - mmseg - INFO - Iter [149150/160000]	lr: 1.131e-06, eta: 2:06:36, time: 0.662, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1648, decode.acc_seg: 92.8896, loss: 0.1648
2023-11-11 15:50:47,087 - mmseg - INFO - Iter [149200/160000]	lr: 1.120e-06, eta: 2:06:01, time: 0.660, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1649, decode.acc_seg: 92.9193, loss: 0.1649
2023-11-11 15:51:21,284 - mmseg - INFO - Iter [149250/160000]	lr: 1.110e-06, eta: 2:05:26, time: 0.685, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1718, decode.acc_seg: 92.6190, loss: 0.1718
2023-11-11 15:51:56,308 - mmseg - INFO - Iter [149300/160000]	lr: 1.100e-06, eta: 2:04:51, time: 0.699, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1642, decode.acc_seg: 93.0238, loss: 0.1642
2023-11-11 15:52:31,492 - mmseg - INFO - Iter [149350/160000]	lr: 1.089e-06, eta: 2:04:16, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1697, decode.acc_seg: 92.7229, loss: 0.1697
2023-11-11 15:53:07,240 - mmseg - INFO - Iter [149400/160000]	lr: 1.079e-06, eta: 2:03:41, time: 0.715, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1781, decode.acc_seg: 92.5474, loss: 0.1781
2023-11-11 15:53:42,351 - mmseg - INFO - Iter [149450/160000]	lr: 1.069e-06, eta: 2:03:06, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1665, decode.acc_seg: 92.8772, loss: 0.1665
2023-11-11 15:54:17,343 - mmseg - INFO - Iter [149500/160000]	lr: 1.059e-06, eta: 2:02:31, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1776, decode.acc_seg: 92.5850, loss: 0.1776
2023-11-11 15:54:50,242 - mmseg - INFO - Iter [149550/160000]	lr: 1.049e-06, eta: 2:01:56, time: 0.658, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1725, decode.acc_seg: 92.8325, loss: 0.1725
2023-11-11 15:55:24,212 - mmseg - INFO - Iter [149600/160000]	lr: 1.039e-06, eta: 2:01:21, time: 0.678, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1625, decode.acc_seg: 93.0613, loss: 0.1625
2023-11-11 15:55:59,700 - mmseg - INFO - Iter [149650/160000]	lr: 1.029e-06, eta: 2:00:46, time: 0.709, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1735, decode.acc_seg: 92.7994, loss: 0.1735
2023-11-11 15:56:34,743 - mmseg - INFO - Iter [149700/160000]	lr: 1.019e-06, eta: 2:00:11, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1635, decode.acc_seg: 93.1848, loss: 0.1635
2023-11-11 15:57:10,005 - mmseg - INFO - Iter [149750/160000]	lr: 1.009e-06, eta: 1:59:36, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1585, decode.acc_seg: 93.0530, loss: 0.1585
2023-11-11 15:57:44,504 - mmseg - INFO - Iter [149800/160000]	lr: 9.996e-07, eta: 1:59:01, time: 0.691, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1563, decode.acc_seg: 93.5148, loss: 0.1563
2023-11-11 15:58:16,952 - mmseg - INFO - Iter [149850/160000]	lr: 9.899e-07, eta: 1:58:26, time: 0.648, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1678, decode.acc_seg: 92.9446, loss: 0.1678
2023-11-11 15:58:52,019 - mmseg - INFO - Iter [149900/160000]	lr: 9.802e-07, eta: 1:57:51, time: 0.702, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1644, decode.acc_seg: 93.1048, loss: 0.1644
2023-11-11 15:59:24,899 - mmseg - INFO - Iter [149950/160000]	lr: 9.705e-07, eta: 1:57:16, time: 0.658, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1689, decode.acc_seg: 92.7791, loss: 0.1689
2023-11-11 15:59:57,909 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 15:59:57,909 - mmseg - INFO - Iter [150000/160000]	lr: 9.609e-07, eta: 1:56:41, time: 0.659, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1665, decode.acc_seg: 92.8760, loss: 0.1665
2023-11-11 16:00:32,885 - mmseg - INFO - Iter [150050/160000]	lr: 9.514e-07, eta: 1:56:06, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1576, decode.acc_seg: 93.2764, loss: 0.1576
2023-11-11 16:01:05,949 - mmseg - INFO - Iter [150100/160000]	lr: 9.419e-07, eta: 1:55:30, time: 0.661, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1661, decode.acc_seg: 93.0446, loss: 0.1661
2023-11-11 16:01:40,906 - mmseg - INFO - Iter [150150/160000]	lr: 9.324e-07, eta: 1:54:55, time: 0.699, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1699, decode.acc_seg: 92.7616, loss: 0.1699
2023-11-11 16:02:16,286 - mmseg - INFO - Iter [150200/160000]	lr: 9.230e-07, eta: 1:54:20, time: 0.708, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1663, decode.acc_seg: 92.9643, loss: 0.1663
2023-11-11 16:02:47,932 - mmseg - INFO - Iter [150250/160000]	lr: 9.136e-07, eta: 1:53:45, time: 0.633, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1627, decode.acc_seg: 92.8944, loss: 0.1627
2023-11-11 16:03:19,749 - mmseg - INFO - Iter [150300/160000]	lr: 9.043e-07, eta: 1:53:10, time: 0.636, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1559, decode.acc_seg: 93.1856, loss: 0.1559
2023-11-11 16:03:51,391 - mmseg - INFO - Iter [150350/160000]	lr: 8.950e-07, eta: 1:52:35, time: 0.632, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1682, decode.acc_seg: 92.8737, loss: 0.1682
2023-11-11 16:04:25,610 - mmseg - INFO - Iter [150400/160000]	lr: 8.858e-07, eta: 1:52:00, time: 0.684, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1622, decode.acc_seg: 93.1000, loss: 0.1622
2023-11-11 16:05:01,026 - mmseg - INFO - Iter [150450/160000]	lr: 8.766e-07, eta: 1:51:25, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1698, decode.acc_seg: 92.9550, loss: 0.1698
2023-11-11 16:05:36,344 - mmseg - INFO - Iter [150500/160000]	lr: 8.675e-07, eta: 1:50:50, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1701, decode.acc_seg: 92.7951, loss: 0.1701
2023-11-11 16:06:11,870 - mmseg - INFO - Iter [150550/160000]	lr: 8.584e-07, eta: 1:50:15, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1555, decode.acc_seg: 93.1670, loss: 0.1555
2023-11-11 16:06:44,979 - mmseg - INFO - Iter [150600/160000]	lr: 8.494e-07, eta: 1:49:40, time: 0.663, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1715, decode.acc_seg: 92.8602, loss: 0.1715
2023-11-11 16:07:18,192 - mmseg - INFO - Iter [150650/160000]	lr: 8.404e-07, eta: 1:49:05, time: 0.664, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1633, decode.acc_seg: 93.1294, loss: 0.1633
2023-11-11 16:07:51,874 - mmseg - INFO - Iter [150700/160000]	lr: 8.315e-07, eta: 1:48:30, time: 0.673, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1715, decode.acc_seg: 92.8675, loss: 0.1715
2023-11-11 16:08:26,125 - mmseg - INFO - Iter [150750/160000]	lr: 8.226e-07, eta: 1:47:54, time: 0.685, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1629, decode.acc_seg: 93.1388, loss: 0.1629
2023-11-11 16:09:01,828 - mmseg - INFO - Iter [150800/160000]	lr: 8.137e-07, eta: 1:47:20, time: 0.714, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1748, decode.acc_seg: 92.5507, loss: 0.1748
2023-11-11 16:09:34,966 - mmseg - INFO - Iter [150850/160000]	lr: 8.049e-07, eta: 1:46:44, time: 0.663, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1645, decode.acc_seg: 93.0249, loss: 0.1645
2023-11-11 16:10:10,606 - mmseg - INFO - Iter [150900/160000]	lr: 7.962e-07, eta: 1:46:09, time: 0.713, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1755, decode.acc_seg: 92.5438, loss: 0.1755
2023-11-11 16:10:46,182 - mmseg - INFO - Iter [150950/160000]	lr: 7.875e-07, eta: 1:45:34, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1662, decode.acc_seg: 92.9819, loss: 0.1662
2023-11-11 16:11:21,461 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 16:11:21,462 - mmseg - INFO - Iter [151000/160000]	lr: 7.788e-07, eta: 1:44:59, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1594, decode.acc_seg: 93.2173, loss: 0.1594
2023-11-11 16:11:54,070 - mmseg - INFO - Iter [151050/160000]	lr: 7.702e-07, eta: 1:44:24, time: 0.653, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1565, decode.acc_seg: 93.2399, loss: 0.1565
2023-11-11 16:12:28,175 - mmseg - INFO - Iter [151100/160000]	lr: 7.617e-07, eta: 1:43:49, time: 0.682, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1663, decode.acc_seg: 92.8128, loss: 0.1663
2023-11-11 16:13:00,101 - mmseg - INFO - Iter [151150/160000]	lr: 7.532e-07, eta: 1:43:14, time: 0.638, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1626, decode.acc_seg: 93.1699, loss: 0.1626
2023-11-11 16:13:31,802 - mmseg - INFO - Iter [151200/160000]	lr: 7.447e-07, eta: 1:42:39, time: 0.634, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1641, decode.acc_seg: 92.9273, loss: 0.1641
2023-11-11 16:14:05,323 - mmseg - INFO - Iter [151250/160000]	lr: 7.363e-07, eta: 1:42:04, time: 0.670, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1721, decode.acc_seg: 92.9121, loss: 0.1721
2023-11-11 16:14:36,655 - mmseg - INFO - Iter [151300/160000]	lr: 7.279e-07, eta: 1:41:29, time: 0.627, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1675, decode.acc_seg: 93.1416, loss: 0.1675
2023-11-11 16:15:11,310 - mmseg - INFO - Iter [151350/160000]	lr: 7.196e-07, eta: 1:40:54, time: 0.692, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1712, decode.acc_seg: 92.8336, loss: 0.1712
2023-11-11 16:15:45,730 - mmseg - INFO - Iter [151400/160000]	lr: 7.113e-07, eta: 1:40:19, time: 0.688, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1656, decode.acc_seg: 93.0256, loss: 0.1656
2023-11-11 16:16:20,540 - mmseg - INFO - Iter [151450/160000]	lr: 7.031e-07, eta: 1:39:44, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1719, decode.acc_seg: 92.5799, loss: 0.1719
2023-11-11 16:16:55,483 - mmseg - INFO - Iter [151500/160000]	lr: 6.949e-07, eta: 1:39:09, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1709, decode.acc_seg: 92.7732, loss: 0.1709
2023-11-11 16:17:30,711 - mmseg - INFO - Iter [151550/160000]	lr: 6.868e-07, eta: 1:38:34, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1511, decode.acc_seg: 93.4100, loss: 0.1511
2023-11-11 16:18:05,481 - mmseg - INFO - Iter [151600/160000]	lr: 6.787e-07, eta: 1:37:59, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1595, decode.acc_seg: 93.0231, loss: 0.1595
2023-11-11 16:18:40,829 - mmseg - INFO - Iter [151650/160000]	lr: 6.707e-07, eta: 1:37:24, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1626, decode.acc_seg: 93.0581, loss: 0.1626
2023-11-11 16:19:15,576 - mmseg - INFO - Iter [151700/160000]	lr: 6.627e-07, eta: 1:36:49, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1636, decode.acc_seg: 93.0080, loss: 0.1636
2023-11-11 16:19:50,333 - mmseg - INFO - Iter [151750/160000]	lr: 6.547e-07, eta: 1:36:14, time: 0.694, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1667, decode.acc_seg: 92.9201, loss: 0.1667
2023-11-11 16:20:25,402 - mmseg - INFO - Iter [151800/160000]	lr: 6.468e-07, eta: 1:35:39, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1720, decode.acc_seg: 92.8970, loss: 0.1720
2023-11-11 16:21:00,818 - mmseg - INFO - Iter [151850/160000]	lr: 6.390e-07, eta: 1:35:04, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1668, decode.acc_seg: 92.9716, loss: 0.1668
2023-11-11 16:21:35,591 - mmseg - INFO - Iter [151900/160000]	lr: 6.312e-07, eta: 1:34:29, time: 0.695, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1614, decode.acc_seg: 92.9622, loss: 0.1614
2023-11-11 16:22:09,000 - mmseg - INFO - Iter [151950/160000]	lr: 6.234e-07, eta: 1:33:54, time: 0.669, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1666, decode.acc_seg: 92.9565, loss: 0.1666
2023-11-11 16:22:42,690 - mmseg - INFO - Saving checkpoint at 152000 iterations
2023-11-11 16:22:47,263 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 16:22:47,264 - mmseg - INFO - Iter [152000/160000]	lr: 6.157e-07, eta: 1:33:19, time: 0.766, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1634, decode.acc_seg: 93.0209, loss: 0.1634
2023-11-11 16:24:17,258 - mmseg - INFO - per class results:
2023-11-11 16:24:17,272 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 77.81 |  88.4 |
|       building      | 81.87 | 91.53 |
|         sky         | 94.56 | 97.55 |
|        floor        | 82.96 | 90.54 |
|         tree        | 75.02 | 88.91 |
|       ceiling       | 83.55 | 92.28 |
|         road        | 82.86 | 89.44 |
|         bed         | 89.76 | 96.23 |
|      windowpane     | 62.47 | 77.85 |
|        grass        | 65.43 | 82.58 |
|       cabinet       | 61.08 | 74.39 |
|       sidewalk      | 65.91 | 81.73 |
|        person       | 81.58 | 93.85 |
|        earth        | 38.45 | 53.85 |
|         door        | 50.26 | 63.44 |
|        table        | 62.99 | 77.93 |
|       mountain      | 53.69 | 66.68 |
|        plant        | 53.42 | 63.51 |
|       curtain       |  74.3 | 85.99 |
|        chair        | 58.07 | 72.09 |
|         car         | 83.78 | 92.53 |
|        water        | 59.91 | 77.22 |
|       painting      | 73.31 | 88.23 |
|         sofa        | 66.79 | 82.83 |
|        shelf        | 43.25 | 61.14 |
|        house        | 43.54 | 59.22 |
|         sea         | 59.13 | 75.49 |
|        mirror       | 69.46 | 76.42 |
|         rug         | 64.23 | 76.11 |
|        field        | 28.51 | 44.99 |
|       armchair      | 44.15 | 61.65 |
|         seat        | 63.05 | 80.03 |
|        fence        | 42.58 | 56.62 |
|         desk        | 51.07 |  72.2 |
|         rock        | 35.66 | 55.86 |
|       wardrobe      | 50.22 | 70.23 |
|         lamp        | 65.82 | 77.37 |
|       bathtub       | 80.98 | 86.37 |
|       railing       | 30.59 | 42.76 |
|       cushion       | 59.57 | 72.01 |
|         base        | 30.84 | 40.53 |
|         box         | 30.51 | 38.26 |
|        column       | 45.76 | 55.34 |
|      signboard      | 39.62 | 53.34 |
|   chest of drawers  |  46.6 | 58.17 |
|       counter       | 37.72 | 44.65 |
|         sand        | 52.07 | 73.77 |
|         sink        | 71.94 | 81.28 |
|      skyscraper     | 47.46 | 59.37 |
|      fireplace      | 75.89 | 89.75 |
|     refrigerator    | 78.55 | 85.77 |
|      grandstand     | 41.45 | 67.72 |
|         path        | 23.61 | 38.02 |
|        stairs       | 30.07 | 40.36 |
|        runway       | 64.74 | 83.25 |
|         case        | 48.52 | 57.52 |
|      pool table     | 93.18 | 96.65 |
|        pillow       | 61.42 | 72.65 |
|     screen door     | 66.94 | 81.32 |
|       stairway      | 29.02 | 37.88 |
|        river        | 11.41 | 22.76 |
|        bridge       | 71.75 |  82.4 |
|       bookcase      | 36.95 | 55.55 |
|        blind        | 44.12 | 49.63 |
|     coffee table    | 53.67 | 81.43 |
|        toilet       | 86.06 | 90.87 |
|        flower       |  37.4 | 52.92 |
|         book        | 46.32 | 68.11 |
|         hill        | 10.94 | 19.54 |
|        bench        | 45.99 | 52.39 |
|      countertop     | 57.17 | 84.17 |
|        stove        | 73.62 | 82.26 |
|         palm        | 48.55 | 71.69 |
|    kitchen island   | 40.26 | 70.68 |
|       computer      | 65.93 | 76.36 |
|     swivel chair    | 50.09 | 67.16 |
|         boat        | 46.45 | 54.66 |
|         bar         | 50.66 |  66.2 |
|    arcade machine   | 76.32 | 82.43 |
|        hovel        | 57.99 | 65.87 |
|         bus         | 86.22 | 96.13 |
|        towel        |  66.2 | 78.02 |
|        light        | 55.65 | 65.69 |
|        truck        | 32.63 | 41.01 |
|        tower        |  5.99 |  8.8  |
|      chandelier     | 67.42 |  82.2 |
|        awning       |  27.9 | 36.94 |
|     streetlight     | 29.32 | 38.62 |
|        booth        | 62.21 | 65.45 |
| television receiver | 66.58 | 81.33 |
|       airplane      |  62.1 | 74.39 |
|      dirt track     |  6.74 | 11.74 |
|       apparel       |  49.9 | 72.94 |
|         pole        |  25.4 | 33.98 |
|         land        |  0.0  |  0.0  |
|      bannister      | 13.54 | 17.31 |
|      escalator      | 37.52 | 48.69 |
|       ottoman       | 45.72 | 61.88 |
|        bottle       |  35.9 |  62.3 |
|        buffet       | 48.41 | 55.43 |
|        poster       | 33.17 | 43.07 |
|        stage        | 16.31 | 21.84 |
|         van         | 40.75 | 56.64 |
|         ship        | 57.23 |  83.0 |
|       fountain      | 40.78 |  41.2 |
|    conveyer belt    |  80.0 |  96.3 |
|        canopy       | 19.69 | 32.35 |
|        washer       | 75.34 | 76.37 |
|      plaything      | 20.88 | 32.73 |
|    swimming pool    | 79.59 | 88.82 |
|        stool        | 36.72 | 51.95 |
|        barrel       |  63.1 | 77.38 |
|        basket       |  37.3 | 49.42 |
|      waterfall      | 69.19 |  92.1 |
|         tent        | 95.78 | 98.42 |
|         bag         | 19.13 | 24.56 |
|       minibike      | 67.81 | 81.26 |
|        cradle       |  80.7 | 97.53 |
|         oven        | 45.15 | 66.03 |
|         ball        | 46.32 | 63.71 |
|         food        | 49.95 | 55.57 |
|         step        | 13.88 | 17.78 |
|         tank        | 48.05 | 54.66 |
|      trade name     | 25.61 | 29.81 |
|      microwave      | 77.51 | 87.98 |
|         pot         | 42.41 | 47.91 |
|        animal       | 53.84 |  56.2 |
|       bicycle       | 58.14 | 80.24 |
|         lake        | 53.42 | 57.57 |
|      dishwasher     | 59.86 | 72.43 |
|        screen       | 55.29 | 78.88 |
|       blanket       | 14.48 | 17.38 |
|      sculpture      | 63.91 | 82.91 |
|         hood        | 61.81 | 74.88 |
|        sconce       | 48.28 | 57.61 |
|         vase        | 38.96 | 51.92 |
|    traffic light    | 28.86 | 47.57 |
|         tray        |  9.27 | 18.04 |
|        ashcan       | 40.54 | 56.69 |
|         fan         | 60.67 | 76.08 |
|         pier        | 11.47 | 12.12 |
|      crt screen     |  2.76 | 10.16 |
|        plate        | 46.23 | 60.56 |
|       monitor       | 11.02 | 11.98 |
|    bulletin board   | 54.27 | 62.49 |
|        shower       |  3.04 |  7.17 |
|       radiator      | 66.52 | 73.03 |
|        glass        | 14.19 |  15.1 |
|        clock        | 36.28 | 44.31 |
|         flag        | 45.77 | 49.72 |
+---------------------+-------+-------+
2023-11-11 16:24:17,272 - mmseg - INFO - Summary:
2023-11-11 16:24:17,273 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 83.32 | 50.57 | 62.45 |
+-------+-------+-------+
2023-11-11 16:24:17,408 - mmseg - INFO - The previous best checkpoint /data/Next-ViT/segmentation/work_dirs/fpn_512_debi_base_160k/best_mIoU_iter_144000.pth was removed
2023-11-11 16:24:20,300 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_152000.pth.
2023-11-11 16:24:20,300 - mmseg - INFO - Best mIoU is 0.5057 at 152000 iter.
2023-11-11 16:24:20,341 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 16:24:20,342 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8332, mIoU: 0.5057, mAcc: 0.6245, IoU.wall: 0.7781, IoU.building: 0.8187, IoU.sky: 0.9456, IoU.floor: 0.8296, IoU.tree: 0.7502, IoU.ceiling: 0.8355, IoU.road: 0.8286, IoU.bed : 0.8976, IoU.windowpane: 0.6247, IoU.grass: 0.6543, IoU.cabinet: 0.6108, IoU.sidewalk: 0.6591, IoU.person: 0.8158, IoU.earth: 0.3845, IoU.door: 0.5026, IoU.table: 0.6299, IoU.mountain: 0.5369, IoU.plant: 0.5342, IoU.curtain: 0.7430, IoU.chair: 0.5807, IoU.car: 0.8378, IoU.water: 0.5991, IoU.painting: 0.7331, IoU.sofa: 0.6679, IoU.shelf: 0.4325, IoU.house: 0.4354, IoU.sea: 0.5913, IoU.mirror: 0.6946, IoU.rug: 0.6423, IoU.field: 0.2851, IoU.armchair: 0.4415, IoU.seat: 0.6305, IoU.fence: 0.4258, IoU.desk: 0.5107, IoU.rock: 0.3566, IoU.wardrobe: 0.5022, IoU.lamp: 0.6582, IoU.bathtub: 0.8098, IoU.railing: 0.3059, IoU.cushion: 0.5957, IoU.base: 0.3084, IoU.box: 0.3051, IoU.column: 0.4576, IoU.signboard: 0.3962, IoU.chest of drawers: 0.4660, IoU.counter: 0.3772, IoU.sand: 0.5207, IoU.sink: 0.7194, IoU.skyscraper: 0.4746, IoU.fireplace: 0.7589, IoU.refrigerator: 0.7855, IoU.grandstand: 0.4145, IoU.path: 0.2361, IoU.stairs: 0.3007, IoU.runway: 0.6474, IoU.case: 0.4852, IoU.pool table: 0.9318, IoU.pillow: 0.6142, IoU.screen door: 0.6694, IoU.stairway: 0.2902, IoU.river: 0.1141, IoU.bridge: 0.7175, IoU.bookcase: 0.3695, IoU.blind: 0.4412, IoU.coffee table: 0.5367, IoU.toilet: 0.8606, IoU.flower: 0.3740, IoU.book: 0.4632, IoU.hill: 0.1094, IoU.bench: 0.4599, IoU.countertop: 0.5717, IoU.stove: 0.7362, IoU.palm: 0.4855, IoU.kitchen island: 0.4026, IoU.computer: 0.6593, IoU.swivel chair: 0.5009, IoU.boat: 0.4645, IoU.bar: 0.5066, IoU.arcade machine: 0.7632, IoU.hovel: 0.5799, IoU.bus: 0.8622, IoU.towel: 0.6620, IoU.light: 0.5565, IoU.truck: 0.3263, IoU.tower: 0.0599, IoU.chandelier: 0.6742, IoU.awning: 0.2790, IoU.streetlight: 0.2932, IoU.booth: 0.6221, IoU.television receiver: 0.6658, IoU.airplane: 0.6210, IoU.dirt track: 0.0674, IoU.apparel: 0.4990, IoU.pole: 0.2540, IoU.land: 0.0000, IoU.bannister: 0.1354, IoU.escalator: 0.3752, IoU.ottoman: 0.4572, IoU.bottle: 0.3590, IoU.buffet: 0.4841, IoU.poster: 0.3317, IoU.stage: 0.1631, IoU.van: 0.4075, IoU.ship: 0.5723, IoU.fountain: 0.4078, IoU.conveyer belt: 0.8000, IoU.canopy: 0.1969, IoU.washer: 0.7534, IoU.plaything: 0.2088, IoU.swimming pool: 0.7959, IoU.stool: 0.3672, IoU.barrel: 0.6310, IoU.basket: 0.3730, IoU.waterfall: 0.6919, IoU.tent: 0.9578, IoU.bag: 0.1913, IoU.minibike: 0.6781, IoU.cradle: 0.8070, IoU.oven: 0.4515, IoU.ball: 0.4632, IoU.food: 0.4995, IoU.step: 0.1388, IoU.tank: 0.4805, IoU.trade name: 0.2561, IoU.microwave: 0.7751, IoU.pot: 0.4241, IoU.animal: 0.5384, IoU.bicycle: 0.5814, IoU.lake: 0.5342, IoU.dishwasher: 0.5986, IoU.screen: 0.5529, IoU.blanket: 0.1448, IoU.sculpture: 0.6391, IoU.hood: 0.6181, IoU.sconce: 0.4828, IoU.vase: 0.3896, IoU.traffic light: 0.2886, IoU.tray: 0.0927, IoU.ashcan: 0.4054, IoU.fan: 0.6067, IoU.pier: 0.1147, IoU.crt screen: 0.0276, IoU.plate: 0.4623, IoU.monitor: 0.1102, IoU.bulletin board: 0.5427, IoU.shower: 0.0304, IoU.radiator: 0.6652, IoU.glass: 0.1419, IoU.clock: 0.3628, IoU.flag: 0.4577, Acc.wall: 0.8840, Acc.building: 0.9153, Acc.sky: 0.9755, Acc.floor: 0.9054, Acc.tree: 0.8891, Acc.ceiling: 0.9228, Acc.road: 0.8944, Acc.bed : 0.9623, Acc.windowpane: 0.7785, Acc.grass: 0.8258, Acc.cabinet: 0.7439, Acc.sidewalk: 0.8173, Acc.person: 0.9385, Acc.earth: 0.5385, Acc.door: 0.6344, Acc.table: 0.7793, Acc.mountain: 0.6668, Acc.plant: 0.6351, Acc.curtain: 0.8599, Acc.chair: 0.7209, Acc.car: 0.9253, Acc.water: 0.7722, Acc.painting: 0.8823, Acc.sofa: 0.8283, Acc.shelf: 0.6114, Acc.house: 0.5922, Acc.sea: 0.7549, Acc.mirror: 0.7642, Acc.rug: 0.7611, Acc.field: 0.4499, Acc.armchair: 0.6165, Acc.seat: 0.8003, Acc.fence: 0.5662, Acc.desk: 0.7220, Acc.rock: 0.5586, Acc.wardrobe: 0.7023, Acc.lamp: 0.7737, Acc.bathtub: 0.8637, Acc.railing: 0.4276, Acc.cushion: 0.7201, Acc.base: 0.4053, Acc.box: 0.3826, Acc.column: 0.5534, Acc.signboard: 0.5334, Acc.chest of drawers: 0.5817, Acc.counter: 0.4465, Acc.sand: 0.7377, Acc.sink: 0.8128, Acc.skyscraper: 0.5937, Acc.fireplace: 0.8975, Acc.refrigerator: 0.8577, Acc.grandstand: 0.6772, Acc.path: 0.3802, Acc.stairs: 0.4036, Acc.runway: 0.8325, Acc.case: 0.5752, Acc.pool table: 0.9665, Acc.pillow: 0.7265, Acc.screen door: 0.8132, Acc.stairway: 0.3788, Acc.river: 0.2276, Acc.bridge: 0.8240, Acc.bookcase: 0.5555, Acc.blind: 0.4963, Acc.coffee table: 0.8143, Acc.toilet: 0.9087, Acc.flower: 0.5292, Acc.book: 0.6811, Acc.hill: 0.1954, Acc.bench: 0.5239, Acc.countertop: 0.8417, Acc.stove: 0.8226, Acc.palm: 0.7169, Acc.kitchen island: 0.7068, Acc.computer: 0.7636, Acc.swivel chair: 0.6716, Acc.boat: 0.5466, Acc.bar: 0.6620, Acc.arcade machine: 0.8243, Acc.hovel: 0.6587, Acc.bus: 0.9613, Acc.towel: 0.7802, Acc.light: 0.6569, Acc.truck: 0.4101, Acc.tower: 0.0880, Acc.chandelier: 0.8220, Acc.awning: 0.3694, Acc.streetlight: 0.3862, Acc.booth: 0.6545, Acc.television receiver: 0.8133, Acc.airplane: 0.7439, Acc.dirt track: 0.1174, Acc.apparel: 0.7294, Acc.pole: 0.3398, Acc.land: 0.0000, Acc.bannister: 0.1731, Acc.escalator: 0.4869, Acc.ottoman: 0.6188, Acc.bottle: 0.6230, Acc.buffet: 0.5543, Acc.poster: 0.4307, Acc.stage: 0.2184, Acc.van: 0.5664, Acc.ship: 0.8300, Acc.fountain: 0.4120, Acc.conveyer belt: 0.9630, Acc.canopy: 0.3235, Acc.washer: 0.7637, Acc.plaything: 0.3273, Acc.swimming pool: 0.8882, Acc.stool: 0.5195, Acc.barrel: 0.7738, Acc.basket: 0.4942, Acc.waterfall: 0.9210, Acc.tent: 0.9842, Acc.bag: 0.2456, Acc.minibike: 0.8126, Acc.cradle: 0.9753, Acc.oven: 0.6603, Acc.ball: 0.6371, Acc.food: 0.5557, Acc.step: 0.1778, Acc.tank: 0.5466, Acc.trade name: 0.2981, Acc.microwave: 0.8798, Acc.pot: 0.4791, Acc.animal: 0.5620, Acc.bicycle: 0.8024, Acc.lake: 0.5757, Acc.dishwasher: 0.7243, Acc.screen: 0.7888, Acc.blanket: 0.1738, Acc.sculpture: 0.8291, Acc.hood: 0.7488, Acc.sconce: 0.5761, Acc.vase: 0.5192, Acc.traffic light: 0.4757, Acc.tray: 0.1804, Acc.ashcan: 0.5669, Acc.fan: 0.7608, Acc.pier: 0.1212, Acc.crt screen: 0.1016, Acc.plate: 0.6056, Acc.monitor: 0.1198, Acc.bulletin board: 0.6249, Acc.shower: 0.0717, Acc.radiator: 0.7303, Acc.glass: 0.1510, Acc.clock: 0.4431, Acc.flag: 0.4972
2023-11-11 16:24:52,545 - mmseg - INFO - Iter [152050/160000]	lr: 6.081e-07, eta: 1:32:48, time: 2.506, data_time: 1.871, memory: 23129, decode.loss_ce: 0.1691, decode.acc_seg: 92.8164, loss: 0.1691
2023-11-11 16:25:24,333 - mmseg - INFO - Iter [152100/160000]	lr: 6.005e-07, eta: 1:32:13, time: 0.635, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1561, decode.acc_seg: 93.3315, loss: 0.1561
2023-11-11 16:25:59,475 - mmseg - INFO - Iter [152150/160000]	lr: 5.929e-07, eta: 1:31:38, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1623, decode.acc_seg: 93.1122, loss: 0.1623
2023-11-11 16:26:34,432 - mmseg - INFO - Iter [152200/160000]	lr: 5.854e-07, eta: 1:31:03, time: 0.700, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1609, decode.acc_seg: 93.2548, loss: 0.1609
2023-11-11 16:27:06,074 - mmseg - INFO - Iter [152250/160000]	lr: 5.779e-07, eta: 1:30:28, time: 0.633, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1624, decode.acc_seg: 92.9884, loss: 0.1624
2023-11-11 16:27:38,585 - mmseg - INFO - Iter [152300/160000]	lr: 5.705e-07, eta: 1:29:53, time: 0.649, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1662, decode.acc_seg: 92.9438, loss: 0.1662
2023-11-11 16:28:13,703 - mmseg - INFO - Iter [152350/160000]	lr: 5.631e-07, eta: 1:29:18, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1648, decode.acc_seg: 92.9891, loss: 0.1648
2023-11-11 16:28:48,944 - mmseg - INFO - Iter [152400/160000]	lr: 5.558e-07, eta: 1:28:43, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1720, decode.acc_seg: 92.8058, loss: 0.1720
2023-11-11 16:29:23,936 - mmseg - INFO - Iter [152450/160000]	lr: 5.485e-07, eta: 1:28:08, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1617, decode.acc_seg: 92.9690, loss: 0.1617
2023-11-11 16:29:58,846 - mmseg - INFO - Iter [152500/160000]	lr: 5.413e-07, eta: 1:27:33, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1654, decode.acc_seg: 92.9388, loss: 0.1654
2023-11-11 16:30:33,820 - mmseg - INFO - Iter [152550/160000]	lr: 5.341e-07, eta: 1:26:58, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1616, decode.acc_seg: 93.0570, loss: 0.1616
2023-11-11 16:31:08,732 - mmseg - INFO - Iter [152600/160000]	lr: 5.270e-07, eta: 1:26:23, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1665, decode.acc_seg: 92.9493, loss: 0.1665
2023-11-11 16:31:44,802 - mmseg - INFO - Iter [152650/160000]	lr: 5.199e-07, eta: 1:25:48, time: 0.721, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1611, decode.acc_seg: 93.3145, loss: 0.1611
2023-11-11 16:32:20,306 - mmseg - INFO - Iter [152700/160000]	lr: 5.129e-07, eta: 1:25:13, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1692, decode.acc_seg: 92.8372, loss: 0.1692
2023-11-11 16:32:55,683 - mmseg - INFO - Iter [152750/160000]	lr: 5.059e-07, eta: 1:24:38, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1714, decode.acc_seg: 92.6678, loss: 0.1714
2023-11-11 16:33:30,697 - mmseg - INFO - Iter [152800/160000]	lr: 4.990e-07, eta: 1:24:03, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1701, decode.acc_seg: 92.6655, loss: 0.1701
2023-11-11 16:34:04,680 - mmseg - INFO - Iter [152850/160000]	lr: 4.921e-07, eta: 1:23:28, time: 0.679, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1681, decode.acc_seg: 92.9094, loss: 0.1681
2023-11-11 16:34:40,318 - mmseg - INFO - Iter [152900/160000]	lr: 4.852e-07, eta: 1:22:53, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1687, decode.acc_seg: 92.7370, loss: 0.1687
2023-11-11 16:35:14,472 - mmseg - INFO - Iter [152950/160000]	lr: 4.784e-07, eta: 1:22:18, time: 0.684, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1587, decode.acc_seg: 92.9664, loss: 0.1587
2023-11-11 16:35:49,407 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 16:35:49,407 - mmseg - INFO - Iter [153000/160000]	lr: 4.717e-07, eta: 1:21:43, time: 0.698, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1683, decode.acc_seg: 93.0882, loss: 0.1683
2023-11-11 16:36:24,650 - mmseg - INFO - Iter [153050/160000]	lr: 4.650e-07, eta: 1:21:08, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1769, decode.acc_seg: 92.5455, loss: 0.1769
2023-11-11 16:36:56,569 - mmseg - INFO - Iter [153100/160000]	lr: 4.583e-07, eta: 1:20:32, time: 0.638, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1655, decode.acc_seg: 93.1001, loss: 0.1655
2023-11-11 16:37:32,399 - mmseg - INFO - Iter [153150/160000]	lr: 4.517e-07, eta: 1:19:57, time: 0.717, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1668, decode.acc_seg: 92.7205, loss: 0.1668
2023-11-11 16:38:04,166 - mmseg - INFO - Iter [153200/160000]	lr: 4.451e-07, eta: 1:19:22, time: 0.636, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1679, decode.acc_seg: 92.8200, loss: 0.1679
2023-11-11 16:38:36,018 - mmseg - INFO - Iter [153250/160000]	lr: 4.386e-07, eta: 1:18:47, time: 0.637, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1656, decode.acc_seg: 92.8694, loss: 0.1656
2023-11-11 16:39:07,342 - mmseg - INFO - Iter [153300/160000]	lr: 4.322e-07, eta: 1:18:12, time: 0.626, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1704, decode.acc_seg: 92.7998, loss: 0.1704
2023-11-11 16:39:41,572 - mmseg - INFO - Iter [153350/160000]	lr: 4.258e-07, eta: 1:17:37, time: 0.683, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1719, decode.acc_seg: 92.7592, loss: 0.1719
2023-11-11 16:40:16,461 - mmseg - INFO - Iter [153400/160000]	lr: 4.194e-07, eta: 1:17:02, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1565, decode.acc_seg: 93.2352, loss: 0.1565
2023-11-11 16:40:51,620 - mmseg - INFO - Iter [153450/160000]	lr: 4.131e-07, eta: 1:16:27, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1588, decode.acc_seg: 93.1513, loss: 0.1588
2023-11-11 16:41:26,431 - mmseg - INFO - Iter [153500/160000]	lr: 4.068e-07, eta: 1:15:52, time: 0.697, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1696, decode.acc_seg: 92.8562, loss: 0.1696
2023-11-11 16:42:02,471 - mmseg - INFO - Iter [153550/160000]	lr: 4.006e-07, eta: 1:15:17, time: 0.720, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1555, decode.acc_seg: 93.2669, loss: 0.1555
2023-11-11 16:42:37,266 - mmseg - INFO - Iter [153600/160000]	lr: 3.944e-07, eta: 1:14:42, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1632, decode.acc_seg: 92.9532, loss: 0.1632
2023-11-11 16:43:11,071 - mmseg - INFO - Iter [153650/160000]	lr: 3.883e-07, eta: 1:14:07, time: 0.676, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1589, decode.acc_seg: 93.2779, loss: 0.1589
2023-11-11 16:43:45,890 - mmseg - INFO - Iter [153700/160000]	lr: 3.822e-07, eta: 1:13:32, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1667, decode.acc_seg: 92.7750, loss: 0.1667
2023-11-11 16:44:20,789 - mmseg - INFO - Iter [153750/160000]	lr: 3.761e-07, eta: 1:12:57, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1595, decode.acc_seg: 93.0704, loss: 0.1595
2023-11-11 16:44:55,816 - mmseg - INFO - Iter [153800/160000]	lr: 3.702e-07, eta: 1:12:22, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1703, decode.acc_seg: 92.7230, loss: 0.1703
2023-11-11 16:45:30,936 - mmseg - INFO - Iter [153850/160000]	lr: 3.642e-07, eta: 1:11:47, time: 0.703, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1643, decode.acc_seg: 93.1567, loss: 0.1643
2023-11-11 16:46:05,869 - mmseg - INFO - Iter [153900/160000]	lr: 3.583e-07, eta: 1:11:12, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1691, decode.acc_seg: 92.7636, loss: 0.1691
2023-11-11 16:46:41,120 - mmseg - INFO - Iter [153950/160000]	lr: 3.525e-07, eta: 1:10:37, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1599, decode.acc_seg: 93.0672, loss: 0.1599
2023-11-11 16:47:13,962 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 16:47:13,962 - mmseg - INFO - Iter [154000/160000]	lr: 3.467e-07, eta: 1:10:02, time: 0.658, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1624, decode.acc_seg: 93.0656, loss: 0.1624
2023-11-11 16:47:48,470 - mmseg - INFO - Iter [154050/160000]	lr: 3.409e-07, eta: 1:09:26, time: 0.690, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1677, decode.acc_seg: 92.9255, loss: 0.1677
2023-11-11 16:48:22,000 - mmseg - INFO - Iter [154100/160000]	lr: 3.352e-07, eta: 1:08:51, time: 0.670, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1690, decode.acc_seg: 92.8654, loss: 0.1690
2023-11-11 16:48:57,107 - mmseg - INFO - Iter [154150/160000]	lr: 3.296e-07, eta: 1:08:16, time: 0.702, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1652, decode.acc_seg: 93.1770, loss: 0.1652
2023-11-11 16:49:29,236 - mmseg - INFO - Iter [154200/160000]	lr: 3.240e-07, eta: 1:07:41, time: 0.643, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1679, decode.acc_seg: 92.8660, loss: 0.1679
2023-11-11 16:50:00,673 - mmseg - INFO - Iter [154250/160000]	lr: 3.184e-07, eta: 1:07:06, time: 0.629, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1560, decode.acc_seg: 93.2787, loss: 0.1560
2023-11-11 16:50:33,751 - mmseg - INFO - Iter [154300/160000]	lr: 3.129e-07, eta: 1:06:31, time: 0.660, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1685, decode.acc_seg: 92.8537, loss: 0.1685
2023-11-11 16:51:07,328 - mmseg - INFO - Iter [154350/160000]	lr: 3.075e-07, eta: 1:05:56, time: 0.673, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1688, decode.acc_seg: 92.9250, loss: 0.1688
2023-11-11 16:51:39,071 - mmseg - INFO - Iter [154400/160000]	lr: 3.021e-07, eta: 1:05:21, time: 0.635, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1706, decode.acc_seg: 92.6929, loss: 0.1706
2023-11-11 16:52:11,190 - mmseg - INFO - Iter [154450/160000]	lr: 2.967e-07, eta: 1:04:46, time: 0.641, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1759, decode.acc_seg: 92.4846, loss: 0.1759
2023-11-11 16:52:43,712 - mmseg - INFO - Iter [154500/160000]	lr: 2.914e-07, eta: 1:04:11, time: 0.652, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1698, decode.acc_seg: 92.7657, loss: 0.1698
2023-11-11 16:53:15,539 - mmseg - INFO - Iter [154550/160000]	lr: 2.861e-07, eta: 1:03:36, time: 0.636, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1670, decode.acc_seg: 92.8174, loss: 0.1670
2023-11-11 16:53:47,037 - mmseg - INFO - Iter [154600/160000]	lr: 2.809e-07, eta: 1:03:00, time: 0.630, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1691, decode.acc_seg: 92.8753, loss: 0.1691
2023-11-11 16:54:18,632 - mmseg - INFO - Iter [154650/160000]	lr: 2.757e-07, eta: 1:02:25, time: 0.632, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1687, decode.acc_seg: 92.5810, loss: 0.1687
2023-11-11 16:54:52,133 - mmseg - INFO - Iter [154700/160000]	lr: 2.706e-07, eta: 1:01:50, time: 0.670, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1616, decode.acc_seg: 92.9768, loss: 0.1616
2023-11-11 16:55:23,602 - mmseg - INFO - Iter [154750/160000]	lr: 2.655e-07, eta: 1:01:15, time: 0.630, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1638, decode.acc_seg: 92.8503, loss: 0.1638
2023-11-11 16:55:59,037 - mmseg - INFO - Iter [154800/160000]	lr: 2.605e-07, eta: 1:00:40, time: 0.708, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1670, decode.acc_seg: 93.1445, loss: 0.1670
2023-11-11 16:56:31,850 - mmseg - INFO - Iter [154850/160000]	lr: 2.555e-07, eta: 1:00:05, time: 0.657, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1560, decode.acc_seg: 93.3625, loss: 0.1560
2023-11-11 16:57:03,790 - mmseg - INFO - Iter [154900/160000]	lr: 2.506e-07, eta: 0:59:30, time: 0.638, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1707, decode.acc_seg: 92.6572, loss: 0.1707
2023-11-11 16:57:37,666 - mmseg - INFO - Iter [154950/160000]	lr: 2.457e-07, eta: 0:58:55, time: 0.678, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1587, decode.acc_seg: 93.2198, loss: 0.1587
2023-11-11 16:58:10,913 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 16:58:10,913 - mmseg - INFO - Iter [155000/160000]	lr: 2.409e-07, eta: 0:58:20, time: 0.665, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1660, decode.acc_seg: 93.0321, loss: 0.1660
2023-11-11 16:58:44,067 - mmseg - INFO - Iter [155050/160000]	lr: 2.361e-07, eta: 0:57:45, time: 0.662, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1646, decode.acc_seg: 92.8758, loss: 0.1646
2023-11-11 16:59:19,258 - mmseg - INFO - Iter [155100/160000]	lr: 2.313e-07, eta: 0:57:10, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1624, decode.acc_seg: 92.9199, loss: 0.1624
2023-11-11 16:59:54,430 - mmseg - INFO - Iter [155150/160000]	lr: 2.266e-07, eta: 0:56:35, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1585, decode.acc_seg: 93.3466, loss: 0.1585
2023-11-11 17:00:27,040 - mmseg - INFO - Iter [155200/160000]	lr: 2.220e-07, eta: 0:56:00, time: 0.652, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1658, decode.acc_seg: 92.8243, loss: 0.1658
2023-11-11 17:01:01,739 - mmseg - INFO - Iter [155250/160000]	lr: 2.174e-07, eta: 0:55:25, time: 0.694, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1638, decode.acc_seg: 93.1055, loss: 0.1638
2023-11-11 17:01:35,179 - mmseg - INFO - Iter [155300/160000]	lr: 2.128e-07, eta: 0:54:50, time: 0.669, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1653, decode.acc_seg: 93.0914, loss: 0.1653
2023-11-11 17:02:10,136 - mmseg - INFO - Iter [155350/160000]	lr: 2.083e-07, eta: 0:54:15, time: 0.699, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1544, decode.acc_seg: 93.4351, loss: 0.1544
2023-11-11 17:02:43,943 - mmseg - INFO - Iter [155400/160000]	lr: 2.039e-07, eta: 0:53:40, time: 0.675, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1643, decode.acc_seg: 92.9086, loss: 0.1643
2023-11-11 17:03:16,053 - mmseg - INFO - Iter [155450/160000]	lr: 1.995e-07, eta: 0:53:04, time: 0.643, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1624, decode.acc_seg: 93.0190, loss: 0.1624
2023-11-11 17:03:47,800 - mmseg - INFO - Iter [155500/160000]	lr: 1.951e-07, eta: 0:52:29, time: 0.635, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1572, decode.acc_seg: 93.1289, loss: 0.1572
2023-11-11 17:04:21,159 - mmseg - INFO - Iter [155550/160000]	lr: 1.908e-07, eta: 0:51:54, time: 0.666, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1656, decode.acc_seg: 93.2228, loss: 0.1656
2023-11-11 17:04:56,737 - mmseg - INFO - Iter [155600/160000]	lr: 1.866e-07, eta: 0:51:19, time: 0.711, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1659, decode.acc_seg: 92.9030, loss: 0.1659
2023-11-11 17:05:32,063 - mmseg - INFO - Iter [155650/160000]	lr: 1.824e-07, eta: 0:50:44, time: 0.707, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1582, decode.acc_seg: 93.0997, loss: 0.1582
2023-11-11 17:06:07,089 - mmseg - INFO - Iter [155700/160000]	lr: 1.782e-07, eta: 0:50:09, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1597, decode.acc_seg: 93.0728, loss: 0.1597
2023-11-11 17:06:42,263 - mmseg - INFO - Iter [155750/160000]	lr: 1.741e-07, eta: 0:49:34, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1675, decode.acc_seg: 92.9183, loss: 0.1675
2023-11-11 17:07:17,070 - mmseg - INFO - Iter [155800/160000]	lr: 1.700e-07, eta: 0:48:59, time: 0.696, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1700, decode.acc_seg: 92.7185, loss: 0.1700
2023-11-11 17:07:52,705 - mmseg - INFO - Iter [155850/160000]	lr: 1.660e-07, eta: 0:48:24, time: 0.713, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1661, decode.acc_seg: 93.0095, loss: 0.1661
2023-11-11 17:08:27,813 - mmseg - INFO - Iter [155900/160000]	lr: 1.620e-07, eta: 0:47:49, time: 0.702, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1645, decode.acc_seg: 93.0479, loss: 0.1645
2023-11-11 17:09:03,315 - mmseg - INFO - Iter [155950/160000]	lr: 1.581e-07, eta: 0:47:14, time: 0.710, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1621, decode.acc_seg: 93.1193, loss: 0.1621
2023-11-11 17:09:38,591 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 17:09:38,591 - mmseg - INFO - Iter [156000/160000]	lr: 1.542e-07, eta: 0:46:39, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1656, decode.acc_seg: 92.9035, loss: 0.1656
2023-11-11 17:10:13,867 - mmseg - INFO - Iter [156050/160000]	lr: 1.504e-07, eta: 0:46:04, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1670, decode.acc_seg: 92.9553, loss: 0.1670
2023-11-11 17:10:49,180 - mmseg - INFO - Iter [156100/160000]	lr: 1.466e-07, eta: 0:45:29, time: 0.708, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1617, decode.acc_seg: 93.2228, loss: 0.1617
2023-11-11 17:11:20,482 - mmseg - INFO - Iter [156150/160000]	lr: 1.429e-07, eta: 0:44:54, time: 0.626, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1744, decode.acc_seg: 92.8304, loss: 0.1744
2023-11-11 17:11:53,693 - mmseg - INFO - Iter [156200/160000]	lr: 1.392e-07, eta: 0:44:19, time: 0.664, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1777, decode.acc_seg: 92.5811, loss: 0.1777
2023-11-11 17:12:28,743 - mmseg - INFO - Iter [156250/160000]	lr: 1.355e-07, eta: 0:43:44, time: 0.701, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1511, decode.acc_seg: 93.4516, loss: 0.1511
2023-11-11 17:13:04,017 - mmseg - INFO - Iter [156300/160000]	lr: 1.320e-07, eta: 0:43:09, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1636, decode.acc_seg: 93.0523, loss: 0.1636
2023-11-11 17:13:38,991 - mmseg - INFO - Iter [156350/160000]	lr: 1.284e-07, eta: 0:42:34, time: 0.699, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1650, decode.acc_seg: 93.0476, loss: 0.1650
2023-11-11 17:14:13,908 - mmseg - INFO - Iter [156400/160000]	lr: 1.249e-07, eta: 0:41:59, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1668, decode.acc_seg: 93.1393, loss: 0.1668
2023-11-11 17:14:49,196 - mmseg - INFO - Iter [156450/160000]	lr: 1.215e-07, eta: 0:41:24, time: 0.706, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1587, decode.acc_seg: 93.1763, loss: 0.1587
2023-11-11 17:15:24,160 - mmseg - INFO - Iter [156500/160000]	lr: 1.181e-07, eta: 0:40:49, time: 0.699, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1565, decode.acc_seg: 93.2743, loss: 0.1565
2023-11-11 17:15:59,085 - mmseg - INFO - Iter [156550/160000]	lr: 1.147e-07, eta: 0:40:14, time: 0.700, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1688, decode.acc_seg: 92.9076, loss: 0.1688
2023-11-11 17:16:33,349 - mmseg - INFO - Iter [156600/160000]	lr: 1.114e-07, eta: 0:39:39, time: 0.684, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1662, decode.acc_seg: 92.9587, loss: 0.1662
2023-11-11 17:17:07,500 - mmseg - INFO - Iter [156650/160000]	lr: 1.082e-07, eta: 0:39:04, time: 0.684, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1639, decode.acc_seg: 93.2286, loss: 0.1639
2023-11-11 17:17:40,744 - mmseg - INFO - Iter [156700/160000]	lr: 1.050e-07, eta: 0:38:29, time: 0.665, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1630, decode.acc_seg: 93.1107, loss: 0.1630
2023-11-11 17:18:14,151 - mmseg - INFO - Iter [156750/160000]	lr: 1.018e-07, eta: 0:37:54, time: 0.668, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1539, decode.acc_seg: 93.4146, loss: 0.1539
2023-11-11 17:18:48,678 - mmseg - INFO - Iter [156800/160000]	lr: 9.873e-08, eta: 0:37:19, time: 0.690, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1652, decode.acc_seg: 93.0541, loss: 0.1652
2023-11-11 17:19:21,715 - mmseg - INFO - Iter [156850/160000]	lr: 9.567e-08, eta: 0:36:44, time: 0.661, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1672, decode.acc_seg: 92.8294, loss: 0.1672
2023-11-11 17:19:53,238 - mmseg - INFO - Iter [156900/160000]	lr: 9.266e-08, eta: 0:36:09, time: 0.630, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1652, decode.acc_seg: 92.9790, loss: 0.1652
2023-11-11 17:20:26,748 - mmseg - INFO - Iter [156950/160000]	lr: 8.969e-08, eta: 0:35:34, time: 0.669, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1605, decode.acc_seg: 93.1117, loss: 0.1605
2023-11-11 17:21:02,667 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 17:21:02,667 - mmseg - INFO - Iter [157000/160000]	lr: 8.678e-08, eta: 0:34:59, time: 0.718, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1573, decode.acc_seg: 93.3190, loss: 0.1573
2023-11-11 17:21:37,155 - mmseg - INFO - Iter [157050/160000]	lr: 8.391e-08, eta: 0:34:24, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1664, decode.acc_seg: 92.7652, loss: 0.1664
2023-11-11 17:22:10,892 - mmseg - INFO - Iter [157100/160000]	lr: 8.109e-08, eta: 0:33:49, time: 0.675, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1704, decode.acc_seg: 92.9528, loss: 0.1704
2023-11-11 17:22:42,320 - mmseg - INFO - Iter [157150/160000]	lr: 7.832e-08, eta: 0:33:14, time: 0.628, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1679, decode.acc_seg: 92.7782, loss: 0.1679
2023-11-11 17:23:15,869 - mmseg - INFO - Iter [157200/160000]	lr: 7.560e-08, eta: 0:32:39, time: 0.671, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1655, decode.acc_seg: 92.8286, loss: 0.1655
2023-11-11 17:23:50,104 - mmseg - INFO - Iter [157250/160000]	lr: 7.292e-08, eta: 0:32:04, time: 0.684, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1592, decode.acc_seg: 93.0527, loss: 0.1592
2023-11-11 17:24:25,276 - mmseg - INFO - Iter [157300/160000]	lr: 7.030e-08, eta: 0:31:29, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1672, decode.acc_seg: 93.1065, loss: 0.1672
2023-11-11 17:24:57,832 - mmseg - INFO - Iter [157350/160000]	lr: 6.772e-08, eta: 0:30:54, time: 0.651, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1663, decode.acc_seg: 92.6666, loss: 0.1663
2023-11-11 17:25:33,072 - mmseg - INFO - Iter [157400/160000]	lr: 6.519e-08, eta: 0:30:19, time: 0.705, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1589, decode.acc_seg: 93.3277, loss: 0.1589
2023-11-11 17:26:06,541 - mmseg - INFO - Iter [157450/160000]	lr: 6.271e-08, eta: 0:29:44, time: 0.670, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1668, decode.acc_seg: 92.9097, loss: 0.1668
2023-11-11 17:26:41,040 - mmseg - INFO - Iter [157500/160000]	lr: 6.028e-08, eta: 0:29:09, time: 0.689, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1625, decode.acc_seg: 93.0596, loss: 0.1625
2023-11-11 17:27:15,205 - mmseg - INFO - Iter [157550/160000]	lr: 5.789e-08, eta: 0:28:34, time: 0.684, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1589, decode.acc_seg: 93.0775, loss: 0.1589
2023-11-11 17:27:46,818 - mmseg - INFO - Iter [157600/160000]	lr: 5.555e-08, eta: 0:27:59, time: 0.631, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1645, decode.acc_seg: 93.0451, loss: 0.1645
2023-11-11 17:28:22,605 - mmseg - INFO - Iter [157650/160000]	lr: 5.326e-08, eta: 0:27:24, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1744, decode.acc_seg: 92.6606, loss: 0.1744
2023-11-11 17:28:57,798 - mmseg - INFO - Iter [157700/160000]	lr: 5.102e-08, eta: 0:26:49, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1742, decode.acc_seg: 92.7590, loss: 0.1742
2023-11-11 17:29:31,237 - mmseg - INFO - Iter [157750/160000]	lr: 4.883e-08, eta: 0:26:14, time: 0.670, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1668, decode.acc_seg: 92.8922, loss: 0.1668
2023-11-11 17:30:05,096 - mmseg - INFO - Iter [157800/160000]	lr: 4.668e-08, eta: 0:25:39, time: 0.676, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1762, decode.acc_seg: 92.6025, loss: 0.1762
2023-11-11 17:30:40,028 - mmseg - INFO - Iter [157850/160000]	lr: 4.459e-08, eta: 0:25:04, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1571, decode.acc_seg: 93.1904, loss: 0.1571
2023-11-11 17:31:12,591 - mmseg - INFO - Iter [157900/160000]	lr: 4.254e-08, eta: 0:24:29, time: 0.652, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1557, decode.acc_seg: 93.3070, loss: 0.1557
2023-11-11 17:31:43,983 - mmseg - INFO - Iter [157950/160000]	lr: 4.054e-08, eta: 0:23:54, time: 0.628, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1581, decode.acc_seg: 93.3140, loss: 0.1581
2023-11-11 17:32:19,236 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 17:32:19,237 - mmseg - INFO - Iter [158000/160000]	lr: 3.859e-08, eta: 0:23:19, time: 0.704, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1635, decode.acc_seg: 93.0931, loss: 0.1635
2023-11-11 17:32:54,381 - mmseg - INFO - Iter [158050/160000]	lr: 3.668e-08, eta: 0:22:44, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1622, decode.acc_seg: 93.0032, loss: 0.1622
2023-11-11 17:33:30,173 - mmseg - INFO - Iter [158100/160000]	lr: 3.483e-08, eta: 0:22:09, time: 0.716, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1640, decode.acc_seg: 92.8940, loss: 0.1640
2023-11-11 17:34:05,229 - mmseg - INFO - Iter [158150/160000]	lr: 3.302e-08, eta: 0:21:34, time: 0.701, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1550, decode.acc_seg: 93.3570, loss: 0.1550
2023-11-11 17:34:40,463 - mmseg - INFO - Iter [158200/160000]	lr: 3.126e-08, eta: 0:20:59, time: 0.704, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1605, decode.acc_seg: 93.1565, loss: 0.1605
2023-11-11 17:35:14,137 - mmseg - INFO - Iter [158250/160000]	lr: 2.955e-08, eta: 0:20:24, time: 0.674, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1662, decode.acc_seg: 92.8432, loss: 0.1662
2023-11-11 17:35:48,973 - mmseg - INFO - Iter [158300/160000]	lr: 2.788e-08, eta: 0:19:49, time: 0.698, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1585, decode.acc_seg: 93.3456, loss: 0.1585
2023-11-11 17:36:22,598 - mmseg - INFO - Iter [158350/160000]	lr: 2.627e-08, eta: 0:19:14, time: 0.671, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1582, decode.acc_seg: 93.2422, loss: 0.1582
2023-11-11 17:36:56,791 - mmseg - INFO - Iter [158400/160000]	lr: 2.470e-08, eta: 0:18:39, time: 0.684, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1705, decode.acc_seg: 92.8440, loss: 0.1705
2023-11-11 17:37:31,027 - mmseg - INFO - Iter [158450/160000]	lr: 2.318e-08, eta: 0:18:04, time: 0.685, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1636, decode.acc_seg: 93.1229, loss: 0.1636
2023-11-11 17:38:05,767 - mmseg - INFO - Iter [158500/160000]	lr: 2.171e-08, eta: 0:17:29, time: 0.695, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1615, decode.acc_seg: 93.2925, loss: 0.1615
2023-11-11 17:38:40,507 - mmseg - INFO - Iter [158550/160000]	lr: 2.029e-08, eta: 0:16:54, time: 0.695, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1666, decode.acc_seg: 93.0664, loss: 0.1666
2023-11-11 17:39:15,049 - mmseg - INFO - Iter [158600/160000]	lr: 1.892e-08, eta: 0:16:19, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1649, decode.acc_seg: 92.9965, loss: 0.1649
2023-11-11 17:39:49,604 - mmseg - INFO - Iter [158650/160000]	lr: 1.759e-08, eta: 0:15:44, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1724, decode.acc_seg: 92.8743, loss: 0.1724
2023-11-11 17:40:24,778 - mmseg - INFO - Iter [158700/160000]	lr: 1.631e-08, eta: 0:15:09, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1697, decode.acc_seg: 92.8928, loss: 0.1697
2023-11-11 17:40:56,882 - mmseg - INFO - Iter [158750/160000]	lr: 1.508e-08, eta: 0:14:34, time: 0.643, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1563, decode.acc_seg: 93.2846, loss: 0.1563
2023-11-11 17:41:28,904 - mmseg - INFO - Iter [158800/160000]	lr: 1.390e-08, eta: 0:13:59, time: 0.639, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1627, decode.acc_seg: 93.1598, loss: 0.1627
2023-11-11 17:42:03,742 - mmseg - INFO - Iter [158850/160000]	lr: 1.277e-08, eta: 0:13:24, time: 0.697, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1609, decode.acc_seg: 93.0288, loss: 0.1609
2023-11-11 17:42:38,319 - mmseg - INFO - Iter [158900/160000]	lr: 1.168e-08, eta: 0:12:49, time: 0.691, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1624, decode.acc_seg: 93.0951, loss: 0.1624
2023-11-11 17:43:12,783 - mmseg - INFO - Iter [158950/160000]	lr: 1.065e-08, eta: 0:12:14, time: 0.690, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1712, decode.acc_seg: 92.7568, loss: 0.1712
2023-11-11 17:43:47,039 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 17:43:47,039 - mmseg - INFO - Iter [159000/160000]	lr: 9.657e-09, eta: 0:11:39, time: 0.685, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1531, decode.acc_seg: 93.2431, loss: 0.1531
2023-11-11 17:44:19,140 - mmseg - INFO - Iter [159050/160000]	lr: 8.717e-09, eta: 0:11:04, time: 0.643, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1636, decode.acc_seg: 92.9830, loss: 0.1636
2023-11-11 17:44:53,209 - mmseg - INFO - Iter [159100/160000]	lr: 7.824e-09, eta: 0:10:29, time: 0.680, data_time: 0.008, memory: 23129, decode.loss_ce: 0.1565, decode.acc_seg: 93.4320, loss: 0.1565
2023-11-11 17:45:28,278 - mmseg - INFO - Iter [159150/160000]	lr: 6.980e-09, eta: 0:09:54, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1682, decode.acc_seg: 92.8613, loss: 0.1682
2023-11-11 17:45:59,466 - mmseg - INFO - Iter [159200/160000]	lr: 6.184e-09, eta: 0:09:19, time: 0.624, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1718, decode.acc_seg: 92.8419, loss: 0.1718
2023-11-11 17:46:34,961 - mmseg - INFO - Iter [159250/160000]	lr: 5.436e-09, eta: 0:08:44, time: 0.709, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1682, decode.acc_seg: 92.8015, loss: 0.1682
2023-11-11 17:47:10,545 - mmseg - INFO - Iter [159300/160000]	lr: 4.736e-09, eta: 0:08:09, time: 0.712, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1701, decode.acc_seg: 92.7010, loss: 0.1701
2023-11-11 17:47:46,375 - mmseg - INFO - Iter [159350/160000]	lr: 4.085e-09, eta: 0:07:34, time: 0.717, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1757, decode.acc_seg: 92.6470, loss: 0.1757
2023-11-11 17:48:18,201 - mmseg - INFO - Iter [159400/160000]	lr: 3.481e-09, eta: 0:06:59, time: 0.638, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1521, decode.acc_seg: 93.4301, loss: 0.1521
2023-11-11 17:48:50,049 - mmseg - INFO - Iter [159450/160000]	lr: 2.926e-09, eta: 0:06:24, time: 0.637, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1675, decode.acc_seg: 92.8492, loss: 0.1675
2023-11-11 17:49:24,709 - mmseg - INFO - Iter [159500/160000]	lr: 2.419e-09, eta: 0:05:49, time: 0.692, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1665, decode.acc_seg: 92.9778, loss: 0.1665
2023-11-11 17:49:59,857 - mmseg - INFO - Iter [159550/160000]	lr: 1.960e-09, eta: 0:05:14, time: 0.703, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1729, decode.acc_seg: 92.7338, loss: 0.1729
2023-11-11 17:50:34,012 - mmseg - INFO - Iter [159600/160000]	lr: 1.550e-09, eta: 0:04:39, time: 0.683, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1608, decode.acc_seg: 93.0231, loss: 0.1608
2023-11-11 17:51:06,716 - mmseg - INFO - Iter [159650/160000]	lr: 1.187e-09, eta: 0:04:04, time: 0.654, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1651, decode.acc_seg: 93.0464, loss: 0.1651
2023-11-11 17:51:42,935 - mmseg - INFO - Iter [159700/160000]	lr: 8.732e-10, eta: 0:03:29, time: 0.724, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1589, decode.acc_seg: 93.1431, loss: 0.1589
2023-11-11 17:52:17,326 - mmseg - INFO - Iter [159750/160000]	lr: 6.072e-10, eta: 0:02:54, time: 0.689, data_time: 0.011, memory: 23129, decode.loss_ce: 0.1549, decode.acc_seg: 93.3631, loss: 0.1549
2023-11-11 17:52:49,363 - mmseg - INFO - Iter [159800/160000]	lr: 3.894e-10, eta: 0:02:19, time: 0.641, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1631, decode.acc_seg: 93.0884, loss: 0.1631
2023-11-11 17:53:20,767 - mmseg - INFO - Iter [159850/160000]	lr: 2.198e-10, eta: 0:01:44, time: 0.628, data_time: 0.009, memory: 23129, decode.loss_ce: 0.1624, decode.acc_seg: 93.1068, loss: 0.1624
2023-11-11 17:53:52,359 - mmseg - INFO - Iter [159900/160000]	lr: 9.833e-11, eta: 0:01:09, time: 0.632, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1658, decode.acc_seg: 92.8820, loss: 0.1658
2023-11-11 17:54:23,773 - mmseg - INFO - Iter [159950/160000]	lr: 2.508e-11, eta: 0:00:34, time: 0.628, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1653, decode.acc_seg: 93.0481, loss: 0.1653
2023-11-11 17:54:56,399 - mmseg - INFO - Saving checkpoint at 160000 iterations
2023-11-11 17:55:01,019 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 17:55:01,019 - mmseg - INFO - Iter [160000/160000]	lr: 1.964e-14, eta: 0:00:00, time: 0.745, data_time: 0.010, memory: 23129, decode.loss_ce: 0.1687, decode.acc_seg: 93.0582, loss: 0.1687
2023-11-11 17:56:28,042 - mmseg - INFO - per class results:
2023-11-11 17:56:28,054 - mmseg - INFO - 
+---------------------+-------+-------+
|        Class        |  IoU  |  Acc  |
+---------------------+-------+-------+
|         wall        | 77.82 | 88.45 |
|       building      | 81.81 | 91.29 |
|         sky         | 94.55 | 97.72 |
|        floor        | 82.86 | 90.47 |
|         tree        | 75.19 | 88.67 |
|       ceiling       | 83.66 | 92.55 |
|         road        | 82.59 |  89.6 |
|         bed         | 89.86 | 96.01 |
|      windowpane     | 62.44 | 78.29 |
|        grass        | 65.71 | 82.66 |
|       cabinet       | 61.29 | 74.51 |
|       sidewalk      | 65.46 | 81.05 |
|        person       | 81.59 | 93.99 |
|        earth        | 37.75 | 52.88 |
|         door        | 50.18 | 63.04 |
|        table        | 62.94 | 77.07 |
|       mountain      | 52.74 | 66.25 |
|        plant        |  53.6 | 63.63 |
|       curtain       | 74.09 | 85.91 |
|        chair        | 58.08 | 72.98 |
|         car         | 83.77 | 92.29 |
|        water        | 60.46 |  78.7 |
|       painting      | 72.33 | 87.23 |
|         sofa        | 67.15 | 83.49 |
|        shelf        | 42.83 | 60.78 |
|        house        |  43.2 | 59.36 |
|         sea         | 59.56 | 74.38 |
|        mirror       | 69.35 | 75.87 |
|         rug         | 64.33 | 76.28 |
|        field        | 28.47 | 44.75 |
|       armchair      | 44.74 | 62.58 |
|         seat        | 62.84 | 79.89 |
|        fence        | 43.19 | 57.95 |
|         desk        | 51.74 | 71.97 |
|         rock        | 35.37 | 55.07 |
|       wardrobe      | 49.97 | 69.96 |
|         lamp        | 65.99 | 77.13 |
|       bathtub       | 81.15 | 86.61 |
|       railing       | 30.73 | 42.93 |
|       cushion       | 59.04 | 71.33 |
|         base        | 30.35 | 39.66 |
|         box         |  30.3 | 37.33 |
|        column       | 45.54 | 55.55 |
|      signboard      | 39.49 |  53.8 |
|   chest of drawers  | 46.59 | 57.33 |
|       counter       |  37.8 | 44.68 |
|         sand        | 51.34 | 72.97 |
|         sink        | 71.87 | 80.81 |
|      skyscraper     | 48.23 | 60.35 |
|      fireplace      | 75.21 | 89.93 |
|     refrigerator    | 77.98 | 85.19 |
|      grandstand     | 44.86 | 67.96 |
|         path        | 23.36 |  38.2 |
|        stairs       | 30.28 | 40.47 |
|        runway       | 64.91 | 83.19 |
|         case        | 45.28 | 54.64 |
|      pool table     | 93.35 |  96.6 |
|        pillow       | 61.67 |  73.4 |
|     screen door     | 68.18 | 83.26 |
|       stairway      | 29.23 | 37.29 |
|        river        | 11.21 | 21.48 |
|        bridge       | 72.95 | 82.36 |
|       bookcase      | 37.29 | 55.73 |
|        blind        | 45.05 | 51.31 |
|     coffee table    | 53.69 | 82.23 |
|        toilet       | 86.47 | 91.37 |
|        flower       | 38.23 |  53.6 |
|         book        | 45.94 | 68.34 |
|         hill        | 10.64 | 19.27 |
|        bench        | 45.92 | 52.42 |
|      countertop     | 58.95 | 82.95 |
|        stove        | 73.62 | 82.04 |
|         palm        | 48.04 | 72.28 |
|    kitchen island   |  40.7 |  71.5 |
|       computer      | 65.31 | 75.43 |
|     swivel chair    | 49.77 | 66.44 |
|         boat        | 46.66 | 55.16 |
|         bar         | 50.98 | 66.04 |
|    arcade machine   | 75.47 | 81.55 |
|        hovel        | 58.12 |  66.2 |
|         bus         | 86.27 | 96.06 |
|        towel        | 66.47 |  77.0 |
|        light        | 55.73 | 66.23 |
|        truck        | 31.99 | 40.14 |
|        tower        |  5.85 |  8.85 |
|      chandelier     | 67.48 | 82.55 |
|        awning       | 27.83 | 37.15 |
|     streetlight     | 29.28 | 38.89 |
|        booth        | 62.18 | 65.46 |
| television receiver | 65.66 | 80.44 |
|       airplane      | 61.44 | 74.76 |
|      dirt track     |  5.4  |  8.03 |
|       apparel       | 50.28 | 71.39 |
|         pole        | 25.49 | 34.73 |
|         land        |  0.01 |  0.01 |
|      bannister      | 13.35 | 17.63 |
|      escalator      | 37.09 | 47.76 |
|       ottoman       | 48.18 | 64.75 |
|        bottle       | 36.13 | 63.78 |
|        buffet       | 51.17 | 59.13 |
|        poster       | 31.89 | 43.73 |
|        stage        | 16.46 | 21.67 |
|         van         | 40.31 | 56.92 |
|         ship        | 57.67 | 83.72 |
|       fountain      | 40.59 | 40.98 |
|    conveyer belt    | 79.67 | 96.33 |
|        canopy       | 21.54 | 34.06 |
|        washer       | 75.13 | 76.21 |
|      plaything      | 20.66 | 32.78 |
|    swimming pool    | 78.55 | 87.45 |
|        stool        | 37.25 | 51.09 |
|        barrel       | 62.07 |  75.2 |
|        basket       | 36.92 | 48.59 |
|      waterfall      | 65.77 | 92.29 |
|         tent        | 96.27 |  98.4 |
|         bag         | 19.44 | 25.83 |
|       minibike      | 66.43 | 80.43 |
|        cradle       | 80.83 |  97.5 |
|         oven        | 42.35 | 66.49 |
|         ball        | 46.49 | 64.18 |
|         food        | 48.54 | 53.48 |
|         step        | 13.33 | 17.21 |
|         tank        | 47.83 | 54.27 |
|      trade name     | 26.52 |  31.2 |
|      microwave      | 75.23 | 85.38 |
|         pot         | 41.97 | 47.18 |
|        animal       | 55.13 | 57.93 |
|       bicycle       | 58.24 | 80.28 |
|         lake        | 53.49 | 57.91 |
|      dishwasher     | 60.78 | 73.97 |
|        screen       | 56.83 | 82.45 |
|       blanket       | 15.33 | 18.94 |
|      sculpture      | 64.34 | 82.44 |
|         hood        |  61.8 | 75.28 |
|        sconce       | 47.43 | 55.62 |
|         vase        | 39.78 | 54.23 |
|    traffic light    | 29.35 | 45.73 |
|         tray        |  9.69 | 19.39 |
|        ashcan       | 41.13 | 56.15 |
|         fan         | 59.78 | 75.99 |
|         pier        | 11.48 | 12.05 |
|      crt screen     |  2.73 | 10.04 |
|        plate        | 48.47 | 63.93 |
|       monitor       | 10.85 | 11.96 |
|    bulletin board   | 52.85 | 61.69 |
|        shower       |  3.29 |  5.02 |
|       radiator      | 67.17 | 74.44 |
|        glass        | 14.69 | 15.75 |
|        clock        | 35.84 | 45.69 |
|         flag        | 45.73 | 49.92 |
+---------------------+-------+-------+
2023-11-11 17:56:28,055 - mmseg - INFO - Summary:
2023-11-11 17:56:28,055 - mmseg - INFO - 
+------+-------+-------+
| aAcc |  mIoU |  mAcc |
+------+-------+-------+
| 83.3 | 50.56 | 62.47 |
+------+-------+-------+
2023-11-11 17:56:28,068 - mmseg - INFO - Exp name: fpn_512_debi_base_160k.py
2023-11-11 17:56:28,068 - mmseg - INFO - Iter(val) [250]	aAcc: 0.8330, mIoU: 0.5056, mAcc: 0.6247, IoU.wall: 0.7782, IoU.building: 0.8181, IoU.sky: 0.9455, IoU.floor: 0.8286, IoU.tree: 0.7519, IoU.ceiling: 0.8366, IoU.road: 0.8259, IoU.bed : 0.8986, IoU.windowpane: 0.6244, IoU.grass: 0.6571, IoU.cabinet: 0.6129, IoU.sidewalk: 0.6546, IoU.person: 0.8159, IoU.earth: 0.3775, IoU.door: 0.5018, IoU.table: 0.6294, IoU.mountain: 0.5274, IoU.plant: 0.5360, IoU.curtain: 0.7409, IoU.chair: 0.5808, IoU.car: 0.8377, IoU.water: 0.6046, IoU.painting: 0.7233, IoU.sofa: 0.6715, IoU.shelf: 0.4283, IoU.house: 0.4320, IoU.sea: 0.5956, IoU.mirror: 0.6935, IoU.rug: 0.6433, IoU.field: 0.2847, IoU.armchair: 0.4474, IoU.seat: 0.6284, IoU.fence: 0.4319, IoU.desk: 0.5174, IoU.rock: 0.3537, IoU.wardrobe: 0.4997, IoU.lamp: 0.6599, IoU.bathtub: 0.8115, IoU.railing: 0.3073, IoU.cushion: 0.5904, IoU.base: 0.3035, IoU.box: 0.3030, IoU.column: 0.4554, IoU.signboard: 0.3949, IoU.chest of drawers: 0.4659, IoU.counter: 0.3780, IoU.sand: 0.5134, IoU.sink: 0.7187, IoU.skyscraper: 0.4823, IoU.fireplace: 0.7521, IoU.refrigerator: 0.7798, IoU.grandstand: 0.4486, IoU.path: 0.2336, IoU.stairs: 0.3028, IoU.runway: 0.6491, IoU.case: 0.4528, IoU.pool table: 0.9335, IoU.pillow: 0.6167, IoU.screen door: 0.6818, IoU.stairway: 0.2923, IoU.river: 0.1121, IoU.bridge: 0.7295, IoU.bookcase: 0.3729, IoU.blind: 0.4505, IoU.coffee table: 0.5369, IoU.toilet: 0.8647, IoU.flower: 0.3823, IoU.book: 0.4594, IoU.hill: 0.1064, IoU.bench: 0.4592, IoU.countertop: 0.5895, IoU.stove: 0.7362, IoU.palm: 0.4804, IoU.kitchen island: 0.4070, IoU.computer: 0.6531, IoU.swivel chair: 0.4977, IoU.boat: 0.4666, IoU.bar: 0.5098, IoU.arcade machine: 0.7547, IoU.hovel: 0.5812, IoU.bus: 0.8627, IoU.towel: 0.6647, IoU.light: 0.5573, IoU.truck: 0.3199, IoU.tower: 0.0585, IoU.chandelier: 0.6748, IoU.awning: 0.2783, IoU.streetlight: 0.2928, IoU.booth: 0.6218, IoU.television receiver: 0.6566, IoU.airplane: 0.6144, IoU.dirt track: 0.0540, IoU.apparel: 0.5028, IoU.pole: 0.2549, IoU.land: 0.0001, IoU.bannister: 0.1335, IoU.escalator: 0.3709, IoU.ottoman: 0.4818, IoU.bottle: 0.3613, IoU.buffet: 0.5117, IoU.poster: 0.3189, IoU.stage: 0.1646, IoU.van: 0.4031, IoU.ship: 0.5767, IoU.fountain: 0.4059, IoU.conveyer belt: 0.7967, IoU.canopy: 0.2154, IoU.washer: 0.7513, IoU.plaything: 0.2066, IoU.swimming pool: 0.7855, IoU.stool: 0.3725, IoU.barrel: 0.6207, IoU.basket: 0.3692, IoU.waterfall: 0.6577, IoU.tent: 0.9627, IoU.bag: 0.1944, IoU.minibike: 0.6643, IoU.cradle: 0.8083, IoU.oven: 0.4235, IoU.ball: 0.4649, IoU.food: 0.4854, IoU.step: 0.1333, IoU.tank: 0.4783, IoU.trade name: 0.2652, IoU.microwave: 0.7523, IoU.pot: 0.4197, IoU.animal: 0.5513, IoU.bicycle: 0.5824, IoU.lake: 0.5349, IoU.dishwasher: 0.6078, IoU.screen: 0.5683, IoU.blanket: 0.1533, IoU.sculpture: 0.6434, IoU.hood: 0.6180, IoU.sconce: 0.4743, IoU.vase: 0.3978, IoU.traffic light: 0.2935, IoU.tray: 0.0969, IoU.ashcan: 0.4113, IoU.fan: 0.5978, IoU.pier: 0.1148, IoU.crt screen: 0.0273, IoU.plate: 0.4847, IoU.monitor: 0.1085, IoU.bulletin board: 0.5285, IoU.shower: 0.0329, IoU.radiator: 0.6717, IoU.glass: 0.1469, IoU.clock: 0.3584, IoU.flag: 0.4573, Acc.wall: 0.8845, Acc.building: 0.9129, Acc.sky: 0.9772, Acc.floor: 0.9047, Acc.tree: 0.8867, Acc.ceiling: 0.9255, Acc.road: 0.8960, Acc.bed : 0.9601, Acc.windowpane: 0.7829, Acc.grass: 0.8266, Acc.cabinet: 0.7451, Acc.sidewalk: 0.8105, Acc.person: 0.9399, Acc.earth: 0.5288, Acc.door: 0.6304, Acc.table: 0.7707, Acc.mountain: 0.6625, Acc.plant: 0.6363, Acc.curtain: 0.8591, Acc.chair: 0.7298, Acc.car: 0.9229, Acc.water: 0.7870, Acc.painting: 0.8723, Acc.sofa: 0.8349, Acc.shelf: 0.6078, Acc.house: 0.5936, Acc.sea: 0.7438, Acc.mirror: 0.7587, Acc.rug: 0.7628, Acc.field: 0.4475, Acc.armchair: 0.6258, Acc.seat: 0.7989, Acc.fence: 0.5795, Acc.desk: 0.7197, Acc.rock: 0.5507, Acc.wardrobe: 0.6996, Acc.lamp: 0.7713, Acc.bathtub: 0.8661, Acc.railing: 0.4293, Acc.cushion: 0.7133, Acc.base: 0.3966, Acc.box: 0.3733, Acc.column: 0.5555, Acc.signboard: 0.5380, Acc.chest of drawers: 0.5733, Acc.counter: 0.4468, Acc.sand: 0.7297, Acc.sink: 0.8081, Acc.skyscraper: 0.6035, Acc.fireplace: 0.8993, Acc.refrigerator: 0.8519, Acc.grandstand: 0.6796, Acc.path: 0.3820, Acc.stairs: 0.4047, Acc.runway: 0.8319, Acc.case: 0.5464, Acc.pool table: 0.9660, Acc.pillow: 0.7340, Acc.screen door: 0.8326, Acc.stairway: 0.3729, Acc.river: 0.2148, Acc.bridge: 0.8236, Acc.bookcase: 0.5573, Acc.blind: 0.5131, Acc.coffee table: 0.8223, Acc.toilet: 0.9137, Acc.flower: 0.5360, Acc.book: 0.6834, Acc.hill: 0.1927, Acc.bench: 0.5242, Acc.countertop: 0.8295, Acc.stove: 0.8204, Acc.palm: 0.7228, Acc.kitchen island: 0.7150, Acc.computer: 0.7543, Acc.swivel chair: 0.6644, Acc.boat: 0.5516, Acc.bar: 0.6604, Acc.arcade machine: 0.8155, Acc.hovel: 0.6620, Acc.bus: 0.9606, Acc.towel: 0.7700, Acc.light: 0.6623, Acc.truck: 0.4014, Acc.tower: 0.0885, Acc.chandelier: 0.8255, Acc.awning: 0.3715, Acc.streetlight: 0.3889, Acc.booth: 0.6546, Acc.television receiver: 0.8044, Acc.airplane: 0.7476, Acc.dirt track: 0.0803, Acc.apparel: 0.7139, Acc.pole: 0.3473, Acc.land: 0.0001, Acc.bannister: 0.1763, Acc.escalator: 0.4776, Acc.ottoman: 0.6475, Acc.bottle: 0.6378, Acc.buffet: 0.5913, Acc.poster: 0.4373, Acc.stage: 0.2167, Acc.van: 0.5692, Acc.ship: 0.8372, Acc.fountain: 0.4098, Acc.conveyer belt: 0.9633, Acc.canopy: 0.3406, Acc.washer: 0.7621, Acc.plaything: 0.3278, Acc.swimming pool: 0.8745, Acc.stool: 0.5109, Acc.barrel: 0.7520, Acc.basket: 0.4859, Acc.waterfall: 0.9229, Acc.tent: 0.9840, Acc.bag: 0.2583, Acc.minibike: 0.8043, Acc.cradle: 0.9750, Acc.oven: 0.6649, Acc.ball: 0.6418, Acc.food: 0.5348, Acc.step: 0.1721, Acc.tank: 0.5427, Acc.trade name: 0.3120, Acc.microwave: 0.8538, Acc.pot: 0.4718, Acc.animal: 0.5793, Acc.bicycle: 0.8028, Acc.lake: 0.5791, Acc.dishwasher: 0.7397, Acc.screen: 0.8245, Acc.blanket: 0.1894, Acc.sculpture: 0.8244, Acc.hood: 0.7528, Acc.sconce: 0.5562, Acc.vase: 0.5423, Acc.traffic light: 0.4573, Acc.tray: 0.1939, Acc.ashcan: 0.5615, Acc.fan: 0.7599, Acc.pier: 0.1205, Acc.crt screen: 0.1004, Acc.plate: 0.6393, Acc.monitor: 0.1196, Acc.bulletin board: 0.6169, Acc.shower: 0.0502, Acc.radiator: 0.7444, Acc.glass: 0.1575, Acc.clock: 0.4569, Acc.flag: 0.4992
